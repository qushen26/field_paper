# Linear Models


### Overview

### Models Structures

#### Variable Transforamtion

Among the above literature, some regression models take logarithm transforms on all variables, while others only transform one or a part. Even though they have various data sources, it is hard to believe they are all correct or equivalent. Some interdisciplinary knowledge could give some hints.

#### Fixed and Mixed Effects

#### Bayesian approaches (Opt.)

\pagebreak

### Models Specifications

#### Theoretical considerations


#### Subset Selection

Empirical or methodological considerations

#### Shrinkage Methods

Ridge Regression and Lasso

#### Dimention Reduction Methods

Principal Components Regression (PCR)

Partial Least Square (PLS)

To address the multicollinearity and interactions issues, @cliftonGettingHereThere2017 suggest to convert the various environmental characteristics to built environment indices. Some research try to use 'compactness indices' to replace the single density measurement [^47]. The primary method is principal components analysis (PCA), which synthesizes many variables to four dimensions: development density, land use mix, activity centering, and street connectivity. The advantage of this method is to increase the elasticity value significantly. A recent study shows that the elasticity of VMT with respect to a county compactness index is -0.78. Using a similar method (common factor analysis), @hongHowBuiltenvironmentFactors2014 convert eight attitudinal questions in the 2006 Household Activity Survey to three factors: Ease, Convenience, and Pro-transit. They fit the model using two geographic scales: 1-km buffer and traffic analysis zone (TAZ). At the TAZ level, the *nonresidential density* and *distance from CBD* have significant effects on VMT. The disadvantage of this method is that the internal mechanisms of the indices are still not clear.

[^47]: @ewingRelationshipUrbanSprawl2014;@hamidiLongitudinalStudyChanges2014;@hamidiMeasuringSprawlIts2015;@ewingUrbanSprawlRisk2016

```{r,eval=F}
# Compute k-means with k = 3
set.seed(123)
res.km <- kmeans(scale(iris[, -5]), 3, nstart = 25)
# K-means clusters showing the group of each individuals
fviz_cluster(res.km, data = iris[, -5],
             palette = "set3", #c("#2E9FDF", "#00AFBB", "#E7B800")
             geom = "point", pointsize=1, # "text"
             ellipse = T,ellipse.type ="t",   #'confidence'"euclid""convex"
             ellipse.level = 0.75, ellipse.alpha = 0.1,
             ggtheme = theme_bw(), main="Models with trip response variable",xlab="data", ylab="models"
             )
```

```{r,eval=F}
res.km <- kmeans(scale(USArrests), 4, nstart = 25)
fviz_cluster(res.km, data = USArrests,
             palette = "set3", #c("#2E9FDF", "#00AFBB", "#E7B800")
             geom = "point", pointsize=1, # "text"
             ellipse = T,ellipse.type ="t",   #'confidence'"euclid""convex"
             ellipse.level = 0.75, ellipse.alpha = 0.1,legend = "none",
             ggtheme = theme_bw(), main="Models with trip response variable",xlab="data", ylab="models"
             )
```

```{r modelcluster, out.width='50%',fig.cap="A diagram of model cluster"}
# Dimension reduction using PCA
res.pca <- prcomp(USArrests,  scale = TRUE) #iris[, -5]
# Coordinates of individuals
ind.coord <- as.data.frame(get_pca_ind(res.pca)$coord)
# Add clusters obtained using the K-means algorithm
res.km <- kmeans(scale(USArrests), 4, nstart = 25)
ind.coord$cluster <- factor(res.km$cluster)
# Add Species groups from the original data sett
# ind.coord$Species <- iris$Species
eigenvalue <- round(get_eigenvalue(res.pca), 1)
variance.percent <- eigenvalue$variance.percent
p <- ggscatter(ind.coord,x = "Dim.1", y = "Dim.2",
  color = "cluster", palette = "npg", title="Find some proper Model clusters",
  ellipse = TRUE, ellipse.type = "t", #"convex"
  ellipse.border.remove=T, ellipse.level = 0.85,
  shape = "cluster", #"Species"
  size = 1.5,legend = "none", ggtheme = theme_bw(),
  xlab = "Various scale or scope",ylab = "Various probability distribution",
) + # theme(axis.text = element_blank()) +
  stat_mean(aes(color = cluster), size = 8,shape = c(1,2,0,3))

ggpar(p,tickslab=F,ticks=F)
```

\pagebreak

### Diagnousis and Validation

The third source, model selection, should also be controlled. In general, modeling is a case-by-case work. Researchers may have their preferred model by weighing the sensitivity and robustness even given the same hypothesis and data. Hence, effective comparison or classification requires a few conditions:

Of course, an effective comparison also should present the full results of model diagnosis and validation. In practice, a more certain modeling framework allows scholars to present more general conclusions and speak with a clear voice to the public and decision-makers.

-   Checking the assumption of Independent Identically Distributed random variables (IID)

#### Checking Residuals

The residual plots prove log-transform of VMT is reasonable because it has a highly right-skewed probability distribution. Only looking at the $R^2$ and p-values cannot disclose this feature. This result suggests that residual diagnosis is an essential step for modeling validation.

#### Checking Residuals

#### Checking multicollinearity

If the Variance Inflation Factor of a predictor variable is high (e.g. VIF=9), this means that the standard error for the coefficient of that predictor variable is 3 times larger than if that predictor had 0 correlation with the other predictors. Table \@ref(tab:vifdd) shows that the county-level model has serious multicollinearity issue. The double log transformation seems to enlarge the VIF values on D1a and D3a (Table \@ref(tab:vifdd),\@ref(tab:vif)).

#### Goodness of fit

Two models with different specifications also have varied outcomes. The goodness-of-fit level is a premise. Although the coefficients of determination $R^2$ are not the only criteria of a good model, it is often available in most published papers. The $R^2$ in many disaggregate studies are around 0.3. A seriously underfitting model's outputs could be biased and unstable. In these cases, the comparison must have strong constraints. By dint of some synthetic variables, the disaggregated model's $R^2$ can be over 0.5. But the risk is these techniques may describe the data themselves, and the results cannot be generalized. Meanwhile, the $R^2$ in some aggregate studies can reach 0.8. It is worthy of a deeper investigation.

### Interpretation

#### Standardized coefficients

General case: It is very easy to go from the metric to the standardized coefficients. There is no need to actually compute the standardized variables and run a new regression.

$$\beta'_k=\beta_k\cdot\frac{s_{x_k}}{s_y}$$

Two IV case: Compare this to the formula for the metric coefficients. Note that correlations take the place of the corresponding variances and covariances.

$$\beta'_1=\frac{r_{y1}-r_{12}r_{y2}}{1-r^2_{12}};\qquad\beta'_2=\frac{r_{y2}-r_{12}r_{y1}}{1-r^2_{12}}$$

One IV case: the standardized coefficient simply equals the correlation between Y and X

$$\beta'_k=r_{yx}$$

<https://www3.nd.edu/~rwilliam/stats1/x92.pdf>

#### Elasticity

Definition: Commonly used to determine the relative importance of a variable in terms of its influence on a dependent variable. It is generally interpreted as the percent change in the dependent variable induced by a 1% change in the independent variable.

$$e_i=\beta_i\frac{X_i}{Y_i}\approx\frac{\partial Y_i}{\partial X_i}\frac{X_i}{Y_i}$$[^48]

[^48]: McCarthy, P.S., Transportation Economics Theory and Practice: A Case Study Approach. Blackwell, Boston, 2001.

+------------+-------------------------+-----------------------------------+
| Model      | Marginal Effects        | Elasticity                        |
+:==========:+:=======================:+:=================================:+
| Linear     | $\beta$                 | $\beta\frac{X_i}{Y_i}$            |
+------------+-------------------------+-----------------------------------+
| Log-linear | $\beta Y_i$             | $\beta X_i$                       |
+------------+-------------------------+-----------------------------------+
| Linear-log | $\beta\frac{1}{X_i}$    | $\beta\frac{1}{Y_i}$              |
+------------+-------------------------+-----------------------------------+
| Double log | $\beta\frac{Y_i}{X_i}$  | $\beta$                           |
+------------+-------------------------+-----------------------------------+
| Mixed      |                         |                                   |
+------------+-------------------------+-----------------------------------+
| Logit      | $\beta p_i(1-p_i)$      | $\beta X_i(1-p_i)$;[^49]          |
+------------+-------------------------+-----------------------------------+
| Poisson    | $\beta \lambda_i$;[^50] | $\beta X_i$                       |
+------------+-------------------------+-----------------------------------+
| NB         | $\beta \lambda_i$       | $\beta X_i$                       |
+------------+-------------------------+-----------------------------------+
| Tr-Pois    | $\delta_i$;[^51]        |                                   |
+------------+-------------------------+-----------------------------------+
| Hurdle-P   | separately [^52]        |                                   |
+------------+-------------------------+-----------------------------------+

: Elasticity Estimates for Various Functional Forms

[^49]: $\beta \bar X\left(1-\frac{\bar Y}{n}\right)$ (Ewing and Cervero, 2010)

[^50]: $\lambda_i=\exp[\mathbf{x}_i'\boldsymbol{\beta}]$ (Greene, 2012, eq.18-21)

[^51]: $\delta_i=\frac{(1-P_{i,0}-\lambda_i P_{i,0})}{(1-P_{i,0})^2}\cdot\lambda_i\beta$ (Greene, 2012, eq.18-23)

[^52]: (Greene, 2012, p.865)

#### Combined effects?

### Summary

Another attempt tries the method of structural equation modeling (SEM). The two studies capture higher elasticities of per capita VMT with respect to density (-0.38 and -0.238) [@cerveroEffectsBuiltEnvironments2010; @ewingStructuralEquationModels2014].

As far as methods, standardization, orthogonalization before fitting the model might be helpful.
