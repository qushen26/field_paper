<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Several Issues | The Association Between Travel and Urban Form</title>
  <meta name="description" content="This is a field paper using the bookdown package. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.24.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Several Issues | The Association Between Travel and Urban Form" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a field paper using the bookdown package. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="qushen26/field_paper" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Several Issues | The Association Between Travel and Urban Form" />
  
  <meta name="twitter:description" content="This is a field paper using the bookdown package. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Shen Qu" />


<meta name="date" content="2021-11-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="common-methods.html"/>
<link rel="next" href="new-trends.html"/>
<script src="book_assets/header-attrs-2.11/header-attrs.js"></script>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Field Paper</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>Part I Theories and Framework</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#analytical-framework"><i class="fa fa-check"></i><b>1.2</b> Analytical Framework</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#content-organization"><i class="fa fa-check"></i><b>1.3</b> Content Organization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="form.html"><a href="form.html"><i class="fa fa-check"></i><b>2</b> Urban Form as Predictors</a>
<ul>
<li class="chapter" data-level="2.1" data-path="form.html"><a href="form.html#influencing-direction"><i class="fa fa-check"></i><b>2.1</b> Influencing Direction</a></li>
<li class="chapter" data-level="2.2" data-path="form.html"><a href="form.html#influencing-factors"><i class="fa fa-check"></i><b>2.2</b> Influencing Factors</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="form.html"><a href="form.html#individual-factors"><i class="fa fa-check"></i><b>2.2.1</b> Individual Factors</a></li>
<li class="chapter" data-level="2.2.2" data-path="form.html"><a href="form.html#environmental-factors"><i class="fa fa-check"></i><b>2.2.2</b> Environmental Factors</a></li>
<li class="chapter" data-level="2.2.3" data-path="form.html"><a href="form.html#density"><i class="fa fa-check"></i><b>2.2.3</b> Density</a></li>
<li class="chapter" data-level="2.2.4" data-path="form.html"><a href="form.html#d-variables"><i class="fa fa-check"></i><b>2.2.4</b> D-variables</a></li>
<li class="chapter" data-level="2.2.5" data-path="form.html"><a href="form.html#synthesized-index"><i class="fa fa-check"></i><b>2.2.5</b> Synthesized Index</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="form.html"><a href="form.html#meta-aanalysis"><i class="fa fa-check"></i><b>2.3</b> Meta-Aanalysis</a></li>
<li class="chapter" data-level="2.4" data-path="form.html"><a href="form.html#scale"><i class="fa fa-check"></i><b>2.4</b> Spatial Scales</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="form.html"><a href="form.html#modifiable-areal-unit-problem-maup"><i class="fa fa-check"></i><b>2.4.1</b> Modifiable areal unit problem (MAUP)</a></li>
<li class="chapter" data-level="2.4.2" data-path="form.html"><a href="form.html#aggregated-analysis"><i class="fa fa-check"></i><b>2.4.2</b> Aggregated Analysis</a></li>
<li class="chapter" data-level="2.4.3" data-path="form.html"><a href="form.html#disaggregated-analysis"><i class="fa fa-check"></i><b>2.4.3</b> Disaggregated Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="travel.html"><a href="travel.html"><i class="fa fa-check"></i><b>3</b> Travel as Response</a>
<ul>
<li class="chapter" data-level="3.1" data-path="travel.html"><a href="travel.html#travel-variables"><i class="fa fa-check"></i><b>3.1</b> Travel Variables</a></li>
<li class="chapter" data-level="3.2" data-path="travel.html"><a href="travel.html#traveler-choice"><i class="fa fa-check"></i><b>3.2</b> Traveler Choice</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="travel.html"><a href="travel.html#rational-choice-theory"><i class="fa fa-check"></i><b>3.2.1</b> Rational Choice Theory</a></li>
<li class="chapter" data-level="3.2.2" data-path="travel.html"><a href="travel.html#bounded-rational-behavior"><i class="fa fa-check"></i><b>3.2.2</b> Bounded Rational Behavior</a></li>
<li class="chapter" data-level="3.2.3" data-path="travel.html"><a href="travel.html#theory-of-planned-behavior"><i class="fa fa-check"></i><b>3.2.3</b> Theory of Planned Behavior</a></li>
<li class="chapter" data-level="3.2.4" data-path="travel.html"><a href="travel.html#prospect-theory"><i class="fa fa-check"></i><b>3.2.4</b> Prospect Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="travel.html"><a href="travel.html#human-mobility"><i class="fa fa-check"></i><b>3.3</b> Human Mobility</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="travel.html"><a href="travel.html#distance-based-theories"><i class="fa fa-check"></i><b>3.3.1</b> Distance Based Theories</a></li>
<li class="chapter" data-level="3.3.2" data-path="travel.html"><a href="travel.html#opportunity-based-theories"><i class="fa fa-check"></i><b>3.3.2</b> Opportunity Based Theories</a></li>
<li class="chapter" data-level="3.3.3" data-path="travel.html"><a href="travel.html#time-geography"><i class="fa fa-check"></i><b>3.3.3</b> Time Geography</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="travel.html"><a href="travel.html#probability-distributions"><i class="fa fa-check"></i><b>3.4</b> Probability Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="struc.html"><a href="struc.html"><i class="fa fa-check"></i><b>4</b> Model Structures</a>
<ul>
<li class="chapter" data-level="4.1" data-path="struc.html"><a href="struc.html#multistage"><i class="fa fa-check"></i><b>4.1</b> Multistage</a></li>
<li class="chapter" data-level="4.2" data-path="struc.html"><a href="struc.html#decision-tree"><i class="fa fa-check"></i><b>4.2</b> Decision Tree</a></li>
<li class="chapter" data-level="4.3" data-path="struc.html"><a href="struc.html#multi-scales"><i class="fa fa-check"></i><b>4.3</b> Multi-scales</a></li>
<li class="chapter" data-level="4.4" data-path="struc.html"><a href="struc.html#other-structures"><i class="fa fa-check"></i><b>4.4</b> Other Structures</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary-i.html"><a href="summary-i.html"><i class="fa fa-check"></i><b>5</b> Summary I</a></li>
<li class="part"><span><b>Part II Statistical Methods</b></span></li>
<li class="chapter" data-level="6" data-path="common-methods.html"><a href="common-methods.html"><i class="fa fa-check"></i><b>6</b> Common Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="common-methods.html"><a href="common-methods.html#for-travel-distance"><i class="fa fa-check"></i><b>6.1</b> For Travel Distance</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="common-methods.html"><a href="common-methods.html#transformations"><i class="fa fa-check"></i><b>6.1.1</b> Transformations</a></li>
<li class="chapter" data-level="6.1.2" data-path="common-methods.html"><a href="common-methods.html#estimations"><i class="fa fa-check"></i><b>6.1.2</b> Estimations</a></li>
<li class="chapter" data-level="6.1.3" data-path="common-methods.html"><a href="common-methods.html#inference"><i class="fa fa-check"></i><b>6.1.3</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="common-methods.html"><a href="common-methods.html#for-tirp-generation"><i class="fa fa-check"></i><b>6.2</b> For Tirp Generation</a></li>
<li class="chapter" data-level="6.3" data-path="common-methods.html"><a href="common-methods.html#for-mode-choice"><i class="fa fa-check"></i><b>6.3</b> For Mode Choice</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="several-issues.html"><a href="several-issues.html"><i class="fa fa-check"></i><b>7</b> Several Issues</a>
<ul>
<li class="chapter" data-level="7.1" data-path="several-issues.html"><a href="several-issues.html#assumptions"><i class="fa fa-check"></i><b>7.1</b> Assumptions</a></li>
<li class="chapter" data-level="7.2" data-path="several-issues.html"><a href="several-issues.html#adequacy"><i class="fa fa-check"></i><b>7.2</b> Adequacy</a></li>
<li class="chapter" data-level="7.3" data-path="several-issues.html"><a href="several-issues.html#multicollinearity"><i class="fa fa-check"></i><b>7.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="7.4" data-path="several-issues.html"><a href="several-issues.html#variables-selections"><i class="fa fa-check"></i><b>7.4</b> Variables Selections</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="new-trends.html"><a href="new-trends.html"><i class="fa fa-check"></i><b>8</b> New Trends</a>
<ul>
<li class="chapter" data-level="8.1" data-path="new-trends.html"><a href="new-trends.html#controlling-spatial-effects"><i class="fa fa-check"></i><b>8.1</b> Controlling Spatial Effects</a></li>
<li class="chapter" data-level="8.2" data-path="new-trends.html"><a href="new-trends.html#capturing-non-linear-relationship"><i class="fa fa-check"></i><b>8.2</b> Capturing Non-Linear Relationship</a></li>
<li class="chapter" data-level="8.3" data-path="new-trends.html"><a href="new-trends.html#other-topics"><i class="fa fa-check"></i><b>8.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>9</b> Meta-Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="meta-analysis.html"><a href="meta-analysis.html#effect-sizes"><i class="fa fa-check"></i><b>9.1</b> Effect Sizes</a></li>
<li class="chapter" data-level="9.2" data-path="meta-analysis.html"><a href="meta-analysis.html#cross-study-heterogeneity"><i class="fa fa-check"></i><b>9.2</b> Cross-study Heterogeneity</a></li>
<li class="chapter" data-level="9.3" data-path="meta-analysis.html"><a href="meta-analysis.html#publication-bias"><i class="fa fa-check"></i><b>9.3</b> Publication Bias</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="meta-analysis.html"><a href="meta-analysis.html#standard-error-based-methods"><i class="fa fa-check"></i><b>9.3.1</b> Standard Error-based Methods</a></li>
<li class="chapter" data-level="9.3.2" data-path="meta-analysis.html"><a href="meta-analysis.html#p-value-based-methods"><i class="fa fa-check"></i><b>9.3.2</b> P value-based Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>10</b> Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Association Between Travel and Urban Form</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="several-issues" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Several Issues</h1>
<p>This chapter goes through several common issues in modeling and discuss the potential risks and remedies.
Ignoring these issues could lead to severe biased estimates or spurious relationships.
In a travel-urban form paper, the part of methodology usually includes data, variables, models, and results. Before publication, the researcher must have done a lot of work: trying any possible data sources, conducting variable selection and completing model validation.
These works often are not shown in the paper.
Therefore, the topics in this chapter are potential issues.
A suitable literature review or convincing criticism requires to gather the original data and replicate the models in the published paper.</p>
<div id="assumptions" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Assumptions</h2>
<ul>
<li>Additive and linearity</li>
</ul>
<p>For regression models, relationship between the explanatory variables should be additive.
The initial relationship could be multiplicative or exponential. log transform can covert the multiplicative relationship to additive <span class="citation">(<a href="#ref-choiAnalysisMetroRidership2012" role="doc-biblioref">Choi et al. 2012</a>)</span>.
The proper specification requires some theoretical and empirical supports.
Gravity Law discloses that travel distance has a multiplicative (inverse) relationship with the ‘masses’ of two places. The ‘masses’ related variable such as population size can be added to the regression model as a additive factor.</p>
<p>Some built environment and socioeconomic factors may have interaction effects. For example, the high density in developed countries have different effects on travel to the developing countries <span class="citation">(<a href="#ref-ewingDoesCompactDevelopment2017" role="doc-biblioref">Reid Ewing and Cervero 2017</a>)</span>.
Exploring the two-way or even higher order interaction effects is not common in travel-urban form studies.
<span class="citation"><a href="#ref-leeComparingImpactsLocal2020" role="doc-biblioref">S. Lee and Lee</a> (<a href="#ref-leeComparingImpactsLocal2020" role="doc-biblioref">2020</a>)</span> examine the interaction effects between population weighted density (PWD) and D-variables at tract level. Other possible paired interactions are not considered in this study.</p>
<p>Linearity means the function of independent variables is compatible with addition and scaling.
That is <span class="math inline">\(f(x+y)=f(x)+f(y)\)</span> and <span class="math inline">\(f(a\cdot x)=a\cdot f(x)\)</span>.
When the assumption of linearity still doesn’t hold after transformation, the study should look for other non-linear models and corresponding approaches.
Does the regression models show the linear property after log transformation?
There are more recent studies start to pay attention to this topic <span class="citation">(<a href="#ref-ewingReducingVehicleMiles2020" role="doc-biblioref">Reid Ewing et al. 2020</a>; <a href="#ref-dingNonlinearAssociationsZonal2021" role="doc-biblioref">Ding et al. 2021</a>)</span>. More discussion of the non-linear relationship is placed in the next chapter.</p>
<ul>
<li>Independent Identically Distributed (IID)</li>
</ul>
<p>Another essential assumption is that random error are Independent Identically Distributed (IID). Random error is also called residual, which refer to the difference between observed <span class="math inline">\(\mathbf{y}\)</span> and fitted <span class="math inline">\(\mathbf{\hat y}\)</span>.
That is <span class="math inline">\(\mathbf{e}=\mathbf{y}-\mathbf{\hat y}\)</span>, where <span class="math inline">\(\mathbf{\hat y}\)</span> are the linear combinations of predictors <span class="math inline">\(\mathbf{X}\)</span>. Residuals represent the part can not be explained by the model.</p>
<!-- \begin{equation} -->
<!-- \mathbf{e}=\mathbf{y}-\mathbf{\hat y} -->
<!-- (\#eq:residual) -->
<!-- \end{equation} -->
<p>‘Identical’ means that random errors should have zero mean and constant variance. The expected value, the variances, and the covariances among the random errors are the first- and second-moment of residuals.
That is <span class="math inline">\(E(\varepsilon) = 0\)</span> and <span class="math inline">\(Var(\varepsilon) = \sigma^2\)</span>.
The homogeneity of variance is also called homoscedasticity.
To satisfy this assumption, some studies chose a subset such as VMT by nonwork purposes <span class="citation">(<a href="#ref-chatmanDeconstructingDevelopmentDensity2008" role="doc-biblioref">Chatman 2008</a>)</span>, bus ridership by time of day, or school children’s metro ridership <span class="citation">(<a href="#ref-liuInfluenceBuiltEnvironment2018" role="doc-biblioref">Liu et al. 2018</a>)</span>. So it also depends on the research design.</p>
<!-- \begin{equation} -->
<!-- E(\varepsilon) = 0, \quad Var(\varepsilon) = \sigma^2 -->
<!-- (\#eq:identical) -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- Cov[\varepsilon_i,\varepsilon_j] = 0,\quad i\neq j -->
<!-- (\#eq:indenpendent) -->
<!-- \end{equation} -->
<p>‘Independent’ requires the random errors are uncorrelated. That is <span class="math inline">\(Cov[\varepsilon_i,\varepsilon_j] = 0\)</span>,<span class="math inline">\(i\neq j\)</span>
In the stage of survey design, researchers try to collect the data by random sampling.
The studies using secondary data usually believe every observations are independent.
But in travel-urban form studies, many dataset cover all the neighbored units in a region. In this situation, the independent assumption often doesn’t hold.</p>
<ul>
<li>Normality</li>
</ul>
<p>The common words in travel-urban form literature are that “We use logarithm transform on travel variables to address the issues of normality and linearity.”
Evidence has demonstrated that travel distance and frequency are not Normal distributed. The Zipf’s law also prove that travel distance follows a power distribution. Using logarithm transformations, the skewed distribution can be converted to an approximate normal distribution.
Meanwhile, some scholars assume the observed travel data are left censored and choose Tobit regression <span class="citation">(<a href="#ref-chatmanHowDensityMixed2003" role="doc-biblioref">Chatman 2003</a>; <a href="#ref-boarnetWalkingUrbanDesign2008" role="doc-biblioref">M. G. Boarnet, Greenwald, and McMillan 2008</a>)</span>.</p>
<p>Actually, the normality requirement for response is a misunderstanding. Neither response variable nor predictor is required to be normal.
Normality is the requirement for residuals.
Note that least squares method itself needs zero mean and constant variance rather than normality.
When conducting hypothesis test and infering confidence intervals, the required assumption is that the response conditional on covariates is normal, that is <span class="math inline">\(\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})\)</span>. Maximum Likelihood Methods also requires this assumption.
There are some quantitative methods which can examine normality of the transformed distributions.</p>
<ul>
<li>A brief Discussion</li>
</ul>
<p>In urban studies, the geographic data often have both independent and identical issues, which are called the spatial autocorrelation and heterogeneity <span class="citation">(<a href="#ref-zhangEvaluationSpatialAutocorrelation2009" role="doc-biblioref">Lianjun Zhang, Ma, and Guo 2009</a>)</span>.
That means, the observation in one location be affected by the neighbored areas for some unknown reasons. And the observations from city to city or region to region are generated by distinct mechanisms.
Once the conditions of IID are satisfied, the Gauss - Markov theorem proves that least-square method could give the minimum-variance unbiased estimators (MVUE) or called the best linear unbiased estimators (BLUE). These conditions are not strict and make regression method widely applicable.</p>
<p>When the sample size is small, the circumstance of violating normality assumption would make the inference misleading.
As <span class="citation"><a href="#ref-lumleyImportanceNormalityAssumption2002" role="doc-biblioref">Lumley et al.</a> (<a href="#ref-lumleyImportanceNormalityAssumption2002" role="doc-biblioref">2002</a>)</span> pointed out, if the sample size is large enough such as <span class="math inline">\(n\ge 100\)</span>, the t-test and OLS still can give the asymptotically unbiased estimates of mean. But for some long-tailed data, median regressions might be appropriate, that also depends on the research question to be asked.
Few paper in travel-urban form literature show their normality test in the published version and investigate the influences.</p>
</div>
<div id="adequacy" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Adequacy</h2>
<p>The estimation and inference themselves can not demonstrate a model’s performance.
If the primary assumptions is violated, the estimations could be biased and the model could be misleading. These problems can also happen when the model is not correctly specified. Making a proper diagnosis and validation are the first thing we need to do when fitting some models.</p>
<!-- "There are different types of model specification errors in regression analysis: " -->
<!-- (1) necessary independent variable(s) is (are) missing from the model (i.e., omission error) (Greene 2012, 96--8, Section 4.3.2); -->
<!-- (2) irrelevant independent variable(s) is (are) added to the model (i.e., commission error) (Greene 2012, 98, Section 4.3.3); -->
<!-- (3) independent variables have an incorrect functional form (Ramsey 1969; Thursby and Schmidt 1977; Hausman 1978; Davidson and MacKinnon 1981); -->
<!-- (4) variables in the model are not accurately measured (i.e., measurement error) (Wooldridge 2015, 287--92, Section 9.4); and -->
<!-- (5) the model is not stand-alone but instead belongs to a system of simultaneous equations (Ramsey 1969; Hausman 1978). -->
<ul>
<li>Residuals Analysis</li>
</ul>
<p>The major assumptions, both IID and normality are related to residual.
Residual diagnosis is an essential step for modeling validation.
There are several scaled residuals can help the diagnosis.
Since <span class="math inline">\(MSE\)</span> is the expected variance of error <span class="math inline">\(\hat\sigma^2\)</span> and <span class="math inline">\(E[\varepsilon]=0\)</span>, standardized residuals (<span class="math inline">\(d_i=\frac{e_i}{\sqrt{MSE}}=e_i\sqrt{\frac{n-p}{\sum_{i=1}^n e_i^2}}\)</span>, <span class="math inline">\(i=1,2,...,n\)</span>) should follow a standard normal distribution.</p>
<p>Recall random error <span class="math inline">\(\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=(\mathbf{I}-\mathbf{H})\mathbf{y}\)</span> and hat matrix <span class="math inline">\(\mathbf{H}=\mathbf{X}(\mathbf{X&#39;X})^{-1}\mathbf{X&#39;}\)</span>.
Let <span class="math inline">\(h_{ii}\)</span> denote the <span class="math inline">\(i\)</span>th diagonal element of hat matrix.
Studentized Residuals can be expressed by <span class="math inline">\(r_i=\frac{e_i}{\sqrt{MSE(1-h_{ii})}}\)</span>, <span class="math inline">\(i=1,2,...,n\)</span>.
It is proved that <span class="math inline">\(0\le h_{ii}\le1\)</span>.
An observation with <span class="math inline">\(h_{ii}\)</span> closed to one will return a large value of <span class="math inline">\(r_i\)</span>. The <span class="math inline">\(x_i\)</span> who has strong influence on fitted value is called leverage point.
Ideally, the scaled residual have zero mean and unit variance. Hence, an observation with <span class="math inline">\(|d_i|&gt;3\)</span> or <span class="math inline">\(|r_i|&gt;3\)</span> is a potential outlier.</p>
<p>Predicted Residual Error Sum of Squares (PRESS) can also be used to detect outliers.
This method predicts the <span class="math inline">\(i^{th}\)</span> fitted response by excluding the <span class="math inline">\(i^{th}\)</span> observation and examine the influence of this point.
The corresponding error <span class="math inline">\(e_{(i)}=e_{i}/(1-h_{ii})\)</span> and <span class="math inline">\(V[e_{(i)}]=\sigma^2/(1-h_{ii})\)</span>.
Thus, if <span class="math inline">\(MSE\)</span> is a good estimate of <span class="math inline">\(\sigma^2\)</span>, PRESS residuals is equivalent to Studentized Residuals.
<span class="math inline">\(\frac{e_{(i)}}{\sqrt{V[e_{(i)}]}}=\frac{e_i/(1-h_{ii})}{\sqrt{\sigma^2/(1-h_{ii})}}=\frac{e_i}{\sqrt{\sigma^2(1-h_{ii})}}\)</span>.</p>
<p>Residual plot can show the pattern of the residuals against fitted <span class="math inline">\(\mathbf{\hat y}\)</span>.
If the assumptions are valid, the shape of points should like a envelope and be evenly distributed around the horizontal line of <span class="math inline">\(e=0\)</span> (Figure <a href="several-issues.html#fig:resid-plot">7.1</a> left panel).
A funnel shape in residual plot shows that the variance of error is a function of <span class="math inline">\(\hat y\)</span> (Figure <a href="several-issues.html#fig:resid-plot">7.1</a> right panel). A suitable transformation to response or predictor could stabilize the variance.
A curved shape means the assumption of linearity is not valid (Figure <a href="several-issues.html#fig:resid-plot">7.1</a> middel panel). It implies that adding quadratic terms or higher-order terms might be suitable.
Residual plot is an essential tool for regression model diagnosis. But it is rarely seen travel-urban form literature.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resid-plot"></span>
<img src="field_paper_files/figure-html/resid-plot-1.png" alt="Three examples of Residual plot" width="100%" />
<p class="caption">
Figure 7.1: Three examples of Residual plot
</p>
</div>
<p>A histogram of residuals can check the normality assumption. In the histogram, probability distribution of VMT is usually highly right-skewed. While the log-transformed VMT is more close to a bell curve.
A better way is a normal quantile – quantile (QQ) plot of the residuals.
An ideal cumulative normal distribution should plot as a straight line.
Only looking at the <span class="math inline">\(R^2\)</span> and p-values cannot disclose this feature.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qq-plot"></span>
<img src="field_paper_files/figure-html/qq-plot-1.png" alt="Three examples of Normal Q-Q Plot" width="100%" />
<p class="caption">
Figure 7.2: Three examples of Normal Q-Q Plot
</p>
</div>
<ul>
<li>Goodness of fit</li>
</ul>
<p>Coefficient of Determination <span class="math inline">\(R^2\)</span> is a proportion to assess the quality of fitted model. This measurement tell us how good the model can explain the data.</p>
<p><span class="math display" id="eq:rsq">\[\begin{equation}
R^2 =\frac{SSR}{SST}=1-\frac{SSE}{SST}
\tag{7.1}
\end{equation}\]</span></p>
<p>when <span class="math inline">\(R^2\)</span> is close to <span class="math inline">\(1\)</span>, the most of variation in response can be explained by the fitted model. Although <span class="math inline">\(R^2\)</span> is not the only criteria of a good model, it is often available in most published papers. Recall the discussion in Part I, the aggregated data will eliminate the difference among individuals, households, or neighborhoods. In the new variance structure, <span class="math inline">\(SSE\)</span> will be much less than disaggregated model. The <span class="math inline">\(R^2\)</span> in many disaggregate studies are around 0.3, while the <span class="math inline">\(R^2\)</span> in some aggregate studies can reach 0.8. A seriously underfitting model’s outputs could be biased and unstable.</p>
<p>A fact is that adding predictors into the model will never decrease <span class="math inline">\(R^2\)</span>.
If a VMT-urban form model added many predictors but adjusted <span class="math inline">\(R^2\)</span> is still low, the association between travel distance and built environment might be spurious.
<span class="math inline">\(R^2\)</span> also cannot reflect the different number of parameters in the models. Adjusted <span class="math inline">\(R^2\)</span> can address this issue by introducing degree of freedom. The degree of freedom denotes the amount of information required to know.</p>
<p><span class="math display" id="eq:df">\[\begin{equation}
\begin{split}
df_T =&amp; df_R + df_E\\
n-1=&amp;(p-1)+(n-p)
\end{split}
\tag{7.2}
\end{equation}\]</span></p>
<p>Then, the mean square (MS) of each sum of squares (SS) can be calculated by <span class="math inline">\(MS=SS/df\)</span>. The mean square error <span class="math inline">\(MSE\)</span> is also called as the expected value of error variance <span class="math inline">\(\hat\sigma^2=MSE=SSE/(n-p)\)</span>. <span class="math inline">\(n-p\)</span> is the degree of freedom. Then adjusted <span class="math inline">\(R^2\)</span> is</p>
<p><span class="math display" id="eq:rsq-adj">\[\begin{equation}
R_{adj}^2 = 1-\frac{MSE}{MST} = 1-\frac{SSE/(n-p)}{SST/(n-1)}
\tag{7.3}
\end{equation}\]</span></p>
<p>Another similar method is <span class="math inline">\(R^2\)</span> for prediction based on PRESS.
Recall the PRESS statistic is the prediction error sum of square by fitting a model with <span class="math inline">\(n-1\)</span> observations.</p>
<p><span class="math display" id="eq:press">\[\begin{equation}
PRESS = \sum_{i=1}^n(y_i-\hat y_{(i)})^2= \sum_{i=1}^n\left(\frac{e_i}{1-h_{ii}}\right)^2
\tag{7.4}
\end{equation}\]</span></p>
<p>A model with smaller PRESS has a better ability of prediction. The <span class="math inline">\(R^2\)</span> for prediction is</p>
<p><span class="math display" id="eq:rsq-pred">\[\begin{equation}
R_{pred}^2 = 1-\frac{PRESS}{MST}
\tag{7.5}
\end{equation}\]</span></p>
<ul>
<li>Heterogeneity and Autocorrelation</li>
</ul>
<p>For spatio-temporal data, the observations often have some relationship over time or space.
When the assumption of constant variance is violated, the linear model has the issue of heterogeneity.
When the assumption of independent errors is violated, the linear model with serially correlated errors is called autocorrelation.
Spatial heterogeneity and spatial autocorrelation are two typical phenomenon in urban studies. All the neighboring geographic entities or stages could impact each other, or sharing the similar environment.
Failing to deal with spatial heterogeneity could produce fake significance in hypothesis test and lead to systematically biased estimates.
Although the estimation of coefficients could be unbiased when there is spatial autocorrelation in regression models, the estimated error variance would be biased and make misleading significance tests <span class="citation">(<a href="#ref-zhangEvaluationSpatialAutocorrelation2009" role="doc-biblioref">Lianjun Zhang, Ma, and Guo 2009</a>)</span>.</p>
<p>Here is a simple case of heterogeneous model.
Recall Generalized least square estimates
<!-- \@ref(eq:glsq-e) and \@ref(eq:glsq-v) -->
, if the residuals are independent but variances are not constant, a simple linear model becomes <span class="math inline">\(\boldsymbol{\varepsilon}\sim MVN(\mathbf{0},\sigma^2\mathbf{V})\)</span> where <span class="math inline">\(\mathbf{V}\)</span> is a diagonal matrix with <span class="math inline">\(v_{ii}=x^2_i\)</span>.
Then <span class="math inline">\(\mathbf{X&#39;V^{-1}X}=n\)</span> and the weighted least squares solution is <span class="math inline">\(\hat\beta_{1,WLS}=\frac1n\sum_{i=1}^{n}\frac{y_i}{x_i}\)</span> and <span class="math inline">\(\hat\sigma^2_{WLS}=\frac1{n-1}\sum_{i=1}^{n}\frac{(y_i-\hat\beta_{1}x_i)^2}{x_i^2}\)</span>.
In this case, the OLS estimates of coefficients are still unbiased but no longer efficient.
The estimates of variances are biased. The corresponding hypothesis test and confidence interval would be misleading.</p>
<!-- \begin{equation} -->
<!-- \mathbf{V}=\begin{bmatrix}  -->
<!-- x_1^2 & 0 & \dots & 0 \\   -->
<!-- 0 & x_2^2 & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & x_n^2 \end{bmatrix},\quad  -->
<!-- \mathbf{V}^{-1}=\begin{bmatrix}  -->
<!-- \frac1{x_1^2} & 0 & \dots & 0 \\   -->
<!-- 0 & \frac1{x_2^2} & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & \frac1{x_n^2} \end{bmatrix} -->
<!-- (\#eq:hete-matrix) -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- \hat\beta_{1,WLS}=\frac1n\sum_{i=1}^{n}\frac{y_i}{x_i} -->
<!-- (\#eq:hete-e) -->
<!-- \end{equation} -->
<!-- and -->
<!-- \begin{equation} -->
<!-- \hat\sigma^2_{WLS}=\frac1{n-1}\sum_{i=1}^{n}\frac{(y_i-\hat\beta_{1}x_i)^2}{x_i^2} -->
<!-- (\#eq:hete-v) -->
<!-- \end{equation} -->
<p>If the data is aggregated to a upper level, it is the cases of geographic modifiable areal unit problem (MAUP) discussed in previous chapter.
Let <span class="math inline">\(u_j\)</span> and <span class="math inline">\(v_j\)</span> are the response and predictors of <span class="math inline">\(j\)</span>th household in a neighborhood. <span class="math inline">\(n_i\)</span> is the sample size in each neighborhood. Then <span class="math inline">\(y_i=\sum_{j=1}^{n_i}u_j/n_i\)</span> and <span class="math inline">\(X_i=\sum_{j=1}^{n_i}v_j/n_i\)</span>.
Then <span class="math inline">\(\mathbf{X&#39;V^{-1}X}=\sum_{i=1}^nn_ix_i^2\)</span> and the WLS estimate of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat\beta_{1,WLS}=\frac1n\frac{\sum_{i=1}^{n}n_ix_iy_i}{\sum_{i=1}^{n}n_ix_i^2}\)</span> and <span class="math inline">\(V[\hat\beta_{1,WLS}]=\frac{\sigma^2}{\sum_{i=1}^{n}n_ix_i^2}\)</span>.
There are three procedures, Bartlett’s likelihood ratio test, Goldfeld-Quandt test, or Breusch-Pagan test which can be used to examine heterogeneity <span class="citation">(<a href="#ref-ravishankerFirstCourseLinear2020" role="doc-biblioref">Ravishanker and Dey 2020, 8.1.3, pp.288–290</a>)</span></p>
<!-- In this case, -->
<!-- \begin{equation} -->
<!-- \mathbf{V}=\begin{bmatrix}  -->
<!-- \frac1{n_1} & 0 & \dots & 0 \\   -->
<!-- 0 & \frac1{n_2} & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & \frac1{n_n} \end{bmatrix},\quad  -->
<!-- \mathbf{V}^{-1}=\begin{bmatrix}  -->
<!-- n_1 & 0 & \dots & 0 \\   -->
<!-- 0 & n_2 & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & n_n \end{bmatrix} -->
<!-- (\#eq:agg-matrix) -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- \hat\beta_{1,WLS}=\frac1n\frac{\sum_{i=1}^{n}n_ix_iy_i}{\sum_{i=1}^{n}n_ix_i^2} -->
<!-- (\#eq:agg-e) -->
<!-- \end{equation} -->
<!-- and -->
<!-- \begin{equation} -->
<!-- V[\hat\beta_{1,WLS}]=\frac{V[\sum_{i=1}^{n}n_ix_iy_i]}{(\sum_{i=1}^{n}n_ix_i^2)^2}=\frac{\sum_{i=1}^{n}n_i^2x_i^2\sigma^2/n_i}{(\sum_{i=1}^{n}n_ix_i^2)^2}=\frac{\sigma^2}{\sum_{i=1}^{n}n_ix_i^2} -->
<!-- (\#eq:agg-v) -->
<!-- \end{equation} -->
<p>Take a case of single dimension autocorrelation for example, it assumes the model have constant variance. That is <span class="math inline">\(E[\varepsilon]=0\)</span>.
But <span class="math inline">\(Cov[\varepsilon_i,\varepsilon_j]=\sigma^2\rho^{|j-i|}\)</span>, <span class="math inline">\(i,j=1,2,...,n\)</span> and <span class="math inline">\(|\rho|&lt;1\)</span>
This is a linear regression with autoregressive order 1 (AR(1)).
The estimates of <span class="math inline">\(\boldsymbol{\hat\beta}\)</span> is the same with the GLS solutions, which are
<span class="math inline">\(\boldsymbol{\hat\beta}_{GLS}=(\mathbf{X&#39;V^{-1}X})^{-1}\mathbf{X&#39;V^{-1}}\mathbf{y}\)</span> and
<span class="math inline">\(\widehat{V[\boldsymbol{\hat\beta}_{GLS}]}=\hat\sigma^2_{GLS}(\mathbf{X&#39;V^{-1}X})^{-1}\)</span>,
where <span class="math inline">\(\hat\sigma^2_{GLS}=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})&#39;\mathbf{V^{-1}}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})\)</span>.
The variance-covariance matrix <span class="math inline">\(\mathbf{V}\)</span> is also called Toeplitz matrix.</p>
<!-- \begin{equation} -->
<!-- \mathbf{V}=\begin{bmatrix}  -->
<!-- 1 & \rho & \rho^2 & \dots & \rho^{n-1} \\   -->
<!-- \rho & 1 & \rho & \dots & \rho^{n-2} \\   -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots \\   -->
<!-- \rho^{n-1} & \rho^{n-2} & \rho^{n-3} & \dots & 1 \end{bmatrix},\quad  -->
<!-- \{\mathbf{V}^{-1}\}_{ij}=\begin{cases}  -->
<!-- \frac{1}{1-\rho^2} & \text{if } i=j=1,n \\   -->
<!-- \frac{1+\rho^2}{1-\rho^2} & \text{if } i=j=2,...,n-1 \\   -->
<!-- \frac{-\rho}{1-\rho^2} & \text{if } |j-i|=1 \\   -->
<!-- 0  & \text{otherwise} \end{cases} -->
<!-- (\#eq:auto-matrix) -->
<!-- \end{equation} -->
<p>It is can be verified that <span class="math inline">\(\boldsymbol{\hat\beta}_{GLS}\le\boldsymbol{\hat\beta}_{OLS}\)</span> always holds and they are equal when <span class="math inline">\(\mathbf{V=I}\)</span> or <span class="math inline">\(\rho=0\)</span>. It proves that <span class="math inline">\(\boldsymbol{\hat\beta}_{GLS}\)</span> are the best linear unbiased estimators (BLUE).
This case can be extended to miltiple regression models and the autocorrelation of a stationary stochastic process at lag-k.
Durbin-Watson test is used to test the null hypothesis of <span class="math inline">\(\rho=0\)</span>.</p>
<ul>
<li>A brief discussion</li>
</ul>
<p>Only reporting the <span class="math inline">\(R^2\)</span> and p-values cannot tell us whether the model is valid or not.
Residual analysis can detect the ill-condition models and should not be ignored in regression studies.</p>
<p>For non-OLS model, such as logistic regression, <span class="math inline">\(R^2\)</span> doesn’t exist. Pseudo <span class="math inline">\(R^2\)</span>s are used to evaluate the goodness-of-fit. McFadden’s <span class="math inline">\(R^2\)</span>, Count <span class="math inline">\(R^2\)</span>, Efron’s <span class="math inline">\(R^2\)</span> are some common Pseudo <span class="math inline">\(R^2\)</span> with different interpretations. For example McFadden’s <span class="math inline">\(R^2\)</span> use the ratio of the log likelihood to reflect how better is the full model than the null model. When examining the effects of built-environment factors on VMT at multiple spatial levels, <span class="citation"><a href="#ref-hongHowBuiltenvironmentFactors2014" role="doc-biblioref">Hong, Shen, and Zhang</a> (<a href="#ref-hongHowBuiltenvironmentFactors2014" role="doc-biblioref">2014</a>)</span> use a Bayesian version of adjusted <span class="math inline">\(R^2\)</span> for multilevel models which proposed by <span class="citation"><a href="#ref-gelmanBayesianMeasuresExplained2006" role="doc-biblioref">Gelman and Pardoe</a> (<a href="#ref-gelmanBayesianMeasuresExplained2006" role="doc-biblioref">2006</a>)</span>.
Therefore, cross comparing <span class="math inline">\(R^2\)</span> and various Pseudo <span class="math inline">\(R^2\)</span> is meaningless. Pseudo <span class="math inline">\(R^2\)</span>s on different data are also not comparable.
Next question is what is a satisfactory model fit for different categories.</p>
<p>Some techniques can deal with spatial heterogeneity and autocorrelation and improve the model performance. More discussions of addressing spatial effects are placed on the next chapter.</p>
</div>
<div id="multicollinearity" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Multicollinearity</h2>
<p>Multicollinearity or near-linear dependence refers to the models with highly correlated predictors. When data is generated from experimental design, the treatments <span class="math inline">\(X\)</span> could be fixed variables and be orthogonal. But travel-urban form model is observational studies and nothing can be controlled as in lab. It is known that there are complex correlations among the built-environment predictors themselves.
Although, the basic IID assumptions do not require that all predictors <span class="math inline">\(\mathbf{X}\)</span> are independent, when the predictors are near-linear dependent, the model is ill-conditioned and the least-square estimators are unstable.</p>
<ul>
<li>Variance Inflation</li>
</ul>
<p>Multicollinearity can make the variances inflated and impact model precision seriously. If some of predictors are exact linear dependent, the matrix <span class="math inline">\((\mathbf{X&#39;X})^{-1}\)</span> is symmetric but non-invertible. By spectral decomposition of symmetric matrix, <span class="math inline">\(\mathbf{X&#39;X}=\mathbf{P&#39;\Lambda P}\)</span> where <span class="math inline">\(\Lambda=\text{diag}(\lambda_1,...,\lambda_p)\)</span>, <span class="math inline">\(\lambda_i\)</span>’s are eigenvalues of <span class="math inline">\(\mathbf{X&#39;X}\)</span>, <span class="math inline">\(\mathbf{P}\)</span> is an orthogonla matrix whose columns are normalize eigenvectors. Then the total-variance of <span class="math inline">\(\boldsymbol{\hat\beta}_{LS}\)</span> is <span class="math inline">\(\sigma^2\sum_{j=1}^p1/\lambda_j\)</span>.
If the predictors are near-linear dependent or nearly singular, <span class="math inline">\(\lambda_j\)</span>s may be very small and the total-variance of <span class="math inline">\(\boldsymbol{\hat\beta}_{LS}\)</span> is highly inflated.
For the same reason, the correlation matrix using unit length scaling <span class="math inline">\(\mathbf{Z&#39;Z}\)</span> will has a inverse matrix with inflated variances. That means that the diagonal elements of <span class="math inline">\((\mathbf{Z&#39;Z})^{-1}\)</span> are not all equal to one. The diagonal elements are called <strong>Variance Inflation Factors</strong>, which can be used to examine multicollinearity. The VIF for a particular predictor is examined by <span class="math inline">\(\mathrm{VIF}_j=\frac{1}{1-R_j^2}\)</span>,
where <span class="math inline">\(R_j^2\)</span> is the coefficient of determination by regressing <span class="math inline">\(x_j\)</span> on all the remaining predictors.</p>
<!-- \begin{equation} -->
<!-- \mathrm{VIF}_j=\frac{1}{1-R_j^2} -->
<!-- (\#eq:vif) -->
<!-- \end{equation} -->
<p>A common approach is to drop off the predictor with greatest VIF and refit the model until all VIFs are less than 10. However, dropping off one or more predictors will lose many information which might be valuable for explaining response. Due to the complexity among predictors, dropping off the predictor with the greatest VIF is not always the best choice. Sometimes, removing a predictor with moderate VIF can make all VIFs less than 10 in the refitted model. Moreover, there is not an unique criteria for VIF value. When the relationship between predictor and response is weak, or the <span class="math inline">\(R^2\)</span> is low, the VIFs less than 10 may also affect the ability of estimation dramatically.
Orthogonalization before fitting the model might be helpful. Other approaches such as principal components regression, ridge regression, etc. could deal with multicollinearity better.</p>
<ul>
<li>Principal Components Regression</li>
</ul>
<p>Principal Components Regression (PCR) is a dimension reduction method which projecting the original predictors into a lower-dimension space.
It still uses a singular value decomposition (SVD) and get <span class="math inline">\(\mathbf{X&#39;X}=\mathbf{Q\Lambda Q}&#39;\)</span>
<span class="math inline">\(\mathbf{Q}\)</span> are the matrix who columns are orthogonal eigenvectors of <span class="math inline">\(\mathbf{X&#39;X}\)</span>. <span class="math inline">\(\Lambda=\text{diag}(\lambda_1,...,\lambda_p)\)</span> is decreasing eigenvalues with <span class="math inline">\(\lambda_1\ge\lambda_1\ge\cdots\ge\lambda_p\)</span>. Then the linear model can transfer to
<span class="math inline">\(\mathbf{y} = \mathbf{XQQ}&#39;\boldsymbol\beta + \varepsilon = \mathbf{Z}\boldsymbol\theta + \varepsilon\)</span>,
where <span class="math inline">\(\mathbf{Z}=\mathbf{XQ}\)</span>, <span class="math inline">\(\boldsymbol\theta=\mathbf{Q}&#39;\boldsymbol\beta\)</span>.
<span class="math inline">\(\boldsymbol\theta\)</span> is called the regression parameters of the principal components.
<span class="math inline">\(\mathbf{Z}=\{\mathbf{z}_1,...,\mathbf{z}_p\}\)</span> is known as the matrix of principal components of <span class="math inline">\(\mathbf{X&#39;X}\)</span>.
Then <span class="math inline">\(\mathbf{z}&#39;_j\mathbf{z}_j=\lambda_j\)</span> is the <span class="math inline">\(j\)</span>th largest eigenvalue of <span class="math inline">\(\mathbf{X&#39;X}\)</span>.
PCR usually chooses several <span class="math inline">\(\mathbf{z}_j\)</span>s with largest <span class="math inline">\(\lambda_j\)</span>s and can eliminate multicollinearity.
Its estimates <span class="math inline">\(\boldsymbol{\hat\beta}_{P}\)</span> results in low bias but the mean squared error <span class="math inline">\(MSE(\boldsymbol{\hat\beta}_{P})\)</span> is smaller than that of least square <span class="math inline">\(MSE(\boldsymbol{\hat\beta}_{LS})\)</span>.</p>
<ul>
<li>Ridge Regression</li>
</ul>
<p>Least squares method gives the unbiased estimates of regression coefficients.
However, multicollinearity will lead to inflated variance and make the estimates unstable and unreliable.
To get a smaller variance, a tradeoff is to release the requirement of unbiasedness.
<span class="citation"><a href="#ref-hoerlRidgeRegressionBiased1970" role="doc-biblioref">Hoerl and Kennard</a> (<a href="#ref-hoerlRidgeRegressionBiased1970" role="doc-biblioref">1970</a>)</span> proposed ridge regression to address the nonorthogonal problems.
The estimates of ridge regression are <span class="math inline">\(\boldsymbol{\hat\beta}_{R}=(\mathbf{X&#39;X}+k\mathbf{I})^{-1}\mathbf{X&#39;}\mathbf{y}\)</span>,
where <span class="math inline">\(k\ge0\)</span> is a selected constant and is called a biasing parameter. When <span class="math inline">\(k=0\)</span>, the ridge estimator reduces to least squares estimators.</p>
<!-- \begin{equation} -->
<!-- \boldsymbol{\hat\beta}_{R}=(\mathbf{X'X}+k\mathbf{I})^{-1}\mathbf{X'}\mathbf{y} -->
<!-- (\#eq:ridge-e) -->
<!-- \end{equation} -->
<!-- Denote $\boldsymbol{\hat\beta}_{R}$ are biased estimates but its variance is small enough. -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathrm{MSE}(\boldsymbol{\hat\beta}_{R})&=E[\boldsymbol{\hat\beta}_{R}-\boldsymbol{\beta}]^2=\mathrm{Var}[\boldsymbol{\hat\beta}_{R}]+\mathrm{Bias}[\boldsymbol{\hat\beta}_{R}]^2\\ -->
<!-- &<\mathrm{MSE}(\boldsymbol{\hat\beta}_{LS})=\mathrm{Var}[\boldsymbol{\hat\beta}_{LS}] -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- When $\mathbf{X}$ is nonsingular and $(\mathbf{X'X})^{-1}$ exists, the ridge estimator is a linear transformation of $\boldsymbol{\hat\beta}_{LS}$. That is $\boldsymbol{\hat\beta}_{R}=\mathbf{Z}_k\boldsymbol{\hat\beta}_{LS}$ where $\mathbf{Z}_k=(\mathbf{X'X}+k\mathbf{I})^{-1}\mathbf{X'X}$ -->
<!-- Recall the total-variance of $\boldsymbol{\hat\beta}_{LS}$ is $\sigma^2\sum_{j=1}^p1/\lambda_j$. -->
<!-- The total-variance of $\boldsymbol{\hat\beta}_{R}$ is  -->
<!-- \begin{equation} -->
<!-- \mathrm{tr}(\mathrm{Cov}[\boldsymbol{\hat\beta}_{R}])=\sigma^2\sum_{j=1}^p\frac{\lambda_j}{(\lambda_j+k)^2} -->
<!-- \end{equation} -->
<!-- Thus, introducing $k$ into the model can avoid tiny denominators and eliminate the inflated variance. -->
<!-- Choosing a proper value of $k$ is to keep the balance of $\mathrm{MSE}$ and $\mathrm{Bias}$. -->
<!-- The bias in $\boldsymbol{\hat\beta}_{R}$ is  -->
<!-- \begin{equation} -->
<!-- \mathrm{Bias}(\boldsymbol{\hat\beta}_{R})^2=k^2\boldsymbol{\beta}'(\mathbf{X'X}+k\mathbf{I})^{-2}\boldsymbol{\beta} -->
<!-- \end{equation} -->
<!-- Hence,increasing $k$ will reduce $MSE$ but make greater $bias$. -->
<!-- Ridge trace is a plot of $\boldsymbol{\hat\beta}_{R}$ versus $k$ that can help to select a suitable value of $k$. -->
<!-- First, at the value of $k$, the estimates should be stable. Second, the estimated coefficients should have proper sign and reasonable values. Third, the $SSE$ also should has a reasonable value. -->
<!-- Ridge regression will not give a greater $R^2$ than least squares method. Because the total sum of squares is fixed. -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathrm{SSE}(\boldsymbol{\hat\beta}_{R})&=(\mathbf{y-X}\boldsymbol{\hat\beta}_{R})'(\mathbf{y-X}\boldsymbol{\hat\beta}_{R})\\ -->
<!-- &=(\mathbf{y-X}\boldsymbol{\hat\beta}_{LS})'(\mathbf{y-X}\boldsymbol{\hat\beta}_{LS})+(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})'\mathbf{X'X}(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})\\ -->
<!-- &=\mathrm{SSE}(\boldsymbol{\hat\beta}_{LS})+(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})'\mathbf{X'X}(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})\\ -->
<!-- &\ge \mathrm{SSE}(\boldsymbol{\hat\beta}_{LS}) -->
<!-- \end{split} -->
<!-- \end{equation} -->
<p>The advantage of ridge regression is to obtain a suitable set of parameter estimates rather than to improve the fitness. It could have a better prediction ability than least squares.
It can also be useful for variable selection. The variables with unstable ridge trace or tending toward the value of zero can be removed from the model.
In many case, the ridge trace is erratic divergence and may revert back to least square estimates.
<span class="citation">(<a href="#ref-jensenSurrogateModelsIllconditioned2010a" role="doc-biblioref">Jensen and Ramirez 2010</a>, <a href="#ref-jensenVariationsRidgeTraces2012" role="doc-biblioref">2012</a>)</span> proposed surrogate model to further improve ridge regression. Surrogate model chooses <span class="math inline">\(k\)</span> depend on matrix <span class="math inline">\(\mathbf{X}\)</span> and free to <span class="math inline">\(\mathbf{Y}\)</span>.</p>
<!-- Using a compact singular value decomposition (SVD), the original can be decomposed to maxtix$\mathbf{X}=\mathbf{PD_{\xi}Q}'$. $\mathbf{P}$ and $\mathbf{Q}$ are orthogonal. The columns of $\mathbf{P}$ and $\mathbf{Q}$ are left-singular vectors and right-singular vectors of $\mathbf{X}$. -->
<!-- It satisfies $\mathbf{P'P}=\mathbf{I}$ and $\mathbf{D}_{\xi}=\text{diag}(\xi_1,...,\xi_p)$ is decreasing singular values. Then $\mathbf{X}_k=\mathbf{PD}((\xi_i^2+k_i)^{1/2})\mathbf{Q}'$ and  -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{X'X}=&\mathbf{QD}_\xi^2\mathbf{Q}'\\ -->
<!-- \mathbf{X}_k'\mathbf{X}_k=&\mathbf{Q(D_\xi^2+K)}\mathbf{Q}'\quad\text{generalized surrogate}\\ -->
<!-- \mathbf{X}_k'\mathbf{X}_k=&\mathbf{QD}_\xi^2\mathbf{Q}'+k\mathbf{I}\quad\text{ordinary surrogate} -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- and the surrogate solution $\boldsymbol{\hat\beta}_{S}$ is -->
<!-- \begin{equation} -->
<!-- \mathbf{Q(D^2_{\xi}+K)Q}'\boldsymbol{\hat\beta}_{S}=\mathbf{X}_k=\mathbf{QD}((\xi_i^2+k_i)^{1/2})\mathbf{P}'\mathbf{y} -->
<!-- (\#eq:surrogate-e) -->
<!-- \end{equation} -->
<!-- Jensen and Ramirez proved that $\mathrm{SSE}(\boldsymbol{\hat\beta}_{S})< \mathrm{SSE}(\boldsymbol{\hat\beta}_{S})$ and surrogate model's canonical traces are monotone in $k$. -->
<ul>
<li>Lasso Regression</li>
</ul>
<p>Ridge regression can be understood as a restricted least squares problem. Denote the constraint <span class="math inline">\(s\)</span>, the solution of ridge coefficient estimates satisfies</p>
<p><span class="math display">\[\begin{equation}
\min_{\boldsymbol\beta}\left\{\sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_j\right)^2\right\}\text{ subject to } \sum_{j=1}^p\beta_j^2\le s
\end{equation}\]</span></p>
<p>Another approach is to replace the constraint term <span class="math inline">\(\sum_{j=1}^p\beta_j^2\le s\)</span> with <span class="math inline">\(\sum_{j=1}^p|\beta_j|\le s\)</span>. This method is called lasso regression.</p>
<!-- \begin{equation} -->
<!-- \min_{\boldsymbol\beta}\left\{\sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_j\right)^2\right\}\text{ subject to } \sum_{j=1}^p|\beta_j|\le s -->
<!-- \end{equation} -->
<p>Suppose the case of two predictors, the quadratic loss function creates a spherical constraint for a geometric illustration, while the norm loss function is a diamond. The contours of <span class="math inline">\(\mathrm{SSE}\)</span> are many expanding ellipses centered around least square estimate <span class="math inline">\(\hat\beta_{LS}\)</span>. Each ellipse represents a <span class="math inline">\(k\)</span> value.
If the restriction <span class="math inline">\(s\)</span> also called ‘budget’ is very large, the restriction area will cover the point of <span class="math inline">\(\hat\beta_{LS}\)</span>. That means <span class="math inline">\(\hat\beta_{LS}=\hat\beta_{R}\)</span> and <span class="math inline">\(k=0\)</span>.
When <span class="math inline">\(s\)</span> is small, the solution is to choose the ellipse contacting the constraint area with corresponding <span class="math inline">\(k\)</span> and <span class="math inline">\(\mathrm{SSE}\)</span>.
Here lasso constraint has sharp corners at each axes. When the ellipse has a intersect point on one corner, that means one of the coefficient equals zero. But it will not happen on ridge constraint.
Therefore, an improvement of lasso with respect to ridge regression is that lasso allow some estimates <span class="math inline">\(\beta_j=0\)</span>. It makes the results more interpretative. Moreover, lasso regression can make variable selection.</p>
<ul>
<li>A brief discussion</li>
</ul>
<p>Many studies such as <span class="citation"><a href="#ref-alamFactorsAffectingTravel2018" role="doc-biblioref">Alam, Nixon, and Zhang</a> (<a href="#ref-alamFactorsAffectingTravel2018" role="doc-biblioref">2018</a>)</span>, check the multicollinearity issue after fitting the models.
Using PCR method, some disaggregated travel models’ <span class="math inline">\(R^2\)</span> can be over 0.5 <span class="citation">(<a href="#ref-hamidiLongitudinalStudyChanges2014" role="doc-biblioref">Hamidi and Ewing 2014</a>; <a href="#ref-tianTrafficGeneratedMixedUse2015" role="doc-biblioref">Tian et al. 2015</a>)</span>.
But the limitation is that the principal components are hard to interpret the meaning.
The results of PCR may just describe the data themselves and they are reproducible but not replicable for other data.</p>
<p>Some research further investigate which variable plays the primary role.
Using meta-regression, <span class="citation"><a href="#ref-gimRelationshipsLandUse2013" role="doc-biblioref">Gim</a> (<a href="#ref-gimRelationshipsLandUse2013" role="doc-biblioref">2013</a>)</span> find that accessibility to regional centers is the primary factor affecting travel behavior, while other D-variables are conditional or auxiliary factors.</p>
<p><span class="citation"><a href="#ref-handyEnoughAlreadyLet2018" role="doc-biblioref">Handy</a> (<a href="#ref-handyEnoughAlreadyLet2018" role="doc-biblioref">2018</a>)</span> says most of the D-variable models have moderate multicollinearity issue and suggest to replace ‘Ds’ with ‘Accessibility’ framework.
A question is, does the multicollinearity disappear in the new ‘A’ framework?
Based on <span class="citation"><a href="#ref-handyPlanningAccessibilityTheory2005" role="doc-biblioref">Handy</a> (<a href="#ref-handyPlanningAccessibilityTheory2005" role="doc-biblioref">2005b</a>)</span> ’s suggestion, <span class="citation"><a href="#ref-proffittAccessibilityPlanningAmerican2019" role="doc-biblioref">Proffitt et al.</a> (<a href="#ref-proffittAccessibilityPlanningAmerican2019" role="doc-biblioref">2019</a>)</span> create an accessibility index by regression tree. It seems like multicollinearity is not the reason of choosing “D” or “A.”</p>
<p>To address multicollinearity, ridge regression, surrogate model, and lasso regression have provided plenty of choices.
<span class="citation"><a href="#ref-tsaoEstimableGroupEffects2019" role="doc-biblioref">Tsao</a> (<a href="#ref-tsaoEstimableGroupEffects2019" role="doc-biblioref">2019</a>)</span> contribute another approach to handle multicollinearity which still uses ordinary least squares regression.
Hence these ‘traditional’ methods are implementable and interpretative. More important is that they are comparable.
Researcher maybe don’t need to rush into some more complex methods or post studies.</p>
</div>
<div id="variables-selections" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Variables Selections</h2>
<p>For Multiple Linear Regression, variables selection is an essential step.
But in travel-urban form literature this step often doesn’t take up much space.
This section introduces several fundamental methods and discuss the related issues at the end.</p>
<ul>
<li>Selecting Procedure</li>
</ul>
<p>Suppose the data has <span class="math inline">\(10\)</span> candidate predictors. There will be <span class="math inline">\(1024\)</span> possible models.
All Possible Regressions fit all <span class="math inline">\(2^p\)</span> models using <span class="math inline">\(p\)</span> candidate predictors. Then one can select the best one based on above criteria.
For high-dimension data, fitting all possible regressions is computing intensive and exhaust the degree of freedom.
In practice, people often choose other more efficient procedures such as best-subset selection. Given a number of selected variables <span class="math inline">\(k\le p\)</span>, there could be <span class="math inline">\(p\choose k\)</span> possible combinations. By fitting all <span class="math inline">\(p\choose k\)</span> models with <span class="math inline">\(k\)</span> predictors, denote the best model with smallest <span class="math inline">\(SSE\)</span>, or largest <span class="math inline">\(R^2\)</span> as <span class="math inline">\(M_k\)</span>.
For each <span class="math inline">\(k=1,2,...,p\)</span>, there will be <span class="math inline">\(M_0,M_1,...,M_p\)</span> models. The final winner could be identified by comparing PRESS,</p>
<p>Stepwise selection include three methods: forward selection, backward elimination, and stepwise regression.
<strong>Forward selection</strong> starts from null model with only intercept. In each step of this procedure, a variable with greatest simple correlation with the response will be added into the model. If the new variable <span class="math inline">\(x_1\)</span> gets a large <span class="math inline">\(F\)</span> statistic and shows a significance effect on response, the second step will calculate the partial correlations between two sets of residuals. One is from the new fitted model <span class="math inline">\(\hat y=\beta_0+\beta_1x_1\)</span>. Another one is the model of other candidates on <span class="math inline">\(x_1\)</span>, that is <span class="math inline">\(\hat x_j=\alpha_{0j}+\alpha_{1j}x_1\)</span>, <span class="math inline">\(j=2,3,...,(p-1)\)</span>. Then the variable with largest partial correlation with <span class="math inline">\(y\)</span> is added into the model.
The two steps will repeat until the partial <span class="math inline">\(F\)</span> statistic is small at a given significant level.
<strong>Backward elimination</strong> starts from the full model with all candidates.
Given a preselected value of <span class="math inline">\(F_0\)</span>, each round will remove the variable with smallest <span class="math inline">\(F\)</span> and refit the model with rest predictors.
Then repeat to drop off one variable each round until all remaining predictors have a partial <span class="math inline">\(F_j&gt;F_0\)</span>.
<strong>Stepwise regression</strong> combines forward selection and backward elimination together. During the forward steps, if some added predictors have a partial <span class="math inline">\(F_j&lt;F_0\)</span>, they also can be removed from the model by backward elimination.
It is common that some candidate predictors are correlated.
At the beginning, a predictor <span class="math inline">\(x_1\)</span> having greater simple correlation with response was added into the model.
However, along with a subset of related predictors were added, <span class="math inline">\(x_1\)</span> could become ‘useless’ in the model. In this case, backward elimination is necessary for achieving the best solution.</p>
<p>Lasso regression can also help dropping off some variables.
When reducing variance, lasso allow the least squares estimates shrinking towards zero. This method is called shrinkage.</p>
<ul>
<li>Model Evaluation Criteria</li>
</ul>
<p><strong>Coefficient of determination</strong> <span class="math inline">\(R^2\)</span>is a basic measure of model performance. It has known that adding more predictor always increases <span class="math inline">\(R^2\)</span>. So the subset regression will stop to add new variables when the change of <span class="math inline">\(R^2\)</span> is not significant.
The improvement of <span class="math inline">\(R^2_{adj}\)</span> is that it is not a monotone increasing function. So one can select a maximum value on a convex curve.
Maximizing <span class="math inline">\(R^2_{adj}\)</span> is equivalent to minimizing residual mean square <span class="math inline">\(\mathrm{MSE}\)</span>
When prediction of the mean response is the interest, <span class="math inline">\(R^2_{pred}\)</span> based on prediction mean square error (PRESS) statistic is more preferred. PRESS is useful for selecting from two competing models.</p>
<p><strong>Akaike Information Criterion (AIC)</strong> is a penalized measure using maximum entropy.
AIC will decrease when adding extra terms into the model. Then one can justify when the model can stop adding the new terms. <span class="math inline">\(\mathrm{AIC}=n\ln\left(\frac1n \mathrm{SSE} \right)+ 2p\)</span>.
<strong>Bayesian information criterion (BIC)</strong> is the extension of AIC. <span class="math inline">\(\mathrm{BIC}=n\ln\left(\frac{1}{n} \mathrm{SSE} \right)+ p\ln(n)\)</span> <span class="citation"><a href="#ref-schwarzEstimatingDimensionModel1978" role="doc-biblioref">Schwarz</a> (<a href="#ref-schwarzEstimatingDimensionModel1978" role="doc-biblioref">1978</a>)</span> proposed a version of BIC with higher penalty for adding predictors when sample size is large.</p>
<p>Beside above criteria, <strong>Mallows <span class="math inline">\(C_p\)</span> statistic</strong> is an important criteria related to the mean square error.
Suppose the fitted subset model has <span class="math inline">\(p\)</span> variables and expected response <span class="math inline">\(\hat y_i\)</span>. <span class="math inline">\(\mathrm{SSE}(p)\)</span> is the total sum square error including two variance components.
<span class="math inline">\(\mathrm{SSE}\)</span> is the true sum square error from the ‘true’ model, while the sum square bias is <span class="math inline">\(\mathrm{SSE}_B(p)=\sum_{i=1}^n(E[y_i]-E[\hat y_i])^2= \mathrm{SSE}(p) - \mathrm{SSE}\)</span>.
Then Mallows <span class="math inline">\(C_p=\frac{\mathrm{SSE}(p)}{\hat\sigma^2} - n + 2p\)</span>.
If the supposed model is true, <span class="math inline">\(\mathrm{SSE}_B(p)=0\)</span>, it gives <span class="math inline">\(E[C_p|\mathrm{Bias}=0] = \frac{(n-p)\sigma^2}{\sigma^2}-(n-2p)=p\)</span>
Hence, a plot of <span class="math inline">\(C_p\)</span> versus <span class="math inline">\(p\)</span> can help to find the best one from many points. The proper model should have <span class="math inline">\(C_p\approx p\)</span> and smaller <span class="math inline">\(C_p\)</span> is preferred.
<span class="math inline">\(C_p\)</span> is often increase when <span class="math inline">\(\mathrm{SSE}(p)\)</span> decrease by adding predictors. A personal judgment can choose the best tradeoff between smaller <span class="math inline">\(C_p\)</span> and smaller <span class="math inline">\(\mathrm{SSE}(p)\)</span>.</p>
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- C_p=&\frac{1}{\hat\sigma^2}( \mathrm{SSE}_B(p) + \sum_{i=1}^n\mathrm{Var}[\hat y_i] )\\ -->
<!-- =&\frac{1}{\hat\sigma^2}( \mathrm{SSE}(p) - \mathrm{SSE} + \sum_{i=1}^n\mathrm{Var}[\hat y_i] )\\ -->
<!-- =&\frac{1}{\hat\sigma^2}( \mathrm{SSE}(p) - (n-p)\hat\sigma^2 + p\hat\sigma^2 )\\ -->
<!-- =&\frac{\mathrm{SSE}(p)}{\hat\sigma^2} - n + 2p -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- \mathrm{AIC}=n\ln\left(\frac1n \mathrm{SSE} \right)+ 2p -->
<!-- (\#eq:aic) -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- \mathrm{BIC}=n\ln\left(\frac{1}{n} \mathrm{SSE} \right)+ p\ln(n) -->
<!-- (\#eq:bic) -->
<!-- \end{equation} -->
<ul>
<li>A brief discussion</li>
</ul>
<p>The original data sources often include more than one hundred variables such as NHTS, ACS, LEHD, and EPA’s Smart Location Database.
It is hard to conduct the systematic variable selections for all of them.
In travel-urban form literature, the variables selections are mainly based on the background knowledge and research design.
The framework conceived by D-variables allows researchers to add new candidates they favorite and compare the results.
<span class="citation"><a href="#ref-astonStudyDesignImpacts2020" role="doc-biblioref">Aston et al.</a> (<a href="#ref-astonStudyDesignImpacts2020" role="doc-biblioref">2020</a>)</span> purpose that, in addition to D-variables, five explanatory variables are most common number in travel-urban form studies.
Hence, for a target model with 10 covariates, above systematic methods of variables selection are still applicable.</p>
<p>For travel-urban form questions, researchers should also consider a target level of goodness-of-fit through variables selection.
Due to data limitation or other reasons, one may only use a subset of the true predictors to fit the model, which is called <strong>underfitting</strong>.
In social studies, underfitting is common because the social activities are very complex. Usual data collection could miss some important factors such as social network. Some psychological factors such as attitude or habit are hard to be quantified.
Sometimes, the coefficients of determination are very low (e.g. <span class="math inline">\(R^2_{adj} = 0.0088\)</span> <span class="citation">(<a href="#ref-laneTAZlevelVariationWork2011" role="doc-biblioref">B. W. Lane 2011</a>)</span>, <span class="math inline">\(R^2_{adj} = 0.085\)</span> <span class="citation">(<a href="#ref-gordonNeighborhoodAttributesCommuting2004" role="doc-biblioref">Gordon et al. 2004, 27</a>)</span>).</p>
<p>In contrast, One may fit a model with extra irrelevant factors. <strong>Overfitting</strong> model fits the data too closely and may only capture the random noise.
Or the extra factors are accidentally related to the response in this data.
Some travel-urban form studies may have the risk of overfitting. For example, <span class="citation"><a href="#ref-leeComplementaryPricingLand2013" role="doc-biblioref">B. Lee and Lee</a> (<a href="#ref-leeComplementaryPricingLand2013" role="doc-biblioref">2013</a>)</span> ’s study applies two-stage least squares (2SLS) method and get <span class="math inline">\(R^2 = 0.96\)</span>. Sometimes high <span class="math inline">\(R^2\)</span> may due to some specific research design or data source (e.g. <span class="math inline">\(R^2 = 0.979\)</span> <span class="citation">(<a href="#ref-zhaoWhatInfluencesMetro2013" role="doc-biblioref">J. Zhao et al. 2013</a>)</span>, <span class="math inline">\(R^2 = 0.952\)</span> <span class="citation">(<a href="#ref-cerveroDirectRidershipModel2010" role="doc-biblioref">Cervero, Murakami, and Miller 2010</a>)</span>).
Without suitable validation, many overfitting models produce false positive relationship and perform badly in prediction.</p>
<!-- Suppose the true model is $\mathbf{y}=\mathbf{X}\boldsymbol\beta +\boldsymbol\varepsilon=\mathbf{X}_1\boldsymbol\beta_1 + \mathbf{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon$. $\mathbf{X}$ is full rank $r(\mathbf{X})=r =r_1+r_2$, $E[\boldsymbol\varepsilon]=0$, and $Cov[\boldsymbol\varepsilon]= \sigma^2\mathbf{I}_n$. -->
<!-- The normal equation $\mathbf{X'X}\boldsymbol\beta=\mathbf{X'y}$ can be rewrite as  -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{X}_1'\mathbf{X}_1\boldsymbol\beta^0_1+\mathbf{X}_1'\mathbf{X}_2\boldsymbol\beta^0_2&=\mathbf{X}_1'\mathbf{y}\\ -->
<!-- \mathbf{X}_2'\mathbf{X}_1\boldsymbol\beta^0_1+\mathbf{X}_2'\mathbf{X}_2\boldsymbol\beta^0_2&=\mathbf{X}_2'\mathbf{y}\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- Let $\mathbf{P}_i=\mathbf{X}_i(\mathbf{X}_i'\mathbf{X}_i)^{-}\mathbf{X}'_i$, $i=1,2$, and -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{M}_1=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{X}_2\\ -->
<!-- \mathbf{M}_2=&\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{X}_2 -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- Then, -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \boldsymbol\beta^0_1=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1(\mathbf{y}-\mathbf{X}_2\boldsymbol\beta^0_2)\\ -->
<!-- \boldsymbol\beta^0_2=&[\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{X}_2]^{-}\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{y}=\mathbf{M}^{-}_2\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{y}\\ -->
<!-- \hat\sigma^2=&\frac{1}{n-r}(\mathbf{y}-\mathbf{X}_1\boldsymbol\beta^0_1-\mathbf{X}_2\boldsymbol\beta^0_2)'(\mathbf{y}-\mathbf{X}_1\boldsymbol\beta^0_1-\mathbf{X}_2\boldsymbol\beta^0_2) -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- - Underfitting -->
<!-- If the fitted model $\mathbf{y}=\mathbf{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon$ doesn't contain $\mathbf{X}_2$ and $\boldsymbol\beta_2$, the least squares solutions are -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \boldsymbol\beta^0_{1,H}=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{y}\\ -->
<!-- \hat\sigma^2_{1,H}=&\frac{1}{n-r_1}\mathbf{y}'(\mathbf{I}-\mathbf{P}_1)\mathbf{y} -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- It is clear that $\boldsymbol\beta^0_{1,H}$ and $\hat\sigma^2_{1,H}$ are biased estimates because -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- E[\boldsymbol\beta^0_{1,H}]=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{X}_1\boldsymbol\beta_1+(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{X}_2\boldsymbol\beta_2\\ -->
<!-- =&\mathbf{H}\boldsymbol\beta_1+\mathbf{M}_1\boldsymbol\beta_2 -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- and -->
<!-- \begin{equation} -->
<!-- E[\hat\sigma^2_{1,H}]=\sigma^2 + \frac{1}{n-r_1}\boldsymbol\beta'_2\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{X}_2\boldsymbol\beta_2 -->
<!-- =\sigma^2 + \frac{1}{n-r_1}\boldsymbol\beta'_2\mathbf{M}\boldsymbol\beta_2 -->
<!-- \end{equation} -->
<!-- $E[\boldsymbol\beta^0_{1,H}]=\boldsymbol\beta_1$ and $E[\hat\sigma^2_{1,H}]=\sigma^2$ only when $\boldsymbol\beta_2=0$ or $\mathbf{M}_1=0$. The later is $\mathbf{X}_1\perp\mathbf{X}_2$ or $\mathbf{X}'_1\mathbf{X}_2=0$. -->
<!-- Since $\mathbf{\hat Y}_{0,H}=\mathbf{X}_{0,1}\boldsymbol\beta^0_{1,H}$, $\mathbf{\hat Y}_{0,H}$ is also biased unless $\boldsymbol\beta_2=0$ or $\mathbf{X}_1$ is orthogonal to $\mathbf{X}_2$. -->
<!-- Denote $MSE_{H}$ as the error mean squares of underfitting model. $MSE=\text{Var-cov}[\boldsymbol{\hat\beta}]+\text{Bias}\cdot\text{Bias}'$. Then -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- MSE_{H}=&\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-} + \mathbf{M}_1\boldsymbol\beta_2\boldsymbol\beta_2'\mathbf{M}'_1\\ -->
<!-- MSE=&\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-} + \mathbf{M}_1Cov[\boldsymbol\beta_2^0]\mathbf{M}'_1\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- Since $Cov[\boldsymbol\beta_2^0]-\boldsymbol\beta_2\boldsymbol\beta_2'$ is a  positive semidefinite matrix (p.s.d.), $MSE\ge MSE_{H}$ always holds. -->
<!-- - Overfitting -->
<!-- That is, the true model is $\mathbf{y}=\mathbf{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon$ -->
<!-- and the fitted model is $\mathbf{y}=\mathbf{X}_1\boldsymbol\beta_1 + \mathbf{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon$. -->
<!-- This case implies $\boldsymbol\beta_2=0$. Then all above estimates are unbiased. -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- E[\boldsymbol\beta^0_{1,H}]=&\mathbf{H}\boldsymbol\beta_1+\mathbf{M}_1\boldsymbol\beta_2=\mathbf{H}\boldsymbol\beta_1\\ -->
<!-- E[\hat\sigma^2_{1,H}]=&\sigma^2 + \frac{1}{n-r_1}\boldsymbol\beta'_2\mathbf{M}\boldsymbol\beta_2=\sigma^2\\ -->
<!-- MSE_{H}=&\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-} + \mathbf{M}_1\boldsymbol\beta_2\boldsymbol\beta_2'\mathbf{M}'_1=\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-}\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- ## Other Topics -->
<!-- ### Bayesian approaches (Opt.) -->
<!-- ### SEM (Opt.) -->
<!-- Another attempt tries the method of structural equation modeling (SEM). The two studies capture higher elasticities of per capita VMT with respect to density (-0.38 and -0.238) [@cerveroEffectsBuiltEnvironments2010; @ewingStructuralEquationModels2014]. -->
<!-- In general, modeling is a case-by-case work. Researchers may have their preferred model by weighing the sensitivity and robustness even given the same hypothesis and data. -->
<!-- The published papers usually don't show the results of diagnosis and validation. -->
<!-- Under this circumstance, compare or summarize these outcomes are unreliable. -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-alamFactorsAffectingTravel2018" class="csl-entry">
Alam, Bhuiyan Monwar, Hilary Nixon, and Qiong Zhang. 2018. <span>“Factors <span>Affecting Travel Demand</span> by <span>Bus</span>: An <span>Empirical Analysis</span> at <span>U</span>.<span>S</span>. <span>Metropolitan Statistical Area Level</span>.”</span> <em>Transportation Research Record</em> 2672 (8): 817–26. <a href="https://doi.org/10.1177/0361198118798714">https://doi.org/10.1177/0361198118798714</a>.
</div>
<div id="ref-astonStudyDesignImpacts2020" class="csl-entry">
Aston, Laura, Graham Currie, Md. Kamruzzaman, Alexa Delbosc, and David Teller. 2020. <span>“Study Design Impacts on Built Environment and Transit Use Research.”</span> <em>Journal of Transport Geography</em> 82 (January): 102625. <a href="https://doi.org/10.1016/j.jtrangeo.2019.102625">https://doi.org/10.1016/j.jtrangeo.2019.102625</a>.
</div>
<div id="ref-boarnetWalkingUrbanDesign2008" class="csl-entry">
Boarnet, Marlon G., Michael Greenwald, and Tracy E. McMillan. 2008. <span>“Walking, <span>Urban Design</span>, and <span>Health</span>: Toward a <span>Cost</span>-<span>Benefit Analysis Framework</span>.”</span> <em>Journal of Planning Education and Research</em> 27 (3): 341–58. <a href="https://doi.org/10.1177/0739456X07311073">https://doi.org/10.1177/0739456X07311073</a>.
</div>
<div id="ref-cerveroDirectRidershipModel2010" class="csl-entry">
Cervero, Robert, Jin Murakami, and Mark Miller. 2010. <span>“Direct <span>Ridership Model</span> of <span>Bus Rapid Transit</span> in <span>Los Angeles County</span>, <span>California</span>.”</span> <em>Transportation Research Record</em> 2145 (1): 1–7. <a href="https://doi.org/10.3141/2145-01">https://doi.org/10.3141/2145-01</a>.
</div>
<div id="ref-chatmanHowDensityMixed2003" class="csl-entry">
Chatman, Daniel G. 2003. <span>“How <span>Density</span> and <span>Mixed Uses</span> at the <span>Workplace Affect Personal Commercial Travel</span> and <span>Commute Mode Choice</span>.”</span> <em>Transportation Research Record</em> 1831 (1): 193–201. <a href="https://doi.org/10.3141/1831-22">https://doi.org/10.3141/1831-22</a>.
</div>
<div id="ref-chatmanDeconstructingDevelopmentDensity2008" class="csl-entry">
———. 2008. <span>“Deconstructing Development Density: Quality, Quantity and Price Effects on Household Non-Work Travel.”</span> <em>Transportation Research Part A: Policy and Practice</em> 42 (7): 1008–30. <a href="https://doi.org/10.1016/j.tra.2008.02.003">https://doi.org/10.1016/j.tra.2008.02.003</a>.
</div>
<div id="ref-choiAnalysisMetroRidership2012" class="csl-entry">
Choi, Jinkyung, Yong Jae Lee, Taewan Kim, and Keemin Sohn. 2012. <span>“An Analysis of <span>Metro</span> Ridership at the Station-to-Station Level in <span>Seoul</span>.”</span> <em>Transportation</em> 39 (3): 705–22. <a href="https://doi.org/10.1007/s11116-011-9368-3">https://doi.org/10.1007/s11116-011-9368-3</a>.
</div>
<div id="ref-dingNonlinearAssociationsZonal2021" class="csl-entry">
Ding, Chuan, Xinyu Cao, Bin Yu, and Yang Ju. 2021. <span>“Non-Linear Associations Between Zonal Built Environment Attributes and Transit Commuting Mode Choice Accounting for Spatial Heterogeneity.”</span> <em>Transportation Research Part A: Policy and Practice</em> 148 (June): 22–35. <a href="https://doi.org/10.1016/j.tra.2021.03.021">https://doi.org/10.1016/j.tra.2021.03.021</a>.
</div>
<div id="ref-ewingDoesCompactDevelopment2017" class="csl-entry">
———. 2017. <span>“<span>‘<span>Does Compact Development Make People Drive Less</span>?’</span> <span>The Answer Is Yes</span>.”</span> <em>Journal of the American Planning Association</em> 83 (1): 19–25. <a href="https://doi.org/10.1080/01944363.2016.1245112">https://doi.org/10.1080/01944363.2016.1245112</a>.
</div>
<div id="ref-ewingReducingVehicleMiles2020" class="csl-entry">
Ewing, Reid, Keunhyun Park, Sadegh Sabouri, Torrey Lyons, Keuntae Kim, Dong-ah Choi, Katherine Daly, and Roya Ghasrodashti. 2020. <span>“Reducing <span>Vehicle Miles Traveled</span>, <span>Encouraging Walk Trips</span>, and <span>Facilitating Efficient Trip Chains Through Polycentric Development</span>.”</span> <em>TREC Final Reports</em>, October. <a href="https://doi.org/10.15760/trec.255">https://doi.org/10.15760/trec.255</a>.
</div>
<div id="ref-gelmanBayesianMeasuresExplained2006" class="csl-entry">
Gelman, Andrew, and Iain Pardoe. 2006. <span>“Bayesian <span>Measures</span> of <span>Explained Variance</span> and <span>Pooling</span> in <span>Multilevel</span> (<span>Hierarchical</span>) <span>Models</span>.”</span> <em>Technometrics</em> 48 (2): 241–51. <a href="https://doi.org/10.1198/004017005000000517">https://doi.org/10.1198/004017005000000517</a>.
</div>
<div id="ref-gimRelationshipsLandUse2013" class="csl-entry">
Gim, Tae-Hyoung Tommy. 2013. <span>“The Relationships Between Land Use Measures and Travel Behavior: A Meta-Analytic Approach.”</span> <em>Transportation Planning and Technology</em> 36 (5): 413–34. <a href="https://doi.org/10.1080/03081060.2013.818272">https://doi.org/10.1080/03081060.2013.818272</a>.
</div>
<div id="ref-gordonNeighborhoodAttributesCommuting2004" class="csl-entry">
Gordon, Peter, Bumsoo Lee, James E. Moore II, Harry W. Richardson, Christopher Williamson, METRANS Transportation Center (Calif.), Planning University of Southern California. School of Policy and Development, and California. Dept. of Transportation. Division of Research and Innovation. 2004. <span>“Neighborhood <span>Attributes</span> and <span>Commuting Behavior</span>: Transit <span>Choice</span>.”</span> FHWA/CA/OR-2005/04. <a href="https://rosap.ntl.bts.gov/view/dot/57477">https://rosap.ntl.bts.gov/view/dot/57477</a>.
</div>
<div id="ref-hamidiLongitudinalStudyChanges2014" class="csl-entry">
Hamidi, Shima, and Reid Ewing. 2014. <span>“A Longitudinal Study of Changes in Urban Sprawl Between 2000 and 2010 in the <span>United States</span>.”</span> <em>Landscape and Urban Planning</em> 128 (August): 72–82. <a href="https://doi.org/10.1016/j.landurbplan.2014.04.021">https://doi.org/10.1016/j.landurbplan.2014.04.021</a>.
</div>
<div id="ref-handyPlanningAccessibilityTheory2005" class="csl-entry">
———. 2005b. <span>“Planning for <span>Accessibility</span>: In <span>Theory</span> and in <span>Practice</span>.”</span> In <em>Access to <span>Destinations</span></em>, edited by David M. Levinson and Kevin J. Krizek, 131–47. <span>Emerald Group Publishing Limited</span>. <a href="https://doi.org/10.1108/9780080460550-007">https://doi.org/10.1108/9780080460550-007</a>.
</div>
<div id="ref-handyEnoughAlreadyLet2018" class="csl-entry">
———. 2018. <span>“Enough with the <span>‘<span>D</span>’s’</span> <span>Already</span> — <span>Let</span>’s <span>Get Back</span> to <span>‘<span>A</span>’</span>.”</span> <em>Transfers Magazine</em>, no. 1 (May). <a href="http://trid.trb.org/view/1709460">http://trid.trb.org/view/1709460</a>.
</div>
<div id="ref-hoerlRidgeRegressionBiased1970" class="csl-entry">
Hoerl, Arthur E., and Robert W. Kennard. 1970. <span>“Ridge <span>Regression</span>: Biased <span>Estimation</span> for <span>Nonorthogonal Problems</span>.”</span> <em>Technometrics</em> 12 (1): 55–67. <a href="https://doi.org/10.1080/00401706.1970.10488634">https://doi.org/10.1080/00401706.1970.10488634</a>.
</div>
<div id="ref-hongHowBuiltenvironmentFactors2014" class="csl-entry">
Hong, Jinhyun, Qing Shen, and Lei Zhang. 2014. <span>“How Do Built-Environment Factors Affect Travel Behavior? A Spatial Analysis at Different Geographic Scales.”</span> <em>Transportation</em> 41 (3): 419–40. <a href="https://doi.org/10.1007/s11116-013-9462-9">https://doi.org/10.1007/s11116-013-9462-9</a>.
</div>
<div id="ref-jensenSurrogateModelsIllconditioned2010a" class="csl-entry">
Jensen, D. R., and D. E. Ramirez. 2010. <span>“Surrogate Models in Ill-Conditioned Systems.”</span> <em>Journal of Statistical Planning and Inference</em> 140 (7): 2069–77. <a href="https://doi.org/10.1016/j.jspi.2010.02.001">https://doi.org/10.1016/j.jspi.2010.02.001</a>.
</div>
<div id="ref-jensenVariationsRidgeTraces2012" class="csl-entry">
———. 2012. <span>“Variations on <span>Ridge Traces</span> in <span>Regression</span>.”</span> <em>Communications in Statistics - Simulation and Computation</em> 41 (2): 265–78. <a href="https://doi.org/10.1080/03610918.2011.586482">https://doi.org/10.1080/03610918.2011.586482</a>.
</div>
<div id="ref-laneTAZlevelVariationWork2011" class="csl-entry">
Lane, Bradley W. 2011. <span>“<span>TAZ</span>-Level Variation in Work Trip Mode Choice Between 1990 and 2000 and the Presence of Rail Transit.”</span> <em>Journal of Geographical Systems</em> 13 (2): 147–71. https://doi.org/<a href="http://dx.doi.org/10.1007/s10109-010-0110-z">http://dx.doi.org/10.1007/s10109-010-0110-z</a>.
</div>
<div id="ref-leeComplementaryPricingLand2013" class="csl-entry">
Lee, Bumsoo, and Yongsung Lee. 2013. <span>“Complementary <span>Pricing</span> and <span>Land Use Policies</span>: Does <span>It Lead</span> to <span>Higher Transit Use</span>?”</span> <em>Journal of the American Planning Association</em> 79 (4): 314–28. <a href="https://doi.org/10.1080/01944363.2014.915629">https://doi.org/10.1080/01944363.2014.915629</a>.
</div>
<div id="ref-leeComparingImpactsLocal2020" class="csl-entry">
Lee, Sungwon, and Bumsoo Lee. 2020. <span>“Comparing the Impacts of Local Land Use and Urban Spatial Structure on Household <span>VMT</span> and <span>GHG</span> Emissions.”</span> <em>Journal of Transport Geography</em> 84 (April): 102694. <a href="https://doi.org/10.1016/j.jtrangeo.2020.102694">https://doi.org/10.1016/j.jtrangeo.2020.102694</a>.
</div>
<div id="ref-liuInfluenceBuiltEnvironment2018" class="csl-entry">
Liu, Yang, Yanjie Ji, Zhuangbin Shi, and Liangpeng Gao. 2018. <span>“The <span>Influence</span> of the <span>Built Environment</span> on <span>School Children</span>’s <span>Metro Ridership</span>: An <span>Exploration Using Geographically Weighted Poisson Regression Models</span>.”</span> <em>Sustainability</em> 10 (12, 12): 4684. <a href="https://doi.org/10.3390/su10124684">https://doi.org/10.3390/su10124684</a>.
</div>
<div id="ref-lumleyImportanceNormalityAssumption2002" class="csl-entry">
Lumley, Thomas, Paula Diehr, Scott Emerson, and Lu Chen. 2002. <span>“The <span>Importance</span> of the <span>Normality Assumption</span> in <span>Large Public Health Data Sets</span>.”</span> <em>Annual Review of Public Health</em> 23 (1): 151–69. <a href="https://doi.org/10.1146/annurev.publhealth.23.100901.140546">https://doi.org/10.1146/annurev.publhealth.23.100901.140546</a>.
</div>
<div id="ref-proffittAccessibilityPlanningAmerican2019" class="csl-entry">
Proffitt, David G, Keith Bartholomew, Reid Ewing, and Harvey J Miller. 2019. <span>“Accessibility Planning in <span>American</span> Metropolitan Areas: Are We There Yet?”</span> <em>Urban Studies</em> 56 (1): 167–92. <a href="https://doi.org/10.1177/0042098017710122">https://doi.org/10.1177/0042098017710122</a>.
</div>
<div id="ref-ravishankerFirstCourseLinear2020" class="csl-entry">
Ravishanker, Nalini, and Dipak K. Dey. 2020. <em>A <span>First Course</span> in <span>Linear Model Theory</span></em>. <span>CRC Press</span>. <a href="https://books.google.com?id=i2r0DwAAQBAJ">https://books.google.com?id=i2r0DwAAQBAJ</a>.
</div>
<div id="ref-schwarzEstimatingDimensionModel1978" class="csl-entry">
Schwarz, Gideon. 1978. <span>“Estimating the <span>Dimension</span> of a <span>Model</span>.”</span> <em>The Annals of Statistics</em> 6 (2): 461–64. <a href="https://www.jstor.org/stable/2958889">https://www.jstor.org/stable/2958889</a>.
</div>
<div id="ref-tianTrafficGeneratedMixedUse2015" class="csl-entry">
Tian, Guang, Reid Ewing, Alex White, Shima Hamidi, Jerry Walters, J. P. Goates, and Alex Joyce. 2015. <span>“Traffic <span>Generated</span> by <span>Mixed</span>-<span>Use Developments</span>: Thirteen-<span>Region Study Using Consistent Measures</span> of <span>Built Environment</span>.”</span> <em>Transportation Research Record</em> 2500 (1): 116–24. <a href="https://doi.org/10.3141/2500-14">https://doi.org/10.3141/2500-14</a>.
</div>
<div id="ref-tsaoEstimableGroupEffects2019" class="csl-entry">
Tsao, Min. 2019. <span>“Estimable Group Effects for Strongly Correlated Variables in Linear Models.”</span> <em>Journal of Statistical Planning and Inference</em> 198 (January): 29–42. <a href="https://doi.org/10.1016/j.jspi.2018.03.003">https://doi.org/10.1016/j.jspi.2018.03.003</a>.
</div>
<div id="ref-zhangEvaluationSpatialAutocorrelation2009" class="csl-entry">
Zhang, Lianjun, Zhihai Ma, and Luo Guo. 2009. <span>“An <span>Evaluation</span> of <span>Spatial Autocorrelation</span> and <span>Heterogeneity</span> in the <span>Residuals</span> of <span>Six Regression Models</span>.”</span> <em>Forest Science</em> 55 (6): 533–48. <a href="https://doi.org/10.1093/forestscience/55.6.533">https://doi.org/10.1093/forestscience/55.6.533</a>.
</div>
<div id="ref-zhaoWhatInfluencesMetro2013" class="csl-entry">
Zhao, Jinbao, Wei Deng, Yan Song, and Yueran Zhu. 2013. <span>“What Influences <span>Metro</span> Station Ridership in <span>China</span>? Insights from <span>Nanjing</span>.”</span> <em>Cities</em> 35 (December): 114–24. <a href="https://doi.org/10.1016/j.cities.2013.07.002">https://doi.org/10.1016/j.cities.2013.07.002</a>.
</div>
</div>
<div id="disqus_thread"></div>

<style>
.hide-social-discuss {
   background: white;
   height: 20px;
   position: relative;
   width: 80%;
   top: -60px;
}
</style>

<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://qs26.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();

    //add this to script
    $("#disqus_thread").append("<div class='hide-social-discuss'></div>")

</script>



<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="common-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="new-trends.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/qushen26/field_paper/edit/master/files/06-issues.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/qushen26/field_paper/blob/master/files/06-issues.Rmd",
"text": null
},
"download": ["field_paper.pdf", "field_paper.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
