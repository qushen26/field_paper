<!-- # Urban-form Predictors -->

Previous research has demonstrated that no single factor determines travel behavior. Choosing a proper set of predictors thus is the second step of modeling. After identifying the influencing factors, the models then estimate the effect sizes for each urban-form predictor. At last, some inference approaches test whether the effect sizes on travel response are statistically significant or not. Those are the desired results of related literature.

## Variables Selections

For Multiple Linear Regression, variables selection is an essential step, and have been developed some systematic selecting procedures such as Forward, Backward, and Stepwise selection [@montgomeryIntroductionLinearRegression2021]. And the evaluating criteria include AIC, BIC, Mallows $C_p$, etc.[^results-1] @dingInfluencesBuiltEnvironment2017 also use AIC and BIC to compare single-level and multilevel mixture hazard models. But in travel-urban form literature, the step of variables selection usually doesn't take up much space [@buchananEffectUrbanGrowth2006]. One reason is that the authors think applying these methods is not a remarkable contribution. Another reason is that the original data sources often include more than one hundred variables such as NHTS, ACS, LEHD, and EPA's Smart Location Database. It is hard to conduct systematic variable selections for all of them. In travel-urban form literature, the first-round variables selections are mainly based on the background knowledge and research design. @mariakockelmanTravelBehaviorFunction1997 examine 18 explanatory variables in the five models with travel responses [^results-2]. After stepwise regression, 5-6 explanatory variables are selected with the most significant and more minor multicollinearity. The chosen sets vary in the five models. In @filionWastedDensityImpact2006 's study, the sample size of high-density census tracts is only 142. Stepwise regression identifies the six significant explanatory variables from 15 candidates. The six variables include built-environment and socioeconomic factors such as density, housing types, age, and income. @abhishekUrbanGrowthIndian2017 's study initially selected eight factors influencing city growth in Indian based on literature. Using Stepwise regression, four factors are the final selected explanatory variables.

[^results-1]: AIC: Akaike information criterion, BIC: Bayesian information criterion

[^results-2]: The five travel variables are total VKT per household, total nonwork home-based (NWHB) VKT per household, automobile ownership, and personal-vehicle mode choice, Walk/bike mode choice

The framework conceived by D-variables allows researchers to add new candidates they favorite and compare the results. Under this framework, researchers treat D-variables as required terms and use statistical approaches to select socio-demographic factors [@gimRelationshipsLandUse2013].

How many predictors are proper? @astonStudyDesignImpacts2020 purpose that, in addition to D-variables, five explanatory variables are the most common number in travel-urban form studies. Hence, for a target model with ten covariates, the above systematic methods of variables selection are still applicable. It's worth noting that some 'new' methods like Lasso, SCAD, and MCP have been developed over decades [^results-3]. Studies in other fields have found that these methods are more accurate and robust than the traditional methods [@wangVariableSelectionZeroinflated2015]. Future travel-urban form studies should involve these methods.

[^results-3]: Lasso: the least absolute shrinkage and selection operator; SCAD: smoothly clipped absolute deviation; MCP: mini-max concave penalty

For travel-urban form questions, variable selections also need to consider a target level of goodness-of-fit through variables selection. Due to data limitations or other reasons, one may only use a subset of the true predictors to fit the model, called underfitting. In social studies, underfitting is common because social activities are very complex. Previous theories of travel behavior have disclosed that attitude, habits, and other psychological factors play an important role in making decisions. But usual data collection could miss these important factors. Moreover, some factors, such as social networks, are hard to capture and quantify. Sometimes, the coefficients of determination are very low (e.g. $R^2_{adj} = 0.0088$ [@laneTAZlevelVariationWork2011], $R^2_{adj} = 0.085$ [@gordonNeighborhoodAttributesCommuting2004, p.27]).

In contrast, One may fit a model with extra irrelevant factors. The overfitting model fits the data too closely and may only capture the random noise. Or the extra factors are accidentally related to the response in this data. Some travel-urban form studies may have the risk of overfitting. For example, @leeComplementaryPricingLand2013 's study applies the two-stage least squares (2SLS) method and gets $R^2 = 0.96$. Sometimes high $R^2$ may due to some specific research design or data source (e.g. $R^2 = 0.979$ [@zhaoWhatInfluencesMetro2013], $R^2 = 0.952$ [@cerveroDirectRidershipModel2010]). Without proper validation, many overfitting models produce false-positive relationships and perform badly in prediction. Comparing the goodness-of-fit in cross studies needs these models to have comparable fitting levels.

<!-- This section introduces several fundamental methods and discuss the related issues at the end. -->

<!-- - Selecting Procedure -->

<!-- Suppose the data has $10$ candidate predictors. There will be $1024$ possible models. -->

<!-- All Possible Regressions fit all $2^p$ models using $p$ candidate predictors. Then one can select the best one based on above criteria. -->

<!-- For high-dimension data, fitting all possible regressions is computing intensive and exhaust the degree of freedom. -->

<!-- In practice, people often choose other more efficient procedures such as best-subset selection. Given a number of selected variables $k\le p$, there could be $\binom{p}{k}$ possible combinations. By fitting all $\binom{p}{k}$ models with $k$ predictors, denote the best model with smallest $SSE$, or largest $R^2$ as $M_k$. -->

<!-- For each $k=1,2,...,p$, there will be $M_0,M_1,...,M_p$ models. The final winner could be identified by comparing PRESS, -->

<!-- Stepwise selection include three methods: forward selection, backward elimination, and stepwise regression. -->

<!-- **Forward selection** starts from null model with only intercept. In each step of this procedure, a variable with greatest simple correlation with the response will be added into the model. If the new variable $x_1$ gets a large $F$ statistic and shows a significance effect on response, the second step will calculate the partial correlations between two sets of residuals. One is from the new fitted model $\hat y=\beta_0+\beta_1x_1$. Another one is the model of other candidates on $x_1$, that is $\hat x_j=\alpha_{0j}+\alpha_{1j}x_1$, $j=2,3,...,(p-1)$. Then the variable with largest partial correlation with $y$ is added into the model. -->

<!-- The two steps will repeat until the partial $F$ statistic is small at a given significant level. -->

<!-- **Backward elimination** starts from the full model with all candidates. -->

<!-- Given a preselected value of $F_0$, each round will remove the variable with smallest $F$ and refit the model with rest predictors. -->

<!-- Then repeat to drop off one variable each round until all remaining predictors have a partial $F_j>F_0$. -->

<!-- **Stepwise regression** combines forward selection and backward elimination together. During the forward steps, if some added predictors have a partial $F_j<F_0$, they also can be removed from the model by backward elimination. -->

<!-- It is common that some candidate predictors are correlated. -->

<!-- At the beginning, a predictor $x_1$ having greater simple correlation with response was added into the model. -->

<!-- However, along with a subset of related predictors were added, $x_1$ could become 'useless' in the model. In this case, backward elimination is necessary for achieving the best solution. -->

<!-- Lasso regression can also help dropping off some variables. -->

<!-- When reducing variance, lasso allow the least squares estimates shrinking towards zero. This method is called shrinkage. -->

<!-- - Model Evaluation Criteria -->

<!-- **Coefficient of determination** $R^2$is a basic measure of model performance. It has known that adding more predictor always increases $R^2$. So the subset regression will stop to add new variables when the change of $R^2$ is not significant. -->

<!-- The improvement of $R^2_{adj}$ is that it is not a monotone increasing function. So one can select a maximum value on a convex curve. -->

<!-- Maximizing $R^2_{adj}$ is equivalent to minimizing residual mean square $\mathrm{MSE}$ -->

<!-- When prediction of the mean response is the interest, $R^2_{pred}$ based on prediction mean square error (PRESS) statistic is more preferred. PRESS is useful for selecting from two competing models. -->

<!-- **Akaike Information Criterion (AIC)** is a penalized measure using maximum entropy. -->

<!-- AIC will decrease when adding extra terms into the model. Then one can justify when the model can stop adding the new terms. $\mathrm{AIC}=n\ln\left(\frac1n \mathrm{SSE} \right)+ 2p$. -->

<!-- **Bayesian information criterion (BIC)** is the extension of AIC. $\mathrm{BIC}=n\ln\left(\frac{1}{n} \mathrm{SSE} \right)+ p\ln(n)$ @schwarzEstimatingDimensionModel1978 proposed a version of BIC with higher penalty for adding predictors when sample size is large. -->

<!-- Beside above criteria, **Mallows $C_p$ statistic** is an important criteria related to the mean square error. -->

<!-- Suppose the fitted subset model has $p$ variables and expected response $\hat y_i$. $\mathrm{SSE}(p)$ is the total sum square error including two variance components. -->

<!-- $\mathrm{SSE}$ is the true sum square error from the 'true' model, while the sum square bias is $\mathrm{SSE}_B(p)=\sum_{i=1}^n(E[y_i]-E[\hat y_i])^2= \mathrm{SSE}(p) - \mathrm{SSE}$. -->

<!-- Then Mallows $C_p=\frac{\mathrm{SSE}(p)}{\hat\sigma^2} - n + 2p$. -->

<!-- If the supposed model is true, $\mathrm{SSE}_B(p)=0$, it gives $E[C_p|\mathrm{Bias}=0] = \frac{(n-p)\sigma^2}{\sigma^2}-(n-2p)=p$ -->

<!-- Hence, a plot of $C_p$ versus $p$ can help to find the best one from many points. The proper model should have $C_p\approx p$ and smaller $C_p$ is preferred. -->

<!-- $C_p$ is often increase when $\mathrm{SSE}(p)$ decrease by adding predictors. A personal judgment can choose the best tradeoff between smaller $C_p$ and smaller $\mathrm{SSE}(p)$. -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- C_p=&\frac{1}{\hat\sigma^2}( \mathrm{SSE}_B(p) + \sum_{i=1}^n\mathrm{Var}[\hat y_i] )\\ -->

<!-- =&\frac{1}{\hat\sigma^2}( \mathrm{SSE}(p) - \mathrm{SSE} + \sum_{i=1}^n\mathrm{Var}[\hat y_i] )\\ -->

<!-- =&\frac{1}{\hat\sigma^2}( \mathrm{SSE}(p) - (n-p)\hat\sigma^2 + p\hat\sigma^2 )\\ -->

<!-- =&\frac{\mathrm{SSE}(p)}{\hat\sigma^2} - n + 2p -->

<!-- \end{split} -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \mathrm{AIC}=n\ln\left(\frac1n \mathrm{SSE} \right)+ 2p -->

<!-- (\#eq:aic) -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \mathrm{BIC}=n\ln\left(\frac{1}{n} \mathrm{SSE} \right)+ p\ln(n) -->

<!-- (\#eq:bic) -->

<!-- \end{equation} -->

## Model Outcomes

<!-- ## Estimations -->

Estimating the effect size of built environment factors on travel is one of the major goals of travel-urban form studies. In regression analysis, the values of coefficients represent the effect size. The units of covariates are inevitably very different in many studies. One thing can be done to standardize the coefficients' values [@zhangHowBuiltEnvironment2012; @leeComparingImpactsLocal2020]. As introduced in the previous chapter, elasticity is also commonly used to determine a variable's relative importance in terms of its influence on a dependent variable [@ewingTravelBuiltEnvironment2010]. It is generally interpreted as the percent change in the dependent variable induced by a 1% change in the independent variable [@mccarthyTransportationEconomicsTheory2001]

<!-- $\mathbf{X}$ value of $\boldsymbol{\hat\beta}$ -->

When a study contains two or more travel-urban form models, the models' responses are the same or similar. Researchers can assume that the observed VMT are randomly sampled from a large population. They often compare the models' performance by adding or removing independent variables. The coefficients from the best-fitted model would be recommended. That means that the models in a study usually are comparable. But the models in cross studies could choose different combinations of covariates having substantial differences or uncertainties. In other words, an estimated coefficient means that given all other coefficients fixed, for each change of one unit in this variable, the average change in the mean of the response variable. Since the estimated coefficients are linear combinations of the response and covariates [@montgomeryIntroductionLinearRegression2021], these models should not take the consistent estimates of coefficients for granted. Comparing the coefficients among different models need to check whether their covariates matrix are similar. The framework of D-variables does help to make cross-study analysis.

Both standardized regression coefficients and elasticities try to make the effect sizes comparable somehow. For example, The population densities at the tract level in Virginia and DC would have distinct ranges [@zhangHowBuiltEnvironment2012]. Standardized regression coefficients can eliminate the different ranges of data. Another example, the unit of population densities in studies could be people per square mile [@alamFactorsAffectingTravel2018] or people per square kilometer [@ingvardsonHowUrbanDensity2018]. Elasticities can eliminate the different units of data. Another way is to unify the units before fitting the models but gathering the original data from various studies is a huge challenge. Which measurement of effect size is better for comparison? A simulation test may answer this question. Some studies sum up the standardized regression coefficients or elasticity of Multiple Linear Regression and call the summation as combined effects [@leeComparingImpactsLocal2020]. Although these values are dimensionless, standardized regression coefficients and elasticities are derived from the estimated coefficients. Is the sum of the partial regression coefficient meaningful? It needs some mathematical proof.

<!-- ### Coefficients -->

<!-- Estimating the effect size of built environment factors on travel is one of the major goals of travel-urban form studies. -->

<!-- In regression analysis, the values of coefficients represent the effect size. -->

<!-- Least Squares is the mainstream method in the past decades. By *Gauss - Markov theorem*, OLS method itself doesn't require explanatory variables and response variable following normal distribution. If the residuals $\varepsilon$ satisfy $E(\varepsilon) = 0$ and $Var(\varepsilon) = \sigma^2$, the least-squares method will give the unbiased estimators with minimum variance. -->

<!-- ::: {#g-m .theorem name="Gauss - Markov theorem"}  @ref(thm:g-m) -->

<!-- For the regression model @ref(eq:lm) with the assumptions $E(\varepsilon) = 0$, $Var(\varepsilon) = \sigma^2$, and uncorrelated errors, the least-squares estimators are unbiased and have minimum variance when compared with all other unbiased estimators that are linear combinations of the $y_i$. [@montgomeryIntroductionLinearRegression2021] -->

<!-- Another version is that: Under Models II - VII, if $\lambda'\beta$ is estimable and $\hat\beta$ is any solution to the normal equations, then $\lambda'\hat\beta$ is a linear unbiased estimator of $\lambda'\beta$ and, under Model II, the variance of $\lambda'\hat\beta$ is uniformly less than that of any other linear unbiased estimator of $\lambda'\beta$ [@kimLectureNotes2020,IX, Theorem E13, p38]. -->

<!-- ::: -->

<!-- Ordinary Least Squares (OLS) method can be used to estimate the coefficients $\boldsymbol{\beta}$. -->

<!-- in equation @ref(eq:lm)  -->

<!-- The dimension of $\mathbf{X}$ is $n\times p$, which means the data contain $n$ observations and $p-1$ predictors. The $p\times1$ vector of least-squares estimators is denoted as $\hat\beta$ and the solution to the normal equations is $\boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y}$ -->

<!-- and $\hat\sigma^2=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta})'(\mathbf{y-X}\boldsymbol{\hat\beta})$ -->

<!-- \begin{equation} -->

<!-- \boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y} -->

<!-- (\#eq:lsq-e) -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \hat\sigma^2=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta})'(\mathbf{y-X}\boldsymbol{\hat\beta}) -->

<!-- (\#eq:lsq-v) -->

<!-- \end{equation} -->

<!-- Here requires $\mathbf{X'X}$ are invertible, that is, the covariates are linearly independent if $\mathbf{X}$ has rank $p$ [@kimLectureNotes2020, V., Definition, p.22]. -->

<!-- When the observations are not independent or have unequal variances, the covariance matrix of error is not identity matrix. The assumption of regression model $V[\boldsymbol{\varepsilon}]=\sigma^2\mathbf{I}$ doesn't hold.  -->

<!-- Denote $\mathbf{V}$ is a known $n\times n$ positive definite matrix and $V[\boldsymbol{\varepsilon}]=\sigma^2\mathbf{V}$. -->

<!-- Then, there exists an $n\times n$ symmetric matrix $\mathbf{K}$ with rank $n$ and $\mathbf{V}=\mathbf{KK'}$. Let -->

<!-- \begin{equation} -->

<!-- \mathbf{z}=\mathbf{K'y},\ \mathbf{B}=\mathbf{K^{-1}X}, \text{and}\ \boldsymbol{\eta}=\mathbf{K'}\boldsymbol{\varepsilon} -->

<!-- \end{equation} -->

<!-- The linear model becomes $\mathbf{z}=\mathbf{B}\boldsymbol{\beta}+\boldsymbol{\eta}$ and $V[\boldsymbol{\eta}]=\sigma^2\mathbf{I}$. -->

<!-- If the model is full rank, that is $rank(\mathbf{X})=p$ then $\mathbf{X'V^{-1}X}$ is invertible. -->

<!-- The generalized least squares solution is $\boldsymbol{\hat\beta}_{GLS}=(\mathbf{X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y}$ -->

<!--  and $\hat\sigma^2_{GLS}=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})'\mathbf{V^{-1}}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})$ -->

<!-- \begin{equation} -->

<!-- \boldsymbol{\hat\beta}_{GLS}=(\mathbf{X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y} -->

<!-- (\#eq:glsq-e) -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \hat\sigma^2_{GLS}=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})'\mathbf{V^{-1}}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS}) -->

<!-- (\#eq:glsq-v) -->

<!-- \end{equation} -->

<!-- ### Standardized coefficients -->

<!-- It is inevitable that the units of covariates $\mathbf{X}$ are very different in many studies. One thing can be done is to standardize the values of coefficients [@zhangHowBuiltEnvironment2012;@leeComparingImpactsLocal2020]. -->

<!-- Unit normal scaling or unit length scaling can convert $\hat \beta_j$ to dimensionless regression coefficient, which is called standardized regression coefficients. A simple expression of standardized coefficients is that $\hat b_j= \hat\beta_j\sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}{\sum_{i=1}^{n}(y_{i}-\bar y)^2}}$,$j=1,2,...(p-1)$, and -->

<!-- $\hat\beta_0=\bar y - \sum_{j=1}^{p-1}\hat\beta_j\bar x_j$ -->

<!-- Let -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- z_{ij}=&\frac{x_{ij}-\bar x_j}{\sqrt{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}}\\ -->

<!-- y^{0}_{i}=&\frac{y_{i}-\bar y}{\sqrt{\sum_{i=1}^{n}(y_{i}-\bar y)^2}} -->

<!-- \end{split} -->

<!-- (\#eq:standize) -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- \mathbf{\hat b}=&(\mathbf{Z'Z})^{-1}\mathbf{Z'}\mathbf{y^{0}},\ \text{or}\\ -->

<!-- \hat b_j= &\hat\beta_j\sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}{\sum_{i=1}^{n}(y_{i}-\bar y)^2}},\ j=1,2,...(p-1),\text{ and}\\ -->

<!-- \hat\beta_0=&\bar y - \sum_{j=1}^{p-1}\hat\beta_j\bar x_j -->

<!-- \end{split} -->

<!-- (\#eq:stand-coef) -->

<!-- \end{equation} -->

<!-- Note that $\mathbf{Z'Z}$ correlations matrix. -->

<!-- \begin{equation} -->

<!-- \mathbf{Z'Z}=\begin{bmatrix}  -->

<!-- 1 & r_{12} & r_{13} & \dots & r_{1k} \\   -->

<!-- r_{21} & 1 & r_{23} & \dots & r_{2k} \\   -->

<!-- r_{31} & _{32} & 1 & \dots & r_{3k} \\   -->

<!-- \vdots & \vdots & \vdots & \ddots & \vdots \\   -->

<!-- r_{k1} & r_{k2} & _{k3} & \dots & 1  \end{bmatrix},\quad  -->

<!-- \mathbf{Z'}\mathbf{y^{0}}=\begin{bmatrix}  -->

<!-- r_{1y} \\ r_{2y} \\ r_{3y} \\ \vdots \\ r_{ky} \end{bmatrix} -->

<!-- (\#eq:corr-matrix) -->

<!-- \end{equation} -->

<!-- where -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- r_{ij}=&\frac{\sum_{u=1}^{n}(x_{ui}-\bar x_i)(x_{uj}-\bar x_j)}{\sqrt{\sum_{u=1}^{n}(x_{ui}-\bar x_i)^2\sum_{u=1}^{n}(x_{uj}-\bar x_j)^2}}\\ -->

<!-- r_{jy}=&\frac{\sum_{u=1}^{n}(x_{uj}-\bar x_j)(y_{u}-\bar y)}{\sqrt{\sum_{u=1}^{n}(x_{uj}-\bar x_j)^2\sum_{u=1}^{n}(y_{u}-\bar y)^2}} -->

<!-- \end{split} -->

<!-- (\#eq:corr-1) -->

<!-- \end{equation} -->

<!-- Here $r_{ij}$ is the simple correlation between $x_i$ and $x_j$. $r_{jy}$ is the simple correlation between $x_j$ and $y$ -->

<!-- ### Elasticity -->

<!-- As introduced in previous chapter, **elasticity** is also commonly used to determine the relative importance of a variable in terms of its influence on a dependent variable [@ewingTravelBuiltEnvironment2010]. It is generally interpreted as the percent change in the dependent variable induced by a 1% change in the independent variable [@mccarthyTransportationEconomicsTheory2001]  -->

<!-- Table @ref(tab:elas-formula). -->

<!-- The values of elasticity are calculated by $e_i=\beta_i\frac{X_i}{Y_i}\approx\frac{\partial Y_i}{\partial X_i}\frac{X_i}{Y_i}$ -->

```{r,eval=F}
library(kableExtra)
kbl(data.frame(
  `Model`=c('Linear','Log-linear','Linear-log','Log-log','Logit','Poisson','NB'),
  `Marginal Effects` = c('$\\beta$','$\\beta Y_i$','$\\beta\\frac{1}{X_i}$', '$\\beta\\frac{Y_i}{X_i}$','$\\beta p_i(1-p_i)$', '$\\beta\\lambda_{i}$','$\\beta \\lambda_{i}$'),
Elasticity= c('$\\beta\\frac{X_i}{Y_i}$','$\\beta X_i$','$\\beta\\frac{1}{Y_i}$','$\\beta$','$\\beta X_i(1-p_i)$','$\\beta X_i$','$\\beta X_i$')
)  , booktabs = TRUE, label = 'elas-formula', digit=2, escape = F, #, align = "llr"
  caption = 'Elasticity Estimates for Various Functional Forms'
) %>% kable_classic(full_width = F,font_size = 7)
```

<!-- It might be a typo that @ewingTravelBuiltEnvironment2010 use the formula of $\beta \bar X\left(1-\frac{\bar Y}{n}\right)$ for Logit model. -->

<!-- In Poisson model and Negative Binomial model, $\lambda_i=\exp[\mathbf{x}_i'\boldsymbol{\beta}]$ [@greeneEconometricAnalysis2018, eq.18-17,21]. -->

<!-- For truncated Poisson model: $\delta_i=\frac{(1-P_{i,0}-\lambda_i P_{i,0})}{(1-P_{i,0})^2}\cdot\lambda_i\beta$ [@greeneEconometricAnalysis2018, eq.18-23]. -->

<!-- Hurdle model will give separate marginal(partial) effects [@greeneEconometricAnalysis2018, example 18.20]. -->

<!-- ### Inference -->

Point estimation in the last section tells us what the effect size is. Statistical inference tells us how it is likely to be true. Most travel-urban form studies are significance-centered. In a typical paper on this topic, if the p-value of one factor is small enough, the significance of regression and the significance of coefficients the estimate of that factor would be accepted. The p-value is just a piece of inference methods. Analysis of variance (ANOVA), hypothesis test, and interval estimation provide complete information.

This section shows that statistical inference relies on some probability distributions. Hence, it requires more conditions than least-squares methods. Checking model adequacy is a necessary step before reaching any conclusion.

Analysis of Variance (ANOVA) is the fundamental approach in regression analysis[@casellaStatisticalInference2002, Ch.11]. ANOVA is a worthwhile method but is rarely seen in travel-urban form studies. @serbanicaSustainableCitiesCentral2017 use a two-way ANOVA to "compare the effects of a country group, population growth and city size on green performance." @laneTAZlevelVariationWork2011 applies the multivariate analysis of covariance (MANCOVA) on examining how "the variation in proportional changes in driving is related to variation in the covariates." This study demonstrates that coefficients are not the only measurement of influencing factors. ANOVA may explain how the effects appear or disappear in various spatial scales. It is interesting to observe the dynamic of $SSR$ of D-variables when the variance structure changes along the scales. @gelmanAnalysisVarianceWhy2005 shows that ANOVA is essential for hierarchical models.

The p-value is one outcome of hypothesis testing. Statisticians have proved that p-value itself is not sufficient evidence for hypothesis test [@hubbardWhyValuesAre2008; @halseyFickleValueGenerates2015], and it should not be the only criteria for statistical inference [@wassersteinASAStatementPValues2016]. Confidence intervals (CI) are a better measurement that can exclude the values that are unlikely to exist in the population [@ranstamWhyPvalueCulture2012]. It can represent the uncertainty better than standard error because $se$ also depends on the sample size. Most travel-urban form studies provide $se$ values of estimates. A few of them give confidence intervals. It calls for empirical or simulated studies to show how CI can tell more about the effect size.

<!-- Through testing hypotheses, the results -->

<!-- After estimating the effect size, statistical inference infers the properties of population.  -->

<!--  show how likely the estimates can represent the population. And interval estimation provide than point estimates. -->

<!-- This section would introduce some underlying principles.  -->

<!-- ### Analysis of Variance -->

<!-- Analysis of Variance (ANOVA) is the fundamental approach in regression analysis. Actually, this method analyses the variation in means rather than variances themselves [@casellaStatisticalInference2002, Ch.11].  -->

<!-- The basic idea is -->

<!-- \mathbf{y'y}=&\mathbf{y'Hy}+\mathbf{y'}(\mathbf{I}-\mathbf{H})\mathbf{y}\\ -->

<!-- \mathbf{y'y}=&\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}+\mathbf{y'y}-\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}\\ -->

<!-- \mathbf{y'y}-n\bar y^2=&\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}-n\bar y^2+\mathbf{y'y}-\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}\\ -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- \mathrm{SST} =& \mathrm{SSR} + \mathrm{SSE}\\ -->

<!-- \sum(y-\bar y)^2=&\sum(\hat y-\bar y)^2+\sum(y-\hat y)^2 -->

<!-- \end{split} -->

<!-- (\#eq:ss) -->

<!-- \end{equation} -->

<!-- where $\mathrm{SST}$ is Sum of Squares Total, $\mathrm{SSR}$ is Sum of Squares Regression, and $\mathrm{SSE}$ is Sum of Square Error.  -->

<!-- For Generalized Least Squares method, $\mathrm{SST}=\mathbf{y'V^{-1}y}$, $\mathrm{SSR}= \boldsymbol{\hat\beta'}\mathbf{B'z}=\mathbf{y'V^{-1}X(X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y}$, and $\mathrm{SSE}=\mathrm{SST}-\mathrm{SSR}$. -->

<!-- $\mathrm{SSR}$ represents the part of variance can be explained by the model. -->

<!-- $\mathrm{SSE}=\mathbf{e'e}$ is the unknown part and $\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=\mathbf{y}-\mathbf{X}\boldsymbol{\hat\beta}$. -->

<!-- This process is called variance decomposition. -->

<!-- Given the estimated coefficients, the model can get the fitted values of response as: -->

<!-- \begin{equation} -->

<!-- \mathbf{\hat y}=\mathbf{X}\boldsymbol{\hat\beta}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'y}= \mathbf{Hy} -->

<!-- (\#eq:fitted-y) -->

<!-- \end{equation} -->

<!-- where $\mathbf{H}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'}$ is hat matrix and $\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=\mathbf{y}-\mathbf{X}\boldsymbol{\hat\beta}=(\mathbf{I}-\mathbf{H})\mathbf{y}$ -->

<!-- Once the linear relationship holds, the response $\mathbf{y}$ can be decomposed to -->

<!-- ### Hypothesis Test -->

<!-- **Significance of regression** means if the linear relationship between response and predictors is adequate. The hypotheses for testing model adequacy are -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- H_0:&\quad \beta_0 = \beta_1 = \cdots =\beta_{p-1}=0\\ -->

<!-- H_1:&\quad \text{at least one } \beta_j \neq 0,\ j=0,1,...,(p-1)\\ -->

<!-- \end{split} -->

<!-- (\#eq:hyp-1) -->

<!-- \end{equation} -->

<!-- By Theorem D14 [@kimLectureNotes2020,XX, p.90], if an $n\times1$random vector $\mathbf{y}\sim N(\boldsymbol{\mu},\mathbf{I})$, then -->

<!-- $\mathbf{y'y} \sim \chi^2(n,\frac12\boldsymbol{\mu'\mu})$ -->

<!-- Recall the assumption of $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$. -->

<!-- By the additive property of $\chi^2$ distribution, $\frac{MSE}{\sigma^2}=\frac{\mathbf{y'(I-H)y}}{(n-p)\sigma^2} \sim \chi^2_{(n-p)}$ and $\frac{MSR}{\sigma^2}=\frac{\mathbf{y'Hy}}{(p-1)\sigma^2} \sim \chi^2_{(p-1)}$. -->

<!-- Though $\sigma^2$ is usually unknown, by the relationship between $\chi^2$ and $F$ distributions, -->

<!-- \begin{equation} -->

<!-- F_0=\frac{MSE}{MSR} \sim F_{(p-1),(n-p),\lambda} -->

<!-- \end{equation}  -->

<!-- where $\lambda$ is the non-centrality parameter. It allows to test the hypotheses given a significance level $\alpha$. If test statistic $F_0>F_{\alpha,(p-1),(n-p)}$, then one can reject $H_0$. -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- \frac{MSE}{\sigma^2}=&\frac{\mathbf{y'(I-H)y}}{(n-p)\sigma^2} \sim \chi^2_{(n-p)}\\ -->

<!-- \frac{MSR}{\sigma^2}=&\frac{\mathbf{y'Hy}}{(p-1)\sigma^2} \sim \chi^2_{(p-1)} -->

<!-- \end{split} -->

<!-- (\#eq:msemsr) -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \mathbf{y'y} \sim \chi^2(n,\frac12\boldsymbol{\mu'\mu}) -->

<!-- (\#eq:chisq) -->

<!-- \end{equation} -->

<!-- **Significance of coefficients** is to test a specific coefficient, the hypothesis is H$_0$: $\beta_j =0$ and H$_1$: $\beta_j \neq 0$. -->

<!-- $\boldsymbol{\hat\beta}$ is a linear combination of $\mathbf{y}$. Based on the assumption of $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$, it can be proved that $\boldsymbol{\hat\beta}\sim N (\boldsymbol{\beta},\sigma^2(\mathbf{X'X})^{-1})$ and -->

<!-- \begin{equation} -->

<!-- t_0=\frac{\hat\beta_j}{se(\hat\beta_j)}=\frac{\hat\beta_j}{\sqrt{\hat\sigma^2C_{jj}}} \sim t_{(n-p)} -->

<!-- \end{equation}  -->

<!-- where $C_{jj}$ is the element at the $j$ row and $j$ column of $(\mathbf{X'X})^{-1}$. If $|t_0|< t_{\alpha/2,(n-p)}$, then the test failed to reject the $H_0$, this predictor can be removed from the model. This test is called partial or marginal test because the test statistic for $\beta_j$ depends on all the predictors in the model. -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- H_0:&\quad \beta_j =0\\ -->

<!-- H_1:&\quad \beta_j \neq 0\\ -->

<!-- \end{split} -->

<!-- (\#eq:hyp-2) -->

<!-- \end{equation} -->

<!-- ### Confidence Intervals -->

<!-- Above results can also construct the confidence interval for each coefficient. A $100(1-\alpha)$ confidence interval for $\beta_j$ is $\hat\beta_j-t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}\le \beta_j \le \hat\beta_j+t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}$. -->

<!-- \begin{equation} -->

<!-- \hat\beta_j-t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}\le \beta_j \le \hat\beta_j+t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}} -->

<!-- \end{equation} -->
