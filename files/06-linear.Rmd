# (PART\*) Part II Methods and Data  {-} 


# Linear Models

This Chapter will not give a complete introduction of linear regression.
It only focus on some critical issues which are related with Travel-Urban Form models.
This chapter only discusses the VMT-urban form models because the response are continue variables in regular linear models.
The part of mode choice model is placed in Chapter of Generalized linear models.



## Assumptions

- Additive and linear

For linear models, the most important assumptions are the additive and linear relationship between the response and predictors.
Gravity Law discloses that travel distance has a multiplicative (inverse) relationship with the 'masses' of two places. If the population size can be a representative of built environment, the additive relationship will not hold.
Previous studies also shows that the effect sizes of built environment with respect of travel are small and complex.
There is not sufficient evidence to support or against the linear hypothesis.

- IID

Another essential assumption is that both predictors and response are Independent Identically Distributed (IID) random variables.

Unfortunately, many of the predictors are correlated.
Moreover, the observations from various cities, regions, or counties are very unlikely identical. This issue is called heteroscedasticity. Related contents are in Section of Diagnousis and Validation.

When data is generated from experimental design, the treatments $X$ could be fixed variables. But travel-urban form model is observational studies and nothing can be controlled as in lab. Thus, all $X$s are random variables.

- Second Moment

The first- and second-moment are about the expected value, the variances, and the covariances among the random errors.
Once the conditions of the first- and second-moment are satisfied, the Gauss - Markov theorem \@ref(thm:g-m), proves that least-square method could give the minimum-variance unbiased estimators (MVUE) or called the best linear unbiased estimators (BLUE).
These conditions are not strict and make regression method widely applicable.

::: {.theorem #g-m name="Gauss - Markov theorem"}

For the regression model \@ref(eq:lm) with the assumptions $E(\varepsilon) = 0$, $Var(\varepsilon) = \sigma^2$, and uncorrelated errors, the least-squares estimators are unbiased and have minimum variance when compared with all other unbiased estimators that are linear combinations of the $y_i$. (Montgomery et al., 2021)

Another version is that:

Under Models II - VII, if $\lambda'\beta$ is estimable and $\hat\beta$ is any solution to the normal equations, then $\lambda'\hat\beta$ is a linear unbiased estimator of $\lambda'\beta$ and, under Model II, the variance of $\lambda'\hat\beta$ is uniformly less than that of any other linear unbiased estimator of $\lambda'\beta$ (IX, Theorem E13, p38)

:::


- Normal Distribution

When conducting Hypothesis test and confidence intervals, the required assumption is $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$.
Maximum Likelihood Methods also requires this assumption.

Evidence has demonstrated that travel distance is not Normal distributed.
The Zipf's law also prove that travel distance follows a power distribution.

## Transforamtions of Predictors

Above discussion has shown that some assumptions are not valid for VMT and urban form variables.
Gravity law and Zipf's law also imply that taking logarithm transformations is a proper choice.

Among the above literature, some regression models take logarithm transforms on all variables, while others only transform one or a part. Even though they have various data sources, it is unlikely that they are all correct or equivalent.


## Multiple Linear Regression

### Parameter Estimations

Least-Squares method can be used to estimate the coefficients $\beta$ in equation \@ref(eq:lm)
The dimension of $\mathbf{X}$ is $n\times p$, which means the data contain $n$ observations and $p-1$ predictors.
The $p\times1$ vector of least-squares estimators is denoted as $\hat\beta$ and the solution to the normal equations is

$$\begin{equation}
\boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y}
(\#eq:lsq)
\end{equation}$$

Here requires $\mathbf{X'X}$ are invertible, that is, the covariates are linearly independent if $\mathbf{X}$ has rank $p$ (V., Definition, p.22). 

Given the estimated coefficients, the model can give the fitted values of response as:

$$\begin{equation}
\mathbf{\hat y}=\mathbf{X}\boldsymbol{\hat\beta}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'y}= \mathbf{Hy}
(\#eq:fitted-y)
\end{equation}$$

where $\mathbf{H}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'y}$ is hat matrix. 
There is always a difference between observed $\mathbf{y}$ and fitted $\mathbf{\hat y}$, which can not be explained by the model.
This part is called residual.


$$\begin{equation}
\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=\mathbf{y}-\mathbf{X}\boldsymbol{\hat\beta}=(\mathbf{I}-\mathbf{H})\mathbf{y}
(\#eq:residual)
\end{equation}$$




### Analysis of Variance

Analysis of Variance (ANOVA) is the fundamental approach in regression analysis.
Actually, this method analysis the variation in means rather than variances themselves (Casella & Berger, 2002, Ch.11).

Once the linear relationship holds, the response $\mathbf{y}$ can be decomposed to

$$\begin{equation}
\begin{split}
\mathbf{y'y}=&\mathbf{y'Hy}+\mathbf{y'}(\mathbf{I}-\mathbf{H})\mathbf{y}\\
\mathbf{y'y}=&\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}+\mathbf{y'y}-\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}\\
\mathbf{y'y}-n\bar y^2=&\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}-n\bar y^2+\mathbf{y'y}-\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}\\
\sum(y-\bar y)^2=&\sum(\hat y-\bar y)^2+\sum(y-\hat y)^2\\
SST =& SSR + SSE
\end{split}
(\#eq:ss)
\end{equation}$$

where $SST$ is Sum of Squares Total, $SSR$ is Sum of Squares Regression, and $SSE$ is Sum of Square Error.
$SSE=\mathbf{e'e}$ represents the unknown part of model. 

- Goodness of fit

This structure tell us how good the model can explain the data. Coefficient of Determination $R^2$ is a proportion to assess the quality of fitted model.

$$\begin{equation}
R^2 =\frac{SSR}{SST}=1-\frac{SSE}{SST}
(\#eq:rsq)
\end{equation}$$

when $R^2$ is close to $1$, the most of variation in response can be explained by the fitted model. 
Although $R^2$ is not the only criteria of a good model, it is often available in most published papers. 
Recall the discussion in Part I, the aggregated data will eliminate the difference among individuals, households, or neighborhoods. In the new variance structure, $SSE$ will be much less than disaggregated model.
The $R^2$ in many disaggregate studies are around 0.3, while the $R^2$ in some aggregate studies can reach 0.8. A seriously underfitting model's outputs could be biased and unstable. 


A fact is that adding predictors into the model will never decrease $R^2$. Thus the models with different number of predictors is not comparable. Adjusted $R^2$ can address this issue by introducing degree of freedom. 
The degree of freedom denotes the amount of information required to know.

$$\begin{equation}
\begin{split}
df_T =& df_R + df_E\\
n-1=&(p-1)+(n-p)
\end{split}
(\#eq:df)
\end{equation}$$

Then, the mean square (MS) of each sum of squares (SS) can be calculated by $MS=SS/df$.
The mean square error $MSE$ is also called as the expected value of error variance $\hat\sigma^2=MSE=SSE/(n-p)$. $n-p$ is the degree of freedom. Then adjusted $R^2$ is


$$\begin{equation}
R_{adj}^2 = 1-\frac{MSE}{MST} = 1-\frac{SSE/(n-p)}{SST/(n-1)}
(\#eq:rsq)
\end{equation}$$


### Hypothesis Test

- Significance of Regression

Significance of regression means if the linear relationship between response and predictors is adequate.
The hypotheses for testing model adequacy are

$$\begin{equation}
\begin{split}
H_0:&\quad \beta_0 = \beta_1 = \cdots =\beta_{p-1}=0\\
H_1:&\quad \text{at least one } \beta_j \neq 0,\ j=0,1,...,(p-1)\\
\end{split}
(\#eq:hyp-1)
\end{equation}$$

By Theorem D14 (XX, p.90), if an $n\times1$random vector $\mathbf{y}\sim N(\boldsymbol{\mu},\mathbf{I})$, then

$$\begin{equation}
\mathbf{y'y} \sim \chi^2(n,\frac12\boldsymbol{\mu'\mu})
(\#eq:chisq)
\end{equation}$$


Recall the assumption of $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$.  
By the additive property of $\chi^2$ distribution, 

$$\begin{equation}
\frac{MSE}{\sigma^2}=\frac{\mathbf{y'(I-H)y}}{(n-p)\sigma^2} \sim \chi^2_{(n-p)}\\
\frac{MSR}{\sigma^2}=\frac{\mathbf{y'Hy}}{(p-1)\sigma^2} \sim \chi^2_{(p-1)}\\
\end{equation}$$

Though $\sigma^2$ is usually unknown, by the relationship between $\chi^2$ and $F$ distributions,

$$\begin{equation}
F_0=\frac{MSE}{MSR} \sim F_{(p-1),(n-p),\lambda}\\
\end{equation}$$
where $\lambda$ is the non-centrality parameter.
It allows to test the hypotheses given a significance level $\alpha$. If test statistic $F_0>F_{\alpha,(p-1),(n-p)}$, then one can reject $H_0$.

If a VMT-urban form model added many predictors but adjusted $R^2$ is still low, the association between travel distance and built environment might be spurious.


- Significance of Coefficients

For testing a specific coefficient, the hypothesis is

$$\begin{equation}
\begin{split}
H_0:&\quad \beta_j =0\\
H_1:&\quad \beta_j \neq 0\\
\end{split}
(\#eq:hyp-2)
\end{equation}$$

$\boldsymbol{\hat\beta}$ is a linear combination of $\mathbf{y}$.
Based on the assumption of $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$, it can be proved that $\boldsymbol{\hat\beta}\sim N (\boldsymbol{\beta},\sigma^2(\mathbf{X'X})^{-1})$ and

$$\begin{equation}
t_0=\frac{\hat\beta_j}{se(\hat\beta_j)}=\frac{\hat\beta_j}{\sqrt{\hat\sigma^2C_{jj}}} \sim t_{(n-p)}\\
\end{equation}$$
where $C_{jj}$ is the element at the $j$ row and $j$ column of $(\mathbf{X'X})^{-1}$.
If $|t_0|< t_{\alpha/2,(n-p)}$, then the test failed to reject the $H_0$, this predictor can be removed from the model.
This test is called partial or marginal test because the test statistic for $\beta_j$ depends on all the predictors in the model.

- Confidence Intervals

Above results can also construct the confidence interval for each coefficient. A $100(1-\alpha)$ confidence interval for $\beta_j$ is

$$\begin{equation}
\hat\beta_j-t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}\le \beta_j \le \hat\beta_j+t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}
\end{equation}$$


## Models Specifications

### Variables Selections

- Underfitting models


- Overfitting models


- Interaction Effects

- Polynomial Regression Models

"There are different types of model specification errors in regression analysis: "

(1) necessary independent variable(s) is (are) missing from the model (i.e., omission error) (Greene 2012, 96--8, Section 4.3.2); 

(2) irrelevant independent variable(s) is (are) added to the model (i.e., commission error) (Greene 2012, 98, Section 4.3.3); 

(3) independent variables have an incorrect functional form (Ramsey 1969; Thursby and Schmidt 1977; Hausman 1978; Davidson and MacKinnon 1981); 

(4) variables in the model are not accurately measured (i.e., measurement error) (Wooldridge 2015, 287--92, Section 9.4); and 

(5) the model is not stand-alone but instead belongs to a system of simultaneous equations (Ramsey 1969; Hausman 1978).


- Theoretical considerations






### Subset Selection

Empirical or methodological considerations

### Shrinkage Methods

Ridge Regression and Lasso

### Dimention Reduction Methods

Principal Components Regression (PCR)

Partial Least Square (PLS)


By dint of some synthetic variables, the disaggregated model's $R^2$ can be over 0.5. But the risk is these techniques may describe the data themselves, and the results cannot be generalized.  It is worthy of a deeper investigation.

```{r,eval=F}
# Compute k-means with k = 3
set.seed(123)
res.km <- kmeans(scale(iris[, -5]), 3, nstart = 25)
# K-means clusters showing the group of each individuals
fviz_cluster(res.km, data = iris[, -5],
             palette = "set3", #c("#2E9FDF", "#00AFBB", "#E7B800")
             geom = "point", pointsize=1, # "text"
             ellipse = T,ellipse.type ="t",   #'confidence'"euclid""convex"
             ellipse.level = 0.75, ellipse.alpha = 0.1,
             ggtheme = theme_bw(), main="Models with trip response variable",xlab="data", ylab="models"
             )
```

```{r,eval=F}
res.km <- kmeans(scale(USArrests), 4, nstart = 25)
fviz_cluster(res.km, data = USArrests,
             palette = "set3", #c("#2E9FDF", "#00AFBB", "#E7B800")
             geom = "point", pointsize=1, # "text"
             ellipse = T,ellipse.type ="t",   #'confidence'"euclid""convex"
             ellipse.level = 0.75, ellipse.alpha = 0.1,legend = "none",
             ggtheme = theme_bw(), main="Models with trip response variable",xlab="data", ylab="models"
             )
```

```{r modelcluster, eval=F, out.width='50%',fig.cap="A diagram of model cluster"}
# Dimension reduction using PCA
res.pca <- prcomp(USArrests,  scale = TRUE) #iris[, -5]
# Coordinates of individuals
ind.coord <- as.data.frame(get_pca_ind(res.pca)$coord)
# Add clusters obtained using the K-means algorithm
res.km <- kmeans(scale(USArrests), 4, nstart = 25)
ind.coord$cluster <- factor(res.km$cluster)
# Add Species groups from the original data sett
# ind.coord$Species <- iris$Species
eigenvalue <- round(get_eigenvalue(res.pca), 1)
variance.percent <- eigenvalue$variance.percent
p <- ggscatter(ind.coord,x = "Dim.1", y = "Dim.2",
  color = "cluster", palette = "npg", title="Find some proper Model clusters",
  ellipse = TRUE, ellipse.type = "t", #"convex"
  ellipse.border.remove=T, ellipse.level = 0.85,
  shape = "cluster", #"Species"
  size = 1.5,legend = "none", ggtheme = theme_bw(),
  xlab = "Various scale or scope",ylab = "Various probability distribution",
) + # theme(axis.text = element_blank()) +
  stat_mean(aes(color = cluster), size = 8,shape = c(1,2,0,3))

ggpar(p,tickslab=F,ticks=F)
```



## Fixed and Mixed Effects

There are two different settings in experimental design. One assigns the treatments having fixed effects on the response. The other assign the treatments having random effects on the response.
In fixed-effect model, the interest is the effects of levels themselves. The levels of the factor are chosen to test the differences among these specific levels. The chosen levels exhaust the pop
The fixed effects across cases are constant. 


While in random-effect model, the interest is the underlying population. The random effects are vary.


In random-effect models, the levels are selected at random from a large number of levels.


In VMT-urban form models, the interest is the effects of built-environment variables, not the specific region's impact.

## Bayesian approaches (Opt.)

\pagebreak

## Diagnousis and Validation

In general, modeling is a case-by-case work. Researchers may have their preferred model by weighing the sensitivity and robustness even given the same hypothesis and data. Hence, effective comparison or classification requires a few conditions:

An effective comparison also should present the full results of model diagnosis and validation. In practice, a more certain modeling framework allows scholars to present more general conclusions and speak with a clear voice to the public and decision-makers.

-   Checking the assumption of Independent Identically Distributed random variables (IID)

### Checking Residuals

The residual plots prove log-transform of VMT is reasonable because it has a highly right-skewed probability distribution. Only looking at the $R^2$ and p-values cannot disclose this feature. This result suggests that residual diagnosis is an essential step for modeling validation.

### Heteroscedasticity

Bartlett's (1937) likelihood ratio test (Ravishanker & Dey, 2020, 8.1.3, p.288)

### Multicollinearity

If the Variance Inflation Factor of a predictor variable is high (e.g. VIF=9), this means that the standard error for the coefficient of that predictor variable is 3 times larger than if that predictor had 0 correlation with the other predictors. Table \@ref(tab:vifdd) shows that the county-level model has serious multicollinearity issue. The double log transformation seems to enlarge the VIF values on D1a and D3a (Table \@ref(tab:vifdd),\@ref(tab:vif)).





## Interpretation

### Standardized coefficients

The value of $\hat \beta_j$ means that, given all other coefficients fixed, for each change of one unit in $x_j$, the average change in the mean of $Y$.
However, the units of predictors $\mathbf{X}$ are very different. Hence, the values of coefficients are not comparable.

Unit normal scaling or Unit length scaling can convert $\hat \beta_j$ to dimensionless regression coefficient, which is called standardized regression coefficients. Let

$$\begin{equation}
\begin{split}
z_{ij}=&\frac{x_{ij}-\bar x_j}{\sqrt{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}},\quad \\
y^{0}_{i}=&\frac{y_{i}-\bar y}{\sqrt{\sum_{i=1}^{n}(y_{i}-\bar y)^2}}
\end{split}
(\#eq:standize)
\end{equation}$$


$$\begin{equation}
\begin{split}
\mathbf{\hat b}=&(\mathbf{Z'Z})^{-1}\mathbf{Z'}\mathbf{y^{0}},\ \text{or}\\
\hat b_j= &\hat\beta_j\sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}{\sum_{i=1}^{n}(y_{i}-\bar y)^2}},\ j=1,2,...(p-1),\text{ and}\\
\hat\beta_0=&\bar y - \sum_{j=1}^{p-1}\hat\beta_j\bar x_j
\end{split}
(\#eq:stand-coef)
\end{equation}$$

 Note that $\mathbf{Z'Z}$ correlations matrix.
 
$$\begin{equation}
\begin{split}
\mathbf{Z'Z}=&\begin{bmatrix} 
1 & r_{12} & r_{13} & \dots & r_{1k} \\  
r_{21} & 1 & r_{23} & \dots & r_{2k} \\  
r_{31} & _{32} & 1 & \dots & r_{3k} \\  
\vdots & \vdots & \vdots & \ddots & \vdots \\  
r_{k1} & r_{k2} & _{k3} & \dots & 1  \end{bmatrix},\quad 
\mathbf{Z'}\mathbf{y^{0}}=&\begin{bmatrix} 
r_{1y} \\ r_{2y} \\ r_{3y} \\ \vdots \\ r_{ky} \end{bmatrix}
\end{split}
(\#eq:corr-matrix)
\end{equation}$$
 
where

$$\begin{equation}
\begin{split}
r_{ij}=&\frac{\sum_{u=1}^{n}(x_{ui}-\bar x_i)(x_{uj}-\bar x_j)}{\sqrt{\sum_{u=1}^{n}(x_{ui}-\bar x_i)^2\sum_{u=1}^{n}(x_{uj}-\bar x_j)^2}}\\
r_{jy}=&\frac{\sum_{u=1}^{n}(x_{uj}-\bar x_j)(y_{u}-\bar y)}{\sqrt{\sum_{u=1}^{n}(x_{uj}-\bar x_j)^2\sum_{u=1}^{n}(y_{u}-\bar y)^2}}
\end{split}
(\#eq:corr-1)
\end{equation}$$

where $r_{ij}$ is the simple correlation between $x_i$ and $x_j$. $r_{jy}$ is the simple correlation between $x_j$ and $y$

It seems that standardized regression coefficients are comparable. However, the value of $\hat b_j$ depends on other predictors. Therefore, comparison between different models is still problematic.


### Elasticity

Definition: Commonly used to determine the relative importance of a variable in terms of its influence on a dependent variable. It is generally interpreted as the percent change in the dependent variable induced by a 1% change in the independent variable.

$$e_i=\beta_i\frac{X_i}{Y_i}\approx\frac{\partial Y_i}{\partial X_i}\frac{X_i}{Y_i}$$[^48]

[^48]: McCarthy, P.S., Transportation Economics Theory and Practice: A Case Study Approach. Blackwell, Boston, 2001.

+------------+-------------------------+-----------------------------------+
| Model      | Marginal Effects        | Elasticity                        |
+:==========:+:=======================:+:=================================:+
| Linear     | $\beta$                 | $\beta\frac{X_i}{Y_i}$            |
+------------+-------------------------+-----------------------------------+
| Log-linear | $\beta Y_i$             | $\beta X_i$                       |
+------------+-------------------------+-----------------------------------+
| Linear-log | $\beta\frac{1}{X_i}$    | $\beta\frac{1}{Y_i}$              |
+------------+-------------------------+-----------------------------------+
| Double log | $\beta\frac{Y_i}{X_i}$  | $\beta$                           |
+------------+-------------------------+-----------------------------------+
| Mixed      |                         |                                   |
+------------+-------------------------+-----------------------------------+
| Logit      | $\beta p_i(1-p_i)$      | $\beta X_i(1-p_i)$;[^49]          |
+------------+-------------------------+-----------------------------------+
| Poisson    | $\beta \lambda_i$;[^50] | $\beta X_i$                       |
+------------+-------------------------+-----------------------------------+
| NB         | $\beta \lambda_i$       | $\beta X_i$                       |
+------------+-------------------------+-----------------------------------+
| Tr-Pois    | $\delta_i$;[^51]        |                                   |
+------------+-------------------------+-----------------------------------+
| Hurdle-P   | separately [^52]        |                                   |
+------------+-------------------------+-----------------------------------+

: Elasticity Estimates for Various Functional Forms

[^49]: $\beta \bar X\left(1-\frac{\bar Y}{n}\right)$ (Ewing and Cervero, 2010)

[^50]: $\lambda_i=\exp[\mathbf{x}_i'\boldsymbol{\beta}]$ (Greene, 2012, eq.18-21)

[^51]: $\delta_i=\frac{(1-P_{i,0}-\lambda_i P_{i,0})}{(1-P_{i,0})^2}\cdot\lambda_i\beta$ (Greene, 2012, eq.18-23)

[^52]: (Greene, 2012, p.865)

### Combined effects?

## Summary

Another attempt tries the method of structural equation modeling (SEM). The two studies capture higher elasticities of per capita VMT with respect to density (-0.38 and -0.238) [@cerveroEffectsBuiltEnvironments2010; @ewingStructuralEquationModels2014].

As far as methods, standardization, orthogonalization before fitting the model might be helpful.
