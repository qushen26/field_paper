--- 
title: "The Association Between Travel and Urban Form"
author: "Shen Qu"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook: 
    lib_dir: "book_assets"
    includes:
      after_body: disqus.html
  bookdown::pdf_book:
    toc: true
    toc_depth: 3
  citation_package: biblatex

documentclass: memoir # amsbook  # scrbook # book
bibliography: [fieldpaper1015.bib, unsorted.bib, book.bib, packages.bib] #
biblio-style: apalike
link-citations: yes
github-repo: qushen26/field_paper
description: "This is a field paper using the bookdown package. The output format for this example is bookdown::gitbook."
# abstract: |
always_allow_html: true
fontsize: 11pt
geometry: margin=1in
classoption: openany
keywords: travel behavior, urban form, regression analysis
header-includes:
   - \usepackage{amssymb}
   - \usepackage{amsmath}
   # - \usepackage{fancyhdr}
   # - \pagestyle{fancy}
   # - \fancyhead[CO,CE]{\thesection}
   # - \fancyfoot[CO,CE]{\thesection}
   # - \fancyfoot[LE,RO]{\thepage}
   # - \fancyhf{}
   # - \rhead{\thechapter}
   # - \chead{Field Paper}
   # - \lhead{\thesection}
   # - \rfoot{Page \thepage}
   # - \usepackage{setspace}\doublespacing
   - \usepackage{subfig}
   - \usepackage{multicol}
   - \usepackage{multirow}


---

# Preface {-}

This field paper is for discussing the relationship between travel and urban form. ^[This is a field paper using the **bookdown** package [@R-bookdown], **knitr** [@xie2015] and TinyTeX: <https://yihui.name/tinytex/>.]
The initial motivation is curious about how the travel distance is affected by urban densities.
Is this relationship existed universally or just context-dependent? 
Are these models in literature replicable or reproducible?
Part I reviews the related literature and tries to cover the main theories and research in this field.
Travel patterns or behaviors as the variable of interest, is associated with many factors including urban form variables.
To answer above questions, this field paper extends to different types of travel variables and more influencing factors.

The travel-urban form studies are many and perplexing.
Part II introduces the common statistical methods for the question raised in Part I.
Although emerging many new techniques in recent years, regression analysis is still the fundamental and explainable methodology.
The first stage is to understand the different methods adapted in previous studies.
After that, it could be possible to give suitable criticism or replicate the previous models.

Under the travel-urban form topic, one goal is to examine the validation of different models. A parallel study on known data may find the issues of mismatch or misreading, and to improve the relevant studies in the future.
Another potential goal is to get some generalized association between travel and urban form. Based on many studies on the same topic, meta-analysis is a useful tool that can help to disclose publication bias in this field.



<!-- - Several Ideas -->

<!-- The discussion will gradually narrow down to the association between driving distance and urban density.  -->
<!-- Instead of using very high dimension model or synthesized index,  -->
<!-- Population Density itself may have more potential to explain the driving distance than people ever thought about. -->

<!--  Many disaggregated travel-urban form models without psychological factors are underfitting, the corresponding estimates are biased. -->
<!-- When the goal of related studies is to provide evidence for public policy, aggregated analysis may be good enough. -->


<!--  The estimated coefficients are incomparable unless the regression models have the common key specification. Because $\boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y}$ is a function depending on $\mathbf{X}$. -->
<!--  Standardized coefficients or elasticities cannot fix this issues. -->

<!--  Due to the non-linear characteristics of many individual and urban-form factors, identifying the effective ranges of variables is useful for policy making. -->



```{r setup, include=FALSE}
# knitr::opts_chunk$set(
#   tidy=FALSE,     # display code as typed
#   size="small")   # slightly smaller font for code
# devtools::install_github('rich-iannone/DiagrammeR')
# remotes::update_packages(c("DiagrammeR", "tinytex", "rmarkdown", "knitr"))
# remotes::install_github('rstudio/rmarkdown')
library(pacman)
p_load(tidyverse,kableExtra ,mosaic,ggpubr,factoextra, DiagrammeR,magrittr) #
# if (!require("webshot")) {install.packages("webshot")}
# webshot::install_phantomjs()

knitr::opts_chunk$set(eval=T,echo=F, message=F, warning=F,cache=T,fig.align = 'center') #,fig.pos="!h",fig.dim = c(8, 6)
```


```{r eval=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
# bookdown::render_book("index.Rmd", bookdown::pdf_book(keep_tex = TRUE))
# bookdown::render_book("index.Rmd", bookdown::word_document2())
bookdown::render_book()
bookdown::publish_book(account="qushen")
```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```



<!--chapter:end:index.Rmd-->

---
title: "The Association Between Travel and Urban Form"
subtitle: "Part I First field: Theories and Framework"
author: "Shen Qu"
date: "`r Sys.Date()`"
# site: bookdown::bookdown_site
output: 
  # bookdown::gitbook: 
  #   lib_dir: "book_assets"
  #   includes:
  #     after_body: disqus.html
  bookdown::pdf_book:
    toc: true
    toc_depth: 4
  citation_package: biblatex

documentclass: article #ElegantPaper # memoir # custom # amsbook  # scrbook # book
bibliography: [fieldpaper1114.bib, unsorted.bib] #, book.bib, packages.bib
biblio-style: apalike
link-citations: yes
# github-repo: qushen26/field_paper
description: "This is a field paper using the bookdown package."
# abstract: |
always_allow_html: true
fontsize: 12pt
geometry: margin=1.25in
# classoption: openany # remove blank pages
keywords: travel behavior, urban form, regression analysis
header-includes:
   - \usepackage{amssymb}
   - \usepackage{amsmath}
   - \usepackage{fancyhdr}
   - \pagestyle{fancy}
   - \setlength{\headheight}{15pt}
   # - \fancyhead[CO]{\thesection}
   # - \fancyhead[CE]{\thesection}
   # - \fancyfoot[CO,CE]{\thesection}
   # - \fancyfoot[LE,RO]{\thepage}
   # - \fancyhf{}
   # - \rhead{\thechapter}
   # - \chead{Field Paper}
   # - \lhead{\thesection}
   # - \rfoot{Page \thepage}
   # - \usepackage{setspace}\doublespacing
   - \usepackage{subfig}
   - \usepackage{multicol}
   - \usepackage{multirow}
---

# Preface {.unnumbered}

This field paper reviews the literature on the relationship between travel and urban form. In the urban planning field, the concept of "compact city" or "smart growth" is widely supported by planners. Its core idea is that a compact urban form can relieve the issues of automobile dependency and sprawl, then contribute to urban well-being and other sustainability goals. But opponents think denser development means crowding and traffic congestion. In the context of the U.S., a policy-making proposal related to increasing urban density is not easy to pass. Academia conducts many studies to provide evidence for the debate on compact development. These studies examine the relationship between travel and urban form using statistical models. These models are used to identify how the urban density, mixed-use, and other factors impact travel behaviors and patterns. Many scholars have found these urban-form factors associated with travel variables as expected, but the effect sizes are small.

This paper aims to understand the major ideologies in previous studies, know the research dynamic in recent years, and prepare for future studies. Researchers keep trying new data sources and methods to improve the models. They also summarize the outcomes from many models of this kind by meta-analysis. Part I of this paper introduces some influential studies on this topic and tries to figure out their frameworks and opinions. This part also extends the view to the relevant theories and research in psychology and geography, which could give some insight into the feature of travel for better understanding the models' elements and settings. Part II explores the statistical methods used in previous studies. This part goes through the common modeling procedure and introduces several critical effects types in travel-urban form studies. Given the available data sources currently, improving the methods has considerable potential in getting more convincing results of the association between travel and urban form.

<!-- This field paper is for discussing the relationship between travel and urban form. The initial motivation is curiosity about how the travel distance is affected by urban densities. Is this relationship existing universally or just context-dependent? Are these models in literature replicable or reproducible? Part I reviews the related literature and tries to cover the main theories and research in this field. Travel patterns or behaviors as the variable of interest are associated with many factors, including urban form variables. To answer the above questions, this field paper extends to different types of travel variables and more influencing factors. -->

<!-- The travel-urban form studies are many and perplexing. Part II introduces the common statistical methods for the question raised in Part I. Although emerging many new techniques in recent years, regression analysis is still the fundamental and explainable methodology. The first stage is to understand the different methods adopted in previous studies. After that, it could be possible to give fair criticism or replicate the previous models. -->

<!-- Under the travel-urban form topic, one goal is to examine the validation of different models. A parallel study on known data may find the issues of mismatch or misreading and improve the relevant studies in the future. Another potential goal is to get some generalized association between travel and urban form. Based on many studies on the same topic, a meta-analysis is a useful tool that can help to disclose publication bias in this field. -->

<!-- - Several Ideas -->

<!-- The discussion will gradually narrow down to the association between driving distance and urban density.  -->

<!-- Instead of using a very high dimension model or synthesized index,  -->

<!-- Population Density itself may have more potential to explain the driving distance than people ever thought about. -->

<!--  Many disaggregated travel-urban form models without psychological factors are underfitting, the corresponding estimates are biased. -->

<!-- When the goal of related studies is to provide evidence for public policy, the aggregated analysis may be good enough. -->

<!--  The estimated coefficients are incomparable unless the regression models have the common key specification. Because $\boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y}$ is a function depending on $\mathbf{X}$. -->

<!--  Standardized coefficients or elasticities cannot fix these issues. -->

<!--  Due to the non-linear characteristics of many individual and urban-form factors, identifying the effective ranges of variables is useful for policymaking. -->

```{r setup, include=FALSE}
# knitr::opts_chunk$set(
#   tidy=FALSE,     # display code as typed
#   size="small")   # slightly smaller font for code
# devtools::install_github('rich-iannone/DiagrammeR')
# remotes::update_packages(c("DiagrammeR", "tinytex", "rmarkdown", "knitr"))
# remotes::install_github('rstudio/rmarkdown')
library(pacman)
p_load(tidyverse,kableExtra ,mosaic,ggpubr,factoextra, DiagrammeR,magrittr) #
# if (!require("webshot")) {install.packages("webshot")}
# webshot::install_phantomjs()
knitr::opts_knit$set(root.dir = "C:/E/PhD/courses/Field paper")
knitr::opts_chunk$set(eval=T,echo=F, message=F, warning=F,cache=T,fig.align = 'center') #,fig.pos="!h",fig.dim = c(8, 6)

```

```{r eval=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
bookdown::render_book("index1.Rmd", bookdown::pdf_book(keep_tex = TRUE))
# bookdown::render_book("index.Rmd", bookdown::word_document2())
# bookdown::render_book()
# bookdown::publish_book(account="qushen")
```

```{r include=FALSE,eval=F}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index1.Rmd-->

--- 
title: "The Association Between Travel and Urban Form"
subtitle: "Part II Second field: Statistical Methods"
author: "Shen Qu"
date: "`r Sys.Date()`"
# site: bookdown::bookdown_site
output: 
  # bookdown::gitbook: 
  #   lib_dir: "book_assets"
  #   includes:
  #     after_body: disqus.html
  bookdown::pdf_book:
    toc: true
    toc_depth: 4
  citation_package: biblatex

documentclass: article #ElegantPaper # memoir # custom # amsbook  # scrbook # book
bibliography: [fieldpaper1114.bib, unsorted.bib] #, book.bib, packages.bib
biblio-style: apalike
link-citations: yes
# github-repo: qushen26/field_paper
description: "This is a field paper using the bookdown package."
# abstract: |
always_allow_html: true
fontsize: 12pt
geometry: margin=1.25in
# classoption: openany # remove blank pages
keywords: travel behavior, urban form, regression analysis
header-includes:
   - \usepackage{amssymb}
   - \usepackage{amsmath}
   - \usepackage{fancyhdr}
   - \pagestyle{fancy}
   - \setlength{\headheight}{15pt}
   # - \fancyhead[CO]{\thesection}
   # - \fancyhead[CE]{\thesection}
   # - \fancyfoot[CO,CE]{\thesection}
   # - \fancyfoot[LE,RO]{\thepage}
   # - \fancyhf{}
   # - \rhead{\thechapter}
   # - \chead{Field Paper}
   # - \lhead{\thesection}
   # - \rfoot{Page \thepage}
   # - \usepackage{setspace}\doublespacing
   - \usepackage{subfig}
   - \usepackage{multicol}
   - \usepackage{multirow}


---



```{r setup, include=FALSE}
# knitr::opts_chunk$set(
#   tidy=FALSE,     # display code as typed
#   size="small")   # slightly smaller font for code
# devtools::install_github('rich-iannone/DiagrammeR')
# remotes::update_packages(c("DiagrammeR", "tinytex", "rmarkdown", "knitr"))
# remotes::install_github('rstudio/rmarkdown')
library(pacman)
p_load(tidyverse,kableExtra ,mosaic,ggpubr,factoextra, DiagrammeR,magrittr) #
# if (!require("webshot")) {install.packages("webshot")}
# webshot::install_phantomjs()
knitr::opts_knit$set(root.dir = "C:/E/PhD/courses/Field paper")
knitr::opts_chunk$set(eval=T,echo=F, message=F, warning=F,cache=T,fig.align = 'center') #,fig.pos="!h",fig.dim = c(8, 6)
```


```{r eval=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
bookdown::render_book("index2.Rmd", bookdown::pdf_book(keep_tex = TRUE))
# bookdown::render_book("index.Rmd", bookdown::word_document2())
# bookdown::render_book()
# bookdown::publish_book(account="qushen")
```


```{r include=FALSE,eval=F}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```



<!--chapter:end:index2.Rmd-->

# (PART\*) Part I Theories and Framework {-} 

# Introduction {#intro}

## Background

In the past decades, efforts have been made to reduce Automobile Dependency in both developed and developing counties. Many research have found that moderating the car use have positive social, economic and environmental impacts. The negative externalities of automobile include, but are not limited to, congestion, collision, unhealthy lifestyle, urban sprawl, social segmentation, pollution, and Greenhouse Gas (GHG) emissions.

In urban planning and transportation, achieving this goal requires two parts. First, researcher find a set of factors which could mostly explain and affect travel behavior. Second, planning and policy are made to intervene the adjustable factors then to reduce the car use significantly.
This paper focuses on the first part and aims at supporting the second part.



## Analytical Framework

@handyCriticalAssessmentLiterature2005 gives a complete assessment for travel-urban form study, which is the mainstream framework in this field.
Under this framework, regression analysis currently is still the dominate method for explaining the relationship between urban form and travel.
A large mount of studies use regression models to identify the influencing factors and evaluate the effect size. Previous research demonstrated that there is not a single factor determining travel behavior. When choosing continues response, like Vehicle Miles Traveled (VMT) ^[or Vehicle Kilometers Traveled (VKT)], the basic structure of the regression model is as below:

\begin{equation}
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
(\#eq:lm)
\end{equation}

where $\mathbf{Y}$ are the variable of VMT. $\mathbf{X}$ are all relevant covariates with corresponding coefficients $\boldsymbol{\beta}$. $\boldsymbol{\varepsilon}$ are a random error term with expected value of $0$ and variance $\sigma^2$.

When the response is categorical variable, such as binary data of driving versus not, polytomous data of mode choice, or count data of trip frequency, the proper model is:

\begin{equation}
E[\mathbf{Y}|\mathbf{X}]=\mu=g^{-1}(\mathbf{X}\boldsymbol{\beta})
(\#eq:glm)
\end{equation}

Where $E[\mathbf{Y}|\mathbf{X}]$ is the expected value of travel choice variable $\mathbf{Y}$ conditional on $\mathbf{X}$; $\mathbf{X}\boldsymbol{\beta}$ is a linear combination (Ditto.); $g(\cdot)$ is a link function.


## Content Organization

<!-- \@ref(struc) -->
Part I introduces the Travel-Urban form studies in recent years. 
As the essential parts of regression analysis, independent variables of [Urban Form](#form) and dependent variables of [Travel](#travel) are presented using two separate chapters. 

Chapter 2 of [Urban Form](#form) starts from a fundamental question: What is the relationship between urban form and travel? How strong the relationship is?
Then the significant influencing factors in literature are systematically introduced.

Aggregated and disaggregated data at vary [Spatial Scales](#scale) can sway the meaning of influencing factors. Units and spatial scales should be careful chosen to make sure the results matching the initial research questions.

Chapter 3, [the theories of travel behavior](#travel) can be divided into [Traveler Choice][Traveler Choice] and [Human Mobility][Human Mobility] by looking travel as object or subject.
These theories and practice can enrich the understanding of travel variable as response of model.

[Chapter 4][Model Structures] presents several common model structures in existing literature of this field. It demonstrate the different perspectives of the relationship between urban form and travel. Some new trends and methods are also included in this chapter.




<!--chapter:end:files/01-intro.Rmd-->

---
editor_options: 
  markdown: 
    wrap: none # sentence
---

# Urban Form as Predictors {#form}

The concepts of 'urban form,' 'built environment,' and 'land use' are often exchangeable in literature. Adopted from some common usages, here, 'urban form' refers to the comprehensive physical expression of land use at the macro scale.\
'Built environment' emphasizes the urban form attributes as a series of external factors with respect to travelers' internal characteristics and is often used in disaggregated studies at mesoscale. When using 'urban form' and 'built environment,' one looks at them as the given conditions of travel decision. While 'land use' can represent either current status descriptions or designated future use at mesoscale and micro scales.

## Influencing Direction

Before discussing the impact of land use on travel, the first question is whether land-use characteristics affect the outcome of travel behavior? Or the affecting direction is opposite? Technically, randomized control experiments can identify the causal relationship between them. But in real life, it is impossible to set up some experimental areas and randomly assign people to live in these areas.

Another way is to observe the dynamic of these factors to figure out the direction of influences. @mullerTransportationUrbanForm2004 reviews the evolution of the U.S. urban form and describe the four eras of intrametropolitan growth (Figure \ref{4age}) in U.S. history: the walking-horsecar era (1800s -- 1890s), the electric streetcar or transit era (1890s -- 1920s), the recreational automobile era (1930s -- 1950s), and the freeway era (1950s -- 2010s) [@rodrigueGeographyTransportSystems2016]. Each four-stage urban transportation development has its dominated spatial structure, which is hard represented by other socio-economic concepts. Each era has a distinctive travel mode, range of distance, and land-use patterns. People can see that the innovation of transportation technology is a determining constraint to other factors and a main driving force to launch the next era.

```{r 4age,eval=T,out.width='50%', fig.align='center',fig.cap="One Hour Commuting According to Different Urban Transportation Modes. Source: P.Hugill (1995), World Trade since 1431, p. 213."}
knitr::include_graphics("fig/one_hour_commmuting2.png")
```

New transportation tools shifted people's travel modes, extended travel distance, and reshaped the urban form. In causal inference, the new tools are called confounder or common cause, affecting both treatment and outcome. But what is the relationship between travel and urban form? A simple way is to observe the sequence of events to happen. In each transition period, the new tools and new modes began ahead of the new urban development. Even today, multiple travel modes exist in the old town, but suburban seldom has the old way like the streetcar. Thus, urban form is more like an outcome rather than treatment.

However, the urban form remains relatively more stable than an individual's travel behavior in each era. A family may be used to driving in Texas and turn to use the subway when they move to New York and vice versa. Given a period, VMT could be affected by density and other factors. It doesn't make sense that VMT will change the density in the short term. Therefore, urban form can be treated as independent variables in the context of the current stage of urban development. The relationship of urban form with respect to travel can hold for a period of time.

Over the long term, the relationships among travel, urban form, and other physical, socio-economic, demographic factors were interactive and iterative. @levinsonMetropolitanTransportLand2018 emphasize transportation is a necessary but not a sufficient factor for any development. The change of eras is a comprehensive outcome of socio-economic and technological development. Which factor caused which effect does not have a simple answer. A conservative view is that land use and travel behavior are determined simultaneously by the transportation costs [@pickrellTRANSPORTATIONLANDUSE1999].

Although randomized control experiments about travel and urban form are impossible, the regression model can still explore their associations once the simultaneous relationship holds.

## Influencing Factors

For the complexity of travel behavior, many objective or subjective factors could change people's travel decisions. They include but are not limited to physical, socio-demographic, individual, and policy determinants. It seems impossible to make an exhaustive list. Here briefly introduces several significant influencing factors.

A dichotomy of individual versus environmental factors is a common framework. All relevant factors involving personal or household characteristics can be categorized as internal factors. In comparison, the built environment and other environmental factors have external influences. Disaggregate analysis usually chooses this structure because the models can distinguish the different sources of variation from individual and environmental factors. For example, the VMT model is as follows.

$$\begin{aligned}
\mathbf{Y}=\mathbf{X}_\mathrm{I}\boldsymbol{\beta}_\mathrm{I}+\mathbf{X}_\mathrm{E}\boldsymbol{\beta}_\mathrm{E}+\boldsymbol{\varepsilon}
\end{aligned}$$

where $\mathbf{X}_\mathrm{I}$ are travelers' internal characteristics; $\mathbf{X}_\mathrm{E}$ are built environment and other environment covariates.

### Individual Factors

Previous research have identified many internal factors have strong impact on travel.\
Vehicle ownership is a good indicator for choosing auto mode and longer travel distance (van der Waard, Jorritsma, and Immers 2013). Employment status or entry into the labor market often increases driving while retirement may have more walking or cycling for fewer time constraints. (Goodwin and Van Dender, 2013; Grimal, Collet, and Madre, 2013; Headicar, 2013)

Some factors shows significant impact on travel but give opposite directions. Sometimes, it implies some nonlinear features. For example, income usually has a positive relationship with car ownership and driving distance. But some studies find less-wealthy groups have more cars and longer driving distance (Goetzke and Weinberger 2012). Sometimes, it depends on the location and social background. Dargay and Hanly (2007) find the number of children in household has a positive relationship in the U.K. While Ding et al. (2017) find a negative effect in the U.S.

@laroucheEffectMajorLife2020 make a scoping review of some major life events on travel behavior. They provide some explanation for the inconsistent results. Relocation provide windows of opportunity for travel behavior change, but the direction depend on people's attitude. Psychological factors such as travelers' habits and preferences are determinant. Similarly, the choice after school transitions depends on the new environmental factors. Marriage is not significant because couples may live together before marriage. In a similar way, parents may have more car use several years after childbirth for the purposes of childcare, school, recreation, etc.

Therefore, a well performed model should contain these influencing factors to increase models' fitness. Controlling these independent variables, which can not be intervened by policy, can help to identify how large the effect sizes of adjustable factors are. These studies also notes that the range, location, and understanding of internal variables are critical for a proper model.

### Environmental Factors

Environmental factors usually impact a large number of people. Three main categories are natural environment, socio-economic environment, and built environment. The natural terrain, temperature, and precipitation could change travelers' choice. These factors can only be examined across cities and regions. They are also hard to change and are not included in many studies.

Socio-economic environment such as fuel price and crime rates also encourage/discourage people choosing driving for economic or safety reasons. Some of them, such as car culture are hard to measure and control as other psychological factors. Some factors could be related to the broader topics such as quality of life, or public security. In these cases car usage is not the core concerns.

Infrastructure supplement is also a set of strong explanatory variables. It has been proven that road capacity and parking space are two primary factors for driving. 
@chatmanDeconstructingDevelopmentDensity2008 finds the effect of denser development on VMT is neutral after controlling the  road service and parking demand.
The problem is that reducing supply is painful for public and is subject to political pressure. Increasing and improving transit services are more attractive through providing a substitution of driving (Kuhnimhof, Zumkeller, and Chlond 2013).

Policy environment as treatment applied on a administrative region, such as restrictions on car use, can only be examined by comparing with the 'control groups'. Meanwhile, transportation policy is a context-dependent factor. The same policy in name may be implemented in very different ways across the country. Cross-sectional study is a challenge because existing the complex interaction effects between a policy and the characteristics of the 'experimental group'. Longitudinal study and Difference in Difference (DID) methods are more common in policy evaluations. There are also two types of policies. Travel Demand Management (TDM) heads directly toward change of travel behavior. For example, studies find parking management and low ticket fare by subsidies can attract more transit passengers (Grimal, Collet, and Madre 2013). While many urban development policies such as UBG, TOD, and rezoning have more comprehensive goals.

Built environment such as urban density and design is a primary focus of attention in urban studies because they are more changeable than natural environment. They are simple and measurable for implement. They are more acceptable for neutral meaning and can change people's behavior inadvertently.

policy or planning might be able to intervene current and future land use, further achieve the goal of travel behavior change. "Attributes of the built environment influence travel by making travel to opportunities more or less convenient and attractive" [@domencichURBANTRAVELDEMAND1975; @levinsonElementsAccessTransport2017; @litmanHowLandUse2017]. There are many complex unknown interaction effects between built-environment and other variables. Some research found that the habit discontinuity hypothesis, major life events may provide windows of opportunity during which individuals may reconsider their travel behaviors and be more sensitive to behavior change interventions [@verplankenContextChangeTravel2008]. The changes in built-environment attributes may capture these windows of opportunity. Further introduction is placed in the next section.

### Density

Density is the first built-environment factor added to the model. Early research of automobile trips and urban density can go back sixty years [@mitchellUrbanTraffic1954]. @levinsonEFFECTSDENSITYURBAN1963 suggest that the people who lived in high-density neighborhoods make fewer automobile trips. This argument stimulated an enormous volume of work. Although later studies construct more complex models, density factor stays in most of travel-urban form model even today.

The most influential aggregate studies start from Newman and Kenworthy. They published a series of studies to show a strong negative correlation between per capita fuel use and gross population density (GPD) [^form-1]. Their sample covers from thirty-two to fifty-eight global cities [@newmanCITIESAUTOMOBILEDEPENDENCE1989; @newmanEndAutomobileDependence2015] and produced very convincing results. Their research points out the relationship rather than estimating the effect size. In this way, the denser cities have less fuel consumption, which implies less automobile dependence. This is a succinct argument and is widely accepted by planners and policy makers.

[^form-1]: @newmanCITIESAUTOMOBILEDEPENDENCE1989;@newmanGasolineConsumptionCities1989;@kenworthyINTERNATIONALSOURCEBOOKAUTOMOBILE1999;@newmanUrbanDesignReduce2006;@newmanPeakCarUse2011;@newmanDensityMultiplierResponse2011;@newmanDensitySustainabilityMultiplier2014;@newmanEndAutomobileDependence2015;@kenworthyAutomobileDependenceEmerging2017

The criticisms include their ideological grounds, dataset, and model specification[@gordonGasolineConsumptionCities1989; @dujardinHometoworkCommutingUrban2012; @perumalContextualDensityUS2017]. A criticism is that, for aggregated data, the population variable on both sides of the equation artificially creates a hyperbolic function (Equation \@ref(eq:hyperbolic)). In many disaggregated studies, the effects of density are not significant and have a small magnitude [@zhaoRethinkingDeterminantsVehicle2021].

```{=tex}
\begin{equation}
\begin{split}
 & \text{VMT}_{average}  =\beta\cdot  \text{Density}  +\cdots\\ 
\implies & \frac{\text{VMT}_{total}}{\text{Population}}  = \beta\cdot  \frac{\text{Population}}{\text{Area}}  +\cdots\\
\implies & \text{VMT}_{total} = \beta\cdot  \frac{(\text{Population})^2}{\text{Area}}  +\cdots
\end{split}
(\#eq:hyperbolic)
\end{equation}
```

Another criticism argues that the global comparisons are not valid, such as comparing Hong Kong and Houston. @ewingTestingNewmanKenworthy2018 created a subset with only U.S. Metropolitan areas from @kenworthyPatternsAutomobileDependence1999 's original data set. They fit the same model but get a much lower $R^2$ (0.096) than Kenworthy and Laube's (0.72). A similar work by @fanisThreeStudiesThat2019 shows the low $R^2$ for U.S. cities (0.1838) and European cities (0.2804) when deconstructing Newman and Kenworthy's data by continent. Actually, any research question has its corresponding sampling design. Choosing a cutoff from the whole data often gets a different result. These criticisms are unfair for Kenworthy and Laube's work. But it is true that U.S. cities have higher VMT and lower density than other countries' cities. Recent evidence over a 20-year period from other cities and counties show that the status quo of the U.S. only represents a small window of the global trend[@kenworthyAutomobileDependenceEmerging2017].

Density as an explanatory variable have some advantages. Density is calculated by population size and area size from census data, which are widely available over the country. In contrast, some individual variables are not as measurable and accurate as density. Some studies found density might be an intermediate variable or proxy to other land use variables such as land use mix, street network, and transit services [@ewingTravelBuiltEnvironment2010; @handyCriticalAssessmentLiterature2005]. The divisions of the statistical units in the U.S. are from an uniform criteria at multiple scales. Thus, density values is more objective, and comparable comparing other measurements.

Density is an informative factor. Except for the mean and variance, The moments function for Urban density such as skewness, kurtosis, or rank, all can be the predictor candidates. Scolar also explore more delicate measurements of density such as Propotional Weighted Density to replace overall density. "population-weighted density is equal to conventional density plus the variance of density across the subareas used for its calculation divided by the conventional density" [@ottensmannPopulationWeightedDensity2018]. Some studies use a geographically weighted regression (GWR) [@GeographicallyWeightedRegression] procedure to identify significant employment density peaks.

Urban density can be represented by many approximate variables -- built-up density, residential density, employment density, destination density, or CBD density. Here this paper focus on population density -- how many people live in a square mile of land - and involves others if possible.

### D-variables

One trenchant criticism of Newman and Kenworthy's work is that the univariate or bivariate models may leave some critical factors out. In a recent debate [@fanisThreeStudiesThat2019], Newman clarified that "All our work shows that there are multiple causes of car dependence and multiple implications." Since travel behavior is a multi-dimensional issue, more socio-demographic and built-environment variables were added to the multivariate analysis. The work started from adding three 'Ds' variables, *density*, *diversity*, and *design* [@cerveroTravelDemand3Ds1997], extended to five 'Ds', adding *destination accessibility* and *distance to transit* [@ewingTravelBuiltEnvironment2001]. It even grows to seven with the addition of *demand management* and *demographics*.

The idea of D-variables is from some urban planning and transportation theories such as "smart growth" and "new urbanism." 
To address the urban sprawl, A "compact city" should be denser than a typical suburban development [@schimekHouseholdMotorVehicle1996;@zhangHouseholdTripGeneration2019]. 
Mixed land use could build a sense of community and allow more external trips being replaced by internal trips [@ewingTrafficGeneratedMixedUse2011;@tianTrafficGeneratedMixedUse2015].
The transit-oriented development (TOD) could reduce the distance to transit and encourage people drive less and choose active modes [@mcneilRevisitingTODsHow2020].
Each variable may explain a part of travel behavior in some way. 

Ds' framework is a significance-centered mixed factor set and has become one of the most influential ideas in travel-built environment literature in the past decades. Using household-level or person-level data, these studies tried to disclose more complex relationships among the candidate factors by adding more relevant variables.
This paper can not list all the studies of the vast travel-urban form literature. A highlight is the results from many studies are mixed. A factor may has significant impact on travel in some studies while not in others, or even has different influencing directions. For example, the effect of population density on bus trips is positive in the study by @brownUnderstandingTransitRidership2014, while it is negative in @alamFactorsAffectingTravel2018 's study.

A criticism from @handyEnoughAlreadyLet2018 is that the D-variables may not be independent. A meta-analysis found that spatial multicollinearity is widespread in this field [@gimRelationshipsLandUse2013]. Although previous studies usually check multicollinearity issues, these variables still correlate with each other in some ways and may have strong interaction effects.

### Synthesized Index

To address the multicollinearity and interactions issues, @cliftonGettingHereThere2017 suggest to convert the various environmental characteristics to built environment indices. Some research try to use 'compactness indices' to replace the single density measurement [@ewingRelationshipUrbanSprawl2014; @hamidiLongitudinalStudyChanges2014; @hamidiMeasuringSprawlIts2015; @ewingUrbanSprawlRisk2016]. The primary method is principal components analysis (PCA) or principal components regression (PCR), which synthesizes many variables to four dimensions: development density, land use mix, activity centering, and street connectivity. The advantage of this method is to increase the elasticity value significantly. A recent study shows that the elasticity of VMT with respect to a county compactness index is -0.78. The disadvantage of this method is that the internal mechanisms of the indices are still not clear. Another disadvantage is that the various indices are not comparable. For example, @zhangHouseholdTripGeneration2019 use urban living infrastructure (ULI) as the local accessibility variable (ULI) and find ULI and household density have significant effects on household trip generation. Their ULI is the count number of retail, services, and social activities. Meanwhile, the Urban Liveability Index (ULI) by @higgsUrbanLiveabilityIndex2019 are some indices supporting health and wellbeing. These indices include not only social infrastructure and transit service, but also walkability, public open space, housing, and employment. The two ULIs have different information and may not be comparable. Hence, the association between ULI and travel mode choice is a specific result unless an uniform measurement is widely applied on other studies and cities.

A smart application of synthesize method (common factor analysis) is to control the physiological effects in models. @hongHowBuiltenvironmentFactors2014 convert eight attitudinal questions in the 2006 Household Activity Survey to three factors: Ease, Convenience, and Pro-transit. They fit the model using two geographic scales: 1-km buffer and traffic analysis zone (TAZ). After contolling the attitudinal effects, the *nonresidential density* and *distance from CBD* have significant effects on VMT at the TAZ level.

## Meta-Aanalysis

This section introduces several influencing meta-analysis of travel-urban form studies. The relevant methods is in a separate chapter of Part II.

- @ewingTravelBuiltEnvironment2010

To get a general, comparable outcome, @ewingTravelBuiltEnvironment2010 collected more than 200 related studies and summarized the elasticity values using meta-analysis. They exclude the aggregated studies to avoid "ecological fallacies." The studies on specific groups such as aged people are also excluded. The selected studies must use multiple regression analysis with at least one response of VMT or travel modes, with at least one predictor from 5D variables. The studies using structural equation models are not included because these models will not give a single effects size of each 5D variables. The coefficients with respect to vary metrics of predictors are incomparable. Thus they convert all the estimates of coefficient to elasticities. Elasticity measures the percentage change in response with respect to a 1 percent increase in a predictor. Thus, it is a dimensionless parameter.

After screening, sixty-two studies were selected. This is a very small sample size because the research question involves five predictors (5D-variables) and three responses (VMT, walking, and transit use). Taking the VMT-density relationships for example, there are only nine selected studies in this meta-analysis. It is not large enough to get a sufficient inference. Looking at the distribution of elasticities, six of nine papers gave zero or insignificant elasticity. Three studies showed significant negative values (two are -0.04 and one is -0.12). @ewingTravelBuiltEnvironment2010 use the nine observation to calculate the weighted-average elasticities of VMT with respect to population density. The result of -0.04 is mainly determined by the three observations.

Moreover, among the nine VMT-density studies, eight use single city/metro data. Only one nationwide study using NPTS data [@schimekHouseholdMotorVehicle1996] finds logarithm of household VMT has a non-significant elasticity (-0.07).

Similarly, in this meta-analysis, the weighted-average elasticity of job density (sample size = 9) was zero. The largest elasticities of VMT are found be -0.20 with respect to job accessibility by auto (sample size = 5) and -0.22 with respect to distance to downtown (sample size = 3).

For the limitation of data quality, the standard error of elasticities are not included in this meta-analysis. When calculating the weighted-average values, @ewingTravelBuiltEnvironment2010 use the sample size of each study as the weight factor. For the same reason, the confidence intervals are also not available.

```{r,eval=T}
library(kableExtra) 
kbl(data.frame( #knitr::kable
  `Study`=c('@ewing2009measuring','@frankMultipleImpactsBuilt2005',
            '@greenwald2009sacsim',
            '@mariakockelmanTravelBehaviorFunction1997',
            '@kuzmyak2009estimating',
            '@kuzmyak2009estimates',
            '@zegrasBuiltEnvironmentMotor2010',
            '@zhouSelfSelectionHomeChoice2008',
            '@schimekHouseholdMotorVehicle1996'),
  `Sites` = c('Portland,OR','Seattle','Sacramento', 'Bay Area','Los Angeles', 'Phoenix','Santiago de Chile','Austin','U.S.'),
Elasticity= c(0.00,0.00,-0.07,0.00,-0.04, 0.00,-0.04,-0.12,-0.07),
note =c('','','Non-peer-reviewed;Non-significant','','Non-peer-reviewed','Non-peer-reviewed',
        '','not log transform,$R^2$=0.097','Non-significant')
)  , booktabs = TRUE, label = 'meta2010', digit=2, #, align = "llr"
  caption = 'The Studies of VMT vresus Density in @ewingTravelBuiltEnvironment2010'
) %>% kable_classic(full_width = F,font_size = 7)  #%>% kable_styling( ) 
   #%>%footnote(symbol = c("Non-peer-reviewed", "Non-significant", 'not log transform,$R^2$=0.097'),footnote_as_chunk = T) # bootstrap_options= "striped"
```

This meta-analysis kindled researchers' enthusiasm for this topic. After that, some studies try to cover multi-region data [@zhangHowBuiltEnvironment2012]. @ewingVaryingInfluencesBuilt2015 accumulated a travel and built environmental dataset from 23 metropolitan regions in US (81,056 households and 815,204 people). They find that all of the 11 D-variables have statistically significant effects on VMT.

- @stevensDoesCompactDevelopment2017

@stevensDoesCompactDevelopment2017 extends this analysis and tries to explain the different outcomes using a meta-regression method. He focuses on the studies with VMT as the response and uses similar screening criteria. Based on the results from 37 studies, he finds the elasticity of population density is small (-0.10) and suggest that compacting development has a tiny influence on driving.

By adding a dummy variable, whether a study control residential self-selection or not, into the meta-regression, @stevensDoesCompactDevelopment2017 shows that self-selection research design could impact the effect size significantly. For the studies with self-selection control, the estimated elasticity of population density becomes -0.22, which is much stronger than @ewingTravelBuiltEnvironment2010 's result (-0.04). An advantage of meta-regression is the two groups with/without self-selection control can share the common errors, that fully utilizes the information and can overcome the small sample size issue to a certain extent. The number of studies with self-selection control is four, while the totoal selected studies is 19. This results is more reliable than the average value in self-selection control group.

Another improvement is that Stevens' meta-regression uses weighted least squares (WLS) method. The weights are the precision (inverse of variance) of each observation. The same weights are applied on the weighted average and precision-effect estimate with standard error (PEESE) method in his 'Technical Appendix for details.' The estimated elasticities of population density with the three methods are -0.22, -0.13, and -0.20 respectively. Unfortunately, Stevens doesn't share any information of standard error of coefficients in the article and technical appendix. It is not easy to check his data and results.

Note that one observation, the study by @chatmanHowDensityMixed2003 may twist Stevens' result about density dramatically. In Chatman's Tobit model, the average VMT is $\bar y= 3.988$; the average household density is $\bar x= 1.902$ (housing units per square mile, residential block group (1,000s)); the coefficient of household density is $\beta=-0.082$. Then the elasticity should be

$$
\beta\cdot\frac{\bar x}{\bar y}=-0.082\cdot\frac{1.902}{3.988}=-0.0391
$$ 

This elasticity calculated by @ewingTravelBuiltEnvironment2010 is -0.58. And it is not selected in meta-analysis because the response is VMT on commercial trips. @stevensDoesCompactDevelopment2017 chooses this study and calculates a different elasticity -0.34. Whatever, -0.34 is the smallest elasticity and the second smallest one [@zahabiSpatiotemporalAnalysisCar2015] is -0.22. While the rest studies give the range of elasticities from 0 to -0.20. Hence, these observations is highly skewed.

@chatmanHowDensityMixed2003 's model is also the only one of four studies with self-selection control. If remove this case (-0.34), the estimated effect size will be much close to zero. @zahabiSpatiotemporalAnalysisCar2015 has the largest sample size (147574). Steven finds the PEESE method for bias correction will change the weighted average elasticity from -0.13 to -0.20. He hesitates to remove this 'outlier' because the outcome will become -0.09. At last, he choose to keep this observation and report the result without bias correction (-0.22). Changing the criteria after seeing the results is called post hoc analysis, or exploratory analysis.

Stevens' work triggers a round of discussion. In @ewingDoesCompactDevelopment2017 's reply, they don't doubt Stevens' results and criticize his conclusions. They agree with the values of elasticity but argue that Stevens' results (-0.22 for density) are not small actually. They emphasize the extensive benefit of compacting development. They don't think reporting bias is widely exist in built environment-travel studies. The difference results of meta-analysis is mainly duo to the studies selection, such as U.S. or international context. Keeping or removing the outlier is also make the difference.

Other scholars also contribute various insights. @manvilleTravelBuiltEnvironment2017 supports the idea that compact development are related to less car use and look it as a "fundamental belief in urban planning". @nelsonCompactDevelopmentReduces2017 agree that selective reporting bias does exist when some "interests or ideology dominate the discussion." @cliftonGettingHereThere2017 points out some weakness and potential sources of bias in current travel behavior studies. @heresFutureResearchLink2017 support more application of meta-analysis on relevant studies. And they remind the substantial difference among the studies with vary methods, data sources of country, and metrics (e.g. commuting and noncommuting trips). They suggest to narrow the scope of studies down to get more specific conclusions for policymakers. @knaapDrivingCompactGrowth2017 provide some suggestions for improving this approach from the perspective of sample size, model specification, and weighing. Among the discussion, a key issue is whether an universal effect of compact development with respect to driving distance exists, or the effect is totally context dependent. If the former is true, Stevens' work should not be criticized for the cross-country scope. If the later is true, it still should be based on evidence rather than belief or experience. The complexity of urban issues makes it being an unsolved problem.

@handyThoughtsMeaningMark2017 agrees with the improvement by meta-regression but thinks that meta-analysis is not a direction worth to further investigation. In a later paper, @handyEnoughAlreadyLet2018 argues that the 5Ds framework should be replaced by accessibility-centered studies. In @stevensResponseCommentariesDoes2017 's response to commentaries, he clarifies some research goals and important questions. He insists this meta-regression is currently "most accurate synthesis of the literature."

Stevens' study shows a uncommon direction of bias on elasticity of population density. An usual assumption of publication bias is small-studies tend to have greater standard errors and effect sizes. In contrast, among the 19 studies including density variable, the effect sizes in small-studies are closer to zero. An possible reason is the studies have high heterogeneity are answering different questions. In a highly heterogeneous field, researcher may not have too much pressure for small effect size. Another possible explanation is that most of recent studies include a bundle of predictors. Once one or more coefficients show significant, the paper will be treated equally. Publication bias only affects the nothing significant studies.

- @astonExploringBuiltEnvironment2021

After the two milestones for meta-analysis of built environment and travel behavior, a recent update re-examines the post-2010 empirical literature. @astonStudyDesignImpacts2020 collected 146 studies containing 467 models and being recorded as 1662 data points. There are 15 predictors of research design, including the number of variables, aggregate/disaggregate data, general/commuter group, trip purposes, time periods, types of model, are used to examine how research design affects the built environment-mode choice studies. Instead of using elasticities as the response variable, they choose correlation, another dimensionless variable to measure the strength of the relationship between built environment and transit use. Their results shows that whether accounting residential self-selection and regional accessibility can account for 40% of variation of mode choice in the meta-regression.

Actually, this meta-regression use Stepwise selection method to remove the insignificant predictors. 40% is the coefficient of determination $R^2$ in the four-predictor model for density and the five-predictor model for Diversity. In these meta-regression models, standard errors $SE_r$ show the largest coefficient value. Control for covariance (which lack of explanation in this paper) contributes the second large one. Control for regional accessibility is a insignificant variable in both Density and Diversity models. How can get the conclusion of the 40% of variation are duo to the control of self-selection and regional accessibility? @astonStudyDesignImpacts2020 mention the asymmetry existed in the funnel plot for density and accessibility. It is an evidence of publication bias that could lead to overestimate the correlation. But the plot is not shown in the paper.

In a later paper, @astonExploringBuiltEnvironment2021 further improve the meta-analysis to examine the impacts of 5D variables on transit use. The number of studies are extended to 187. And 418 of 505 elasticities are used as valid response. They find that, using a random-effect model, the elasiticity of density on transit use (0.10) is close to @ewingTravelBuiltEnvironment2010 's result (0.07). The standard error of estimate is also included $SE=0.013$. Using this estimites, they re-examine the effects of control for self-selection and regional accessibility. The paired tests show that both of the two indicators have significant effects on elasticities of density. They also find the estimated elasticity of density in the studies after 2010 is significantly higher than the studies before 2010. The authors explain this change by more diverse study locations and more studies which control for regional accessibility after 2010.

<!-- better understanding the association between travel behavior and built environment. -->

<!-- To estimate the association of VMT response and urban density predictor,  -->

<!-- how it is connected to other factors.  -->

<!-- and try to affect travel behavior by ing other  -->

<!-- Previous theories has point out several facets affecting travel behavior: economic, psychological, and physical. -->

<!-- land use, including the main study subjects in transportation and urban planning. -->

## Spatial Scales {#scale}

### Modifiable areal unit problem (MAUP)

Due to the various data sources or research interests, travel-urban form studies divide into two groups. One group uses aggregated travel and built-environment variables at the city, county, or metropolitan level. At the same time, the other group uses trip data at the individual or household level.
The results of travel models at different scales are often inconsistent. Using the same data source, @ewingTestingNewmanKenworthy2018 found that the elasticities of VMT with respect to population density is -0.164 in the aggregate models, which is a much higher value than disaggregate studies (-0.04 in the meta-analysis of @ewingTravelBuiltEnvironment2010). They suspect that this phenomenon is aggregation bias or ecological fallacy. They further explain that the two scales represent two different questions: The metropolitan-level density, which strongly affects the VMT, is not equivalent to the neighborhood density, which has much weaker effects on VMT.

Early in 1930, scholars noticed that, when a set of smaller areal units was aggregated into larger areal units, the variance structure will be changed and the estimated coefficients will be larger [@gehlkeCertainEffectsGrouping1934]. This inconsistency/sensitivity of analysis results is called modifiable areal unit problem (MAUP) or ecological fallacy [@openshawEcologicalFallaciesAnalysis1984].
In spatial analysis, two kinds of MAUP often happen simultaneously [@wongModifiableArealUnit2004]. The first one called 'scale effect' means that the correlation among variables depends on the size of areal units. Larger units usually lead to larger estimations. The second one, 'zone effect' describe the various results of correlation by choosing different areal shape or subset at the same scale.

@fotheringhamModifiableArealUnit1991 found that multivariate analysis is unreliable when using the data from areal units. Both value and direction of estimated coefficients may change for different spatial configurations [@leeModifiableArealUnit2016; @xuModifiableArealUnit2018].
The factors measured at a specific scale could only explain the variation generated at or above that level. Some factors such as density has cross scales. Their distributions in different units and scales are not identical. It is reasonable for them to have various meanings and influences on travel. A systematic comparison should be conducted among multi-scale studies. The inconsistent might not be about correct or wrong. As @ewingTestingNewmanKenworthy2018 commented, the aggregate and disaggregate studies are asking the apples and oranges questions.



### Aggregated Analysis

Aggregate data is more accessible and more convenient to combine with other data sources. 
Once a travel survey contains the attribute of [geographic identifiers](https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html)  defined by Census, this information of travel can be integrated to other demographic, employment, and built environment data (e.g. [American Community Survey (ACS)](https://www.census.gov/programs-surveys/decennial-census/about/census-acs.html)). Using the uniform coding, the data can further mapping to other levels (Table \@ref(tab:geoid)).
For example, @ewingTestingNewmanKenworthy2018 use the average per capita VMT of all urbanized areas across the U.S. from FHWA's Highway Statistics. Then they join the 2010 census data in 157 urbanized areas (with populations of two hundred thousand or more) to FHWA's VMT data.

```{r}
library(kableExtra)
kbl(data.frame( #knitr::kable
  `Area Type`=c("State","County","County Subdivision","Tract","Block Group","Block",
                "CSA","CMSA","CBSA","UACE","Places", "PUMA"),
   GEOID = c("41", "41051",	"4105192520",	"410510056",	"410510056002",	"410510056002014",
             "440", "6442", "38900", "71317",	"4159000", "4101314"),
`Geographic Area`= c("Oregon","Multnomah County, OR", "Portland West CCD, Multnomah County, OR", 
                     "Census Tract 56, Multnomah County, OR", "Block Group 2, Census Tract 56, Multnomah County, OR",
                     "Block 2014, Census Tract 56, Multnomah County, OR",
                     "Portland-Vancouver-Salem, OR-WA", "Portland-Salem, OR-WA", "Portland-Vancouver-Hillsboro, OR-WA", 
                     "Portland, OR-WA", "Portland city, OR", "Portland City (Northwest & Southwest)")
)  , booktabs = TRUE, label = 'geoid', align = "llr",
  caption = 'GEOID Structure for Geographic Areas' # \\label{tab:geoid}
) %>% kable_classic() %>% kable_styling(full_width = F,bootstrap_options = "striped", font_size = 7) %>% 
 pack_rows(index = c("Nested Entities" = 6, "Other Entities" = 6)) 
```

Some aggregate studies shows that, using the simple averages of individual data, the estimations of coefficients in linear model are unbiased [@praisGroupingObservationsRegression1954]. A condition is that the regression model must fix the *omission error* using proper specification [@amrheinSearchingElusiveAggregation1995; @yeImpactsModifiableAreal2021]. The check of unit consistency may help to examine the biases by MAUP on the estimations.

Tradition of aggregate analysis treat a city or metropolitan as an observation. Both dependent and independent variables are aggregated at macro level. The aggregate models confound the individual level's variance. Urban form factors usually show significant effect.
@vandecoeveringReevaluatingImpactUrban2006 carry on Newman and Kenworthy's work and consider four sets of potential explanatory variables: ten of urban form, six of transport service, five of housing and development history, and thirteen of socio-economic situations. They fit some linear regression models (all the variables keep the initial magnitude without taking logarithm or other transformation) and all of their adjusted $R^2$ are higher than 0.7. Their models also show that the cities with higher population density drive less. They found the land use characteristics of the inner area are more important than metropolitan-wide population density. 

In aggregate analysis the urban form factors measure the overall magnitude, such as population density, and can not reflect the land-use pattern or structure.
A recent city-level study [@gimAnalyzingCitylevelEffects2021] fits multiple regression models based on the data from 65 global cities. Using structural equation modeling, their results show that fuel price, household size, and congestion level have strong effects on travel time. In their model, the effect of overall population density becomes not significant while in the high-density built-up areas, the population density still has a larger effect on travel.


### Disaggregated Analysis

For disaggregate studies, collecting complete personal travel records and the built environment information is difficult. A common way is to get travel survey data from the local department of transportation and combine it with census data and GIS data. Some scholars start their relevant research from individual data, then make a bottom-up aggregation to traffic zones (TAZ) or higher levels. [@zhaoRethinkingDeterminantsVehicle2021].
In disaggregate analysis, the travel records by individual or household are the basic unit of dependent variables. Traveler's socio-demographic characteristics such as income, working status, and vehicle ownership also keep this resolution. However, built environment factors technically have a minimum geographic unit as the measure scope. Census tract and block group are the most common unit in disaggregate analysis.

Scholars who choose disaggregate analysis believe that the internal difference of urban characteristics be neglected at region level. They are interested in the impact of meso-level built-environment factors like the population and employment distribution of intra-urban [@buchananEffectUrbanGrowth2006; @sultanaJourneytoWorkPatternsAge2007].
Some study also confirm that individual-level data make the travel-land use model more reliable [@boarnetInfluenceLandUse2001].
Using disaggregate data can disclose the neighborhood-level differences and eliminate aggregation bias. Using logarithms of VMTs per vehicle from *National Personal Travel Survey (NPTS)* data with 114 urban areas, @bentoEffectsUrbanSpatial2005 fit the linear model with 19 variables. They found that, instead of population density, population centrality has a significant effect on VMT. The elasticity of annual VMT with respect to population centrality is 1.5. [^form-2]

[^form-2]: "population centrality measure is computed by averaging the difference between the cumulative population in annulus n (expressed as a percentage of total population) and the cumulative distance-weighted population in annulus n (expressed as a percentage of total distance-weighted population)."


Aggregate and disaggregate are relative concepts. Literature usually treat the data at household level as disaggregated but it is aggregated by person or trip. Form Census Block to Tract, County, and Metropolitan Area, the data at these levels are all called aggregated but they have substantial difference. 
@schwanenImpactMetropolitanStructure2004 explains that many urban form dimensions are tied to specific geographical scales. Recently, more studies import the spatial scales as an explanatory variable. In a report of travel and polycentic development, @ewingReducingVehicleMiles2020 identify 589 centers in 28 U.S. regions. Then a categorical variable, 'within/outside a center' is added into the model. The results show that the household living within a center have more walk trips and fewer VMT than who living outside a center.
@leeComparingImpactsLocal2020 also conduct a study involving factors at three level: household, census tract, and urbanized area. They find that density and centrality affect VMT at urban level as well as the meso-scale jobs-housing mix. After controlling for factors, the effect of local factors the urban-level spatial structure moderates the effects size of local built environment on travel.

<!-- The significant effects of 'population centrality' or ' high-density built-up areas'.  -->

<!-- The effects of density are not significant and have a small magnitude.  -->

<!-- The aggregated studies show that density has a strong effect on VMT at the city scale -->

<!-- The disaggregated studies support the 'collaborated' framework.  -->

<!--chapter:end:files/02-form.Rmd-->

# Travel as Response {#travel}

Two different perspectives, individual and collective, can explain travel behavior and car use. When people contextualizing travel as a personal choice or decision-making, the traveler as a subject make mode choices, driving or not. 
When travel behavior is understood as a social phenomenon, researcher observe and understand all the trip distance, time, and distributions as a whole. 

The two perspectives derived two schools of theory, Traveler choice and human mobility. In the school of Traveler choice, travel distance could be treat as an independent variable, a part of travel cost, or could be decided in the next step after mode choice , such as route choice. 
In the school of human mobility, driving distance grab more attentions.

## Travel Variables

Three dimensions can reflect the degree of car use, travel mode, driving frequency, and driving distance.
Previous studies commonly choose two metrics for measuring them, the share of auto trips (or other modes) and Vehicle Miles Traveled (VMT).

The share of mode is calculated by dividing the number of chosen mode over the total number of trips.
The main travel modes, transit, bicycle, and walking are the alternatives to driving personal car.
Given the same amount of travel demand, more active and transit modes means less car use.
VMT is used to measure the travel distance made by a private vehicle.
An integrated viewpoint is to treat the non-auto trips as zero-VMT.
In this way, the probability distribution of VMT can comprehensively represents the travel behavior. 


The smallest unit of VMT is recorded by trip from a daily travel survey. Then these records can be aggregated to personal or household daily VMT (DVMT).
A traveler's or household's DVMT can account for the degree of automobile dependency by combining the number of trips and driving distance during a day.
Given the survey day is randomly selected, DVMT can reflect the typical travel pattern in general.

Although, there are other approaches collect weekly, monthly, or longer VMT records by tracking car usage.
The odometer records are more likely to represent the usage of vehicle rather than traveler's behavior.
It is not easy to acquire long-term VMT through survey-based method. The annual mileage and fuel efficiency information provided in some public data usually are estimated values using daily records and are not as accurate as DVMT.

On a personal scale, VMT relates to the economic cost of travel by car, while another dimension, travel time measuring the time cost of vehicle travel.
For society as a whole, the total VMT measures the usage of road network. 
Thus, it acts as a major interest within the field of transportation, especially in the research of travel demand and infrastructure capacity.


And VMT highly correlated with the amount of fuel consumption, which is one of the main indicators of pollution and GHG emission. Since transportation is the second source of GHG emissions, it is also one of the priority issues involving sustainable development and climate change.

Previous research found that reducing VMT is instrumental in solving some urban problems and improving the qualities of urban life. The proportion of transportation cost in household expenditures is about 15 to 25 percent in the U.S.
It is natural that urban studies try to figure out the relationship between VMT and some urban built-environment factors. Then urban or regional policies could identify the best practices to contribute VMT reduction.


## Traveler Choice

Are 'decision' and 'choice' the same when discussing travel modes? Literally, a 'choice' is one decision given all available options at the same time. While 'decision' is a broader concept. A decision could be a schedule with a combination of many choices, such as modes, destination, and activities. A decision related to travel behavior could even include bicycle or car purchase, and relocation. This section will start from the theories of mode choice, then extend to a broader discussion of decision processes.

### Rational Choice Theory

For prescriptive, analytical everyday decision-making, rationality is a basic assumption in reasoned behavior or rational choice theories [@edwardsTheoryDecisionMaking1954; @vonneumannTheoryGamesEconomic1944].

This category is also called 'Normative Decision Theory', which assume people a traveler is an ideal decision maker who are full rational. It requires three necessary steps including information collection, utility evaluation, and choice making.

-   Expected Utility Theory (EUT)

Traditional economics focus on the utility evaluation and come up with the Expected Utility Theory (EUT) which is also called Consumer Choice Theory. The rule of EUT is Random Utility Maximization (RUM) [@ben-akivaDiscreteChoiceAnalysis1985; @mcfaddenConditionalLogitAnalysis1973]. This classical theory claims that customer always choose the one most appropriate by comparing the advantages and disadvantages of a range of alternatives, evaluating the benefits and costs of each possible outcome. Eventually travelers will select the optimal solution with the maximum 'utility' from the choice set.

In real life, Rational Choice Theory can not accurately describe the actual human behavior. Individuals do not often collect and analyse all the relevant information. They are not 'ideal' and are not able to calculate the utility for all possible alternatives with perfect accuracy. In many cases, the travel decision is not regarded as the 'best' one to achieve travelers' desired objective. Many other theories were developmed to fix these issues.

### Bounded Rational Behavior

Bounded rationality focused on the limitation of self-control [@marchCognitiveLimitsRationality2005]. In reality, individuals are behaving under many constraints including incomplete information, limited time, and cognitive capacity. The observed behaviors often are not optimal and are inconsistent with 'pure' rationality. Bounded rationality claims that, when people make decisions under constraints, heuristics and rules of thumb are more common than statistical inference. People are satisfied with a 'good enough' decision unless there is a definitively better alternative. The recently witnessed events would have stronger effects on an individual's decision than others [@camererAdvancesBehavioralEconomics2004].

### Theory of Planned Behavior

In psychology, many theories and models are developed to explain people's decision-making processes. [^theory-1]

[^theory-1]: CMDT=Cognitive moral development theory (Kohlberg, 1984), 

    ITB=Ipsative theory of behavior (Frey, 1988),
    
    NAM=Norm activation model (Schwartz, 1977,Schwartz and Howard, 1981),
    
    SDT=Self-determination theory(Deci & Ryan, 1985),
    
    TAM=Technology acceptance model(Davis, 1989),
    
    TDM=Travel demand management measures,
    
    TNC=Theory of normative conduct (Cialdini et al., 1990,Cialdini et al., 1991),
    
    TPB=Theory of planned behavior(Ajzen, 1985,Ajzen, 1991), 
    
    VBN=Value-belief-norm (Stern, 2000,Stern et al., 1999), 
    
    MGB=Model of goal-directed behavior(Perugini & Bagozzi, 2001)


@ajzenAttitudebehaviorRelationsTheoretical1977 proposed the Theory of Reasoned Action (TRA) to understand people's *behavioral intentions* and actual behaviors. 
They found two deciding psychological elements as *attitudes* and *subjective norms*. 
@ajzenTheoryPlannedBehavior1991 adds a new part of *Perceived Behavioral Control* (PBC) and renames TRA as Theory of Planned Behavior (TPB).



Attitudes are personal evaluation and it means how people prefer or are against performing an activity. For example, a commuter might choose transit in spite of the longer travel time because this person believes that transit is an environment-friendly transport mode.

Subjective norm is the social pressure from others. In the example above, choosing transit is because of other people's normative expectations rather than personal desirability.

PBC represents some nonvolitional factors such as time, budget, and resources. PBC is assessed by the individual's perception of ease or difficulty of the behavior. PBC is one reason of the different between intentions and actual behaviors, which is called attitude-behavior gap [@kollmussMindGapWhy2002; @laneAdoptionCleanerVehicles2007]. In this case, a commuter might choose transit because this person is confident in catching the bus every day.

Based on RUM models, @mcfaddenEconomicChoices2001 proposes a similar framework called the choice process including attitudes, perception, and preference. This framework is further developed to hybrid choice model (HCM) and non-RUM decision protocols [@ben-akivaHybridChoiceModels2002].

Two meta-analyses found that intentions to drive, perceived behavioral control, habits and past behavior play the primary roles in travel mode choice. Among these factors, PBC have the strongest effects on private car use. People don't want to reduce the car use because they think it is very inconvenient. The effect of attitudes is modest while subjective norms have weak effect on car use [@lanziniSheddingLightPsychological2017; @gardnerPsychologicalCorrelatesCar2008].



### Prospect Theory

@kahnemanProspectTheoryAnalysis1979a introduced the ProspectTheory to study the impacts of biases. Prospect Theory is a descriptive theory with three main components: First, people are more sensitive to the sure things (e.g., the probability between 0.9 and 1.0, or between 0.0 and 0.1 ), while being indifferent to the middle range (e.g., from 0.45 to 0.55). Second, people care more about the change of overall proportion than the absolute values regardless of gains or losses. Third, people make choice based on a reference point, rather than the overall situation or worth. Economist also extend the theory of expected utility maximization to Behavioral Economics by address the influence of psychology on human behavior.

-   Regret Theory

Regret Theory introduces the notions of risk or uncertainty in decisions [@loomesRegretTheoryAlternative1982]. Psychological studies found that individuals will not only try to maximize the utility but to minimize the anticipation of regret. The fear of regret could affect people's rational behavior. For example, A high risk of congestion in peak hours could encourage a commuter to choose transit mode. Likewise, a good reputation for punctuality can give traveler confidence in the rail system.

In addition to the traditional utility framework, a regret term is added to address the uncertainty resolution. The utility function on the best alternative outcome will be smaller after subtracting the regret term, which is an increasing, continuous and non-negative function.

-   Cognitive Bias

Another psychological factor, cognitive bias can result in judgement errors. For example, people treat potential gains and losses differently, that is called Loss Aversion. Loss Aversion suggests that the negative feeling about losses is greater than the positive response to gains [@tverskyAdvancesProspectTheory1992]. As a result, individual's decisions may not be consistent with evidence and tend to pay additional costs to avoid losses.

## Human Mobility


In Physics and Geography, travel distance and pattern are treated as an objective phenomenon.
There is a long history of human mobility studies. The related theories try to use some statistical expressions to fit the aggregated trip distributions.

Gravity Law is a dominant theory in this field. Scholars have developed some more delicate forms of Gravity Law and found some mathematical relationship to other famous distribution laws. Some theories from different perspectives, like intervening opportunities also show strong ability for explaining travel patterns and regularities.

### Distance Based Theories

-   Law of Migration

An early theory called *Law of Migration* by @ravensteinLawsMigration1885 tried to explain the regional migration patterns. This found is based on observation rather than quantitative analysis. But it capture the fact that the direction of migration is toward the regional center with great commerce and industry. It also pointed out that distance is a primary factor for migrant. This theory inspired many studies on population movement consequently. Even today, socio-economic factors and distance-constraints are the essential parts in the relevant models and frameworks.

-   Zipf's Law

Zip's law is also called *discrete Pareto distribution*. It is found in linguistics to explain the inverse relationship between the frequency and rank of a word. The charm is that this rank-frequency distribution disclosed a universal law in many realms of society and physics, such as urban size, corporation sizes, cells' transcriptomes and so on. Zipf interpreted the two competing factors as *force of diversification and unification*. The former produces larger amount of cases and the later tries to upgrade the rank. An equilibrium of the rank-frequency balance is controlled through a parameter $\alpha$ in the exponent. For example, a city's population size $m$ has a negative power relationship to its rank $r$ as below. [@visserZipfLawPower2013;@jiangZipfLawAll2015;@rozenfeldAreaPopulationCities2011;@gomez-lievanoStatisticsUrbanScaling2012;@hackmannEvolutionZipfLaw2020]

$$
m \sim 1 / r^{\alpha}
$$ 

@zipfP1P2Hypothesis1946 extended this expression to describe the traffic in both directions between two cities:

$$
t_{ij}\propto \frac{m_i m_j} {d_{ij}}
$$

where $t_{ij}$ represent the traffic flow of goods between two centers $i$ and $j$ with population sizes $m_i$ and $m_j$. $d_{ij}$ is the distance from $i$ to $j$. Because Zipf's formula has a same form with Newtonian mechanics [@newton18481687], people call this expression as Gravity Law.

-   Gravity Law

As the most influential theory, Gravity Law asserts that the amount of traffic flow between two centers is proportional to the product of their mass and inverse to their distance. The mass is often measured by population size.

\begin{equation}
p_{ij}\propto m_i m_j f(d_{ij}), \qquad i\ne j 
(\#eq:gravity-law)
\end{equation}

where $p_{ij}$ is the probability of commuting between origin $i$ and destination $j$, satisfying $\sum_{i,j=1}^n p_{ij}=1$. $m_i$ and $m_j$ are the population of two census units. The travel cost between the two places is represented as a distance decay function of $d_{ij}$ .

Exponential and power are the two forms of the distance decay function with a parameter $\lambda$ showed as below:

$$
f(d_{ij})=\exp(-\lambda d_{ij})
$$ and

$$
f(d_{ij})={d_{ij}}^{-\lambda}
$$ The function implies that the movements between the origin and destination decays with their distance. In transportation modeling, a common form of gravity model is :

$$
T_{ij}= \alpha_i O_i \cdot \beta_j D_j \cdot f(d_{ij})
$$

where $T_{ij}$ is the flow between $i$ and $j$. the two population are replaced by total tirp generation of origin $O_i$ and total trip attraction of destination $D_i$. $\alpha_i$ and $\beta_j$ are two constraining parameters to satisfy $\sum_{i}^{n_i}T_{ij} = D_j$ and $\sum_{j}^{n_j}T_{ij} = O_i$. It means that $\alpha_i = [\sum_{j}^{n_j} \beta_j D_j \cdot f(d_{ij})]^{-1}$ and $\beta_j = [\sum_{i}^{n_i} \alpha_i O_i \cdot f(d_{ij})]^{-1}$. Thus, this model is called as doubly constrained gravity model.

If it relieves the two constrains. this model will be simplified to single-constrained and unconstrained gravity model. By assuming $\alpha\beta$ is an adjustment parameter irrelevant to locations $i$ and $j$ for controlling the total flows, this model will not guarantee that the attraction of a destination equals the sum of flow from all origins, and the generation of a origin equals the sum of flow to all destinations.

-   Power Law

Broadly speaking, Zipf's law and Gravity Law have a common essence of power law, or scaling pattern. The Zipfian distribution is one of a family of power-law probability distributions. The power-law distribution also holds in many realms: urban size, population density, street blocks, building heights, etc.

The state-of-the-art studies of human mobility agree that travel behavior follows a power-law distribution at the population level [@barbosaHumanMobilityModels2018]. An example is @brockmannScalingLawsHuman2006 use dollar bills to track travel habits and confirm this theory. It reflects the fact that both trip and land use, as two geographic variables, follow some Paretian-like distribution. Apparently, it conflicts with Gaussian thinking, the foundation frame of linear models based on the location and scale parameters [@jiangZipfLawAll2011;@chenHierarchicalScalingSystems2018;@jiangGeospatialAnalysisRequires2018;@jiangSpatialHeterogeneityScale2018]

Meanwhile, the log-normal distribution may be asymptotically equivalent to a special case of Zipf's law, which could support the logarithm transform in current VMT-density models [@saichevTheoryZipfLaw2010].

### Opportunity Based Theories

-   Law of Intervening Opportunities

*Law of Intervening Opportunities* by @stoufferInterveningOpportunitiesTheory1940 developed the migration theory in a different direction. Stouffer proposed that "the number of people going a given distance is directly proportional to the number of opportunities at that distance and inversely proportional to the number of intervening opportunities."

Comparing with gravity law, the number of intervening opportunities $s_{ij}$ replaces the distance between origin and destination. For example, a resident living in location $i$ is attracted to location $j$ with $s_{ij}$ job opportunities in between.

$$
p_{ij}\propto m_i \frac{P(1|m_i,m_j,s_{ij})}{\sum_{k=1}^n P(1|m_i,m_j,s_{ij})}, \qquad i\ne j 
$$

where the conditional probability $P(1|m_i,m_j,s_{ij})$ can be expressed by Schneider (1959) as:

$$
P(1|m_i,m_j,s_{ij})=\exp[-\gamma s_{ij}] - \exp[-\gamma (m_j + s_{ij})]
$$

-   Radiation Law

@siminiUniversalModelMobility2012 propose a radiation model express the probability of the destination $j$ absorbing a person living in location $i$ as below:

$$
P(1|m_i,m_j,s_{ij})= \frac{m_i m_j}{(m_i + s_{ij})(m_i + m_j + s_{ij})}
$$

Or in transportation model it is expressed as:

$$
T_{ij}= O_i\cdot\frac{m_i m_j}{(m_i + s_{ij})(m_i + m_j + s_{ij})}
$$ To approximating the number of opportunities, $s_{ij}$ is from the population within a circle centered at origin. The radius is the distance between $i$ and $j$. Then $m_i + m_j + s_{ij}$ represents the total population within the circle, and $m_i + s_{ij}$ is the total population within the circle but excluding $j$, that is:

$$
T_{ij}= O_i\cdot\frac{m_i }{m_i + s_{ij}}\cdot\frac{m_j}{m_i + m_j + s_{ij}}
$$ The part of fraction converts to the product of two weights, the weights of origin and destination in the whole region. Although distance $d_{ij}$ doesn't appear in the expression of radiation model, it is still a determinant as in gravity model.

-   Distance Decay (hazard models)

Using the survival analysis framework, @yangLimitsPredictabilityCommuting2014 further extended this model by assuming a trip from origin to destination as a time-to-event process. Here time variable is replaced by the number of opportunities.

The survival function $S(t)=Pr(T>t)$ represents the cumulative probability of the event not happened within a certain amount of opportunities. Choosing Weibull distribution as the survival function, $S(t)=\exp[-\lambda t^\alpha]$ with scale parameter $\lambda \in (0, +\infty)$. By assuming $f(\lambda)=\exp[-\lambda]$ and integral on $\lambda$, the derivation is:

```{=tex}
\begin{equation}
(\#eq:sf1)
\begin{split}
P(T>t)=&E\{\exp[-\lambda t^\alpha]\} \\ 
=&\int_0^{+\infty}\exp[-\lambda t^\alpha]\exp[-\lambda]d\lambda\\
=&\frac{1}{1+t^{\alpha}}
\end{split}
\end{equation}
```
By replacing $t$ with $m_i+s_{ij}$, the conditional probability is:

$$
\begin{aligned}
P(1|m_i,m_j,s_{ij})= &\frac{P(T>m_i+s_{ij})-P(T> m_i+s_{ij}+m_j)}{P(T>m_i)} \\ 
=&\frac{[(m_i + s_{ij} + m_j)^{\alpha}-(m_i + s_{ij})^{\alpha}](m_i^{\alpha}+1)}{[(m_i + s_{ij} + m_j)^{\alpha}+1][(m_i + s_{ij})^{\alpha}+1]}\\
\end{aligned}
$$

where $\alpha$ is a parameter adjusting the effect of the number of job opportunities between origins and destinations.

A similar method can be found in @dingInfluencesBuiltEnvironment2017 's study. They use a multilevel hazard model to examine the effects of TAZ level and individual level factors with respect to commuting distance using the data of Washington metropolitan area.

Based on commuting data from six countries, @lenormandSystematicComparisonTrip2016 found gravity law performs better than the intervening opportunities law. The reasons could be the circle with radius $d_{ij}$ can not accurately represent the real influencing area, and the different between population and opportunities is not captured in this way.

### Time Geography

In contrast to overall trip distribution, the movements of individuals are always research interest in geography. @hagerstraandWhatPeopleRegional1970 proposed some concepts and tools in space and time to measure and understand the individual trajectories. This branch is called time geography. The famous "space-time aquarium/prism" is a 3D cube by adding temporal scales on the geographic space. It can capture the detailed structure and behavior of traveler.

A daily travel could include multiple trips and form a travel chain. The traveler may switch the sequence or adjust the routes to optimize the chain and minimize the travel costs. The daily total travel distance is the summation of every trip distances. The number of trips denotes as trip count. It exists but not so common that driving itself is the travel purpose, especially in daily life.

At individual level, time geography borrows some physical and mathematical concept and methods such as random walk, Brownian motion, and Levy flight

Along with the wide usage of Global Positioning System (GPS), high performance computer, and sophisticated algorithms, the high-resolution data being collected. The relevant studies also have a dramatic increase after 2005.

## Probability Distributions

In the transportation field, there are some valuable studies to identify the distributions of trip variables. Based on some theoretical or empirical studies [@hellersteinTheoreticalFoundationCount1993;@jangCountDataModels2005;@bienInflatedMultivariateInteger2011], scholars prove that trip generating frequency should not choose the linear regression models based on continuous functional forms. A zero-inflated negative binomial model is appropriate to solve the problems of over-dispersion and excess zero. This study implies that the diagnosis of variable distribution may be critical for regression modeling.


For continuous variables, it seems like choosing log-normal distribution for trip distance/time is a convention. @puAnalyticRelationshipsTravel2011 choose log-normal as prior assumption because a report called Future Strategic Highway Research Program (F-SHRP) [@cambridge2003providing;@kittelson2013evaluating] says "the log-normal distribution is the closest traditional statistical distribution that describes the distribution of travel times." Actually, the new version, SHRP2 says "formal tests (e.g., a Kolmogorov-Smirnov test) could be employed to evaluate the assumption and identify the sensitivity of the results to departures from this assumption." (p. 130)


Meanwhile, @linEstimationEnergyUse2012 validates the daily vehicle miles traveled (DVMT) follows a gamma distribution in the context of PHEV energy analysis. Based on the multidate (7-200 days) data sets from four countries, @plotzDistributionIndividualDaily2017 found Weibull distribution is an overall good two-parameter distribution for daily VKT; while the log-normal estimates are more conservative. The studies on trip distance are still not conclusive. But the attention of three distributions is similar to the survival analysis, which is also called time-to-event analysis [@kleinbaumSurvivalAnalysisSelfLearning2012]. This shows a potential relation with the Distance-Decay, or travel-time-budget theories [@marchettiAnthropologicalInvariantsTravel1994]. Similar to that, @kolblEnergyLawsHuman2003 show a canonical-like energy distribution for short trips by modes, which imply "a law of constant average energy consumption for the physical activity of daily travel". Some studies are not limited in parameter methods. @siminiUniversalModelMobility2012 propose a parameter-free model that predicts patterns of commuting.


<!-- ## Summary (Opt.) -->

<!-- The theories of travel behavior follow a positivism tradition for a long time. Economics and geography give some strong explanations for both macro and micro travel patterns. In order to remove the limitation of ideal rationality, more sociological and psychological theories and methods are introduced into this field. Gradually, people realized the normative concept is not sufficient for real world applications. More descriptive and narrative arguments appear in transportation and land use planning. An example is the shift from mobility to accessibility. -->

<!-- A primary trend in urban and transportation fields recently is the transition from techno-centric to socio-centric [@lanziniTechnocentrismSociocentrismEvolution2021] The socio-centric methods claim that accessibility is the key concept for evaluating urban sustainable mobility. This trend emphasizes the interpretations of travel behavior are context dependent and avoids generalizations. -->


<!-- Research in human mobility insist the positivism methodology and has some significant contributions because the individuals differences are confounded at the macro level. Under this framework, geographic distance always plays a prominent role in all human mobility theories. In adding to travel distance and Origin-Destination Matrices, Some primary metrics such as Mean Square Displacement and Radius of gyration are defined to quantitatively describe travel behaviors -->

<!-- A vital insight is that human behavior has two mobility roles: explorers and returners. It might be an inherent property of society, the instinct of exploring more territory and keeping together for division of labor. The explorers' behavior is consistent with the theory of utility maximization. People are always looking for more benefit. The concept of habit also match the behavior of 'preferential return', which means people are natural or nurtured likely to return to frequently visited locations or recently-visited locations. -->

<!-- Both gravity and opportunities theory choose population size as the source of travel demand. This is a rough assumption and is not enough to get more accurate predictions. One solution is to use empirical observed demand to calibrate the model case by case. Another way is to find more suitable variables such as residential, employment, or activity size to improve the model. -->

<!-- When area of interest is intra-urban, the O-D matrix records the trip connections among all paired locations. The matrix contain plenty of information including urban spatial structure, opportunities, activities and other socio-economic characteristics. The theories imply that O-D matrix have some strong connections to travel behavior in some ways. The first challenge is how to mine the information and extract some explainable elements. A limitation is that the empirical O-D matrix may only reflect the particular characteristics in that city and can not be applied to others. The second challenge is how to get a generalized interpretation, -->

<!-- Once choosing the individual perspective, current theories and methods are still insufficient. For example, the physical transportation network is only a part of travel decisions. Social networks with a 'hub-and-spoke structure' play a prominent role in finding a job. Using social media data, some studies provide valuable insight but still have a gap to form new theories. -->

<!-- An interdisciplinary perspective could provide a theoretical explanation for model selection. Existing mobility theories can play an anchor to identify the key variables' property and confirm the additive and linear relation among the factors. -->

<!--chapter:end:files/03-travel.Rmd-->

# Model Structures {#struc}

Based on related theories and studies, this section introduces several different analytical frameworks. @gotschiComprehensiveConceptualFramework2017  analyses the frameworks of 26 studies on active travel behavior. They propose a conceptual framework covering physical and social determinants, individual and multi-spatial levels.  The three explanatory frameworks introduced in this section can find supportive evidences and reflect the cognitive differences on travel-urban form studies. This section does not intend to figure out a 'best' framework. It demonstrates the structures from various perspectives would lead to distinct models and results. 


## Multistage

```{r ,eval=F,echo=F,out.width='100%', fig.align='center',fig.cap=""}
# manypoint<- plot(rnorm(30),rnorm(30))
png("~/urbanstudy/field_paper/fig/mstage.png")
library(DiagrammeR)
library(DiagrammeRsvg)
# digraph1 <-export_svg(
grViz("
digraph Mstage {
compound=true; # allow edges between clusters.
graph[fontname = 'helvetica-bold',  nodesep=0.4, overlap='prism1000']
rankdir = LR
node [shape = oval, style=filled, fillcolor = White, height = 0.75, width = 1, 
      fontname = 'helvetica',fontsize=12, fontcolor = black];

subgraph cluster_L{
label = 'Long-term';
style=dashed; color=black; 
node [fillcolor = WhiteSmoke];
R[label = 'Residential \\n location choice']; 
L2[label ='...'];
L3[label ='...']; # , style=invis, height = 0.4, width = 0, margin=0
# R -> NA[style=invis];
}    

subgraph cluster_M{
label = 'Medium-term';
style=dashed; color=black;
# rank = same;
node [fillcolor = PapayaWhip];
C[label = 'Car \\n ownership'];  
D[label = 'Distance to \\n destination'];
M2[label ='...'];
# C -> D[style=invis];
}   


subgraph cluster_S{
label = 'Short-term';
style=dashed; color=black;
node [fillcolor = '#fee08b'];
M[label = 'Mode \\n Choice'];
Ro[label = 'Route \\n Choice']; 
S3[label ='...'];
# M -> Ro[style=invis];
}  

# NA -> D # [ltail=cluster_L,lhead=cluster_M];
# D -> Ro # [ltail=cluster_M,lhead=cluster_S];
subgraph{
node [fillcolor =yellowgreen];
TB[label = 'Travel \\n Behavior'];
L2 -> D[ltail=cluster_L,lhead=cluster_M]
D -> Ro[ltail=cluster_M,lhead=cluster_S]
Ro -> TB[ltail=cluster_S]
}

  # c3d1 -> c3ad1[weight= 10]  c3d2 -> c3ad2   c3d3 -> c3ad3[weight= 10]    
  # {rank = same; c3ad1;c3ad2;c3ad3}    
	# c3d1 -> c3d2-> c3d3[style=dashed,splines=curved,constraint=true] #,dir=none
  #  c2d2 -> c2ad c2d1 -> c2ad[style=invis];
  # edge[style=invisible,dir=none];
  #   node [shape = square, style=filled,fontsize=9,fillcolor = Beige];
  #   b [label = 'The relationship \\n between \\n Auto Dependence \\n & \\n Urban Density']
  #   node [shape = oval, style=filled,fontsize=12, fillcolor = WhiteSmoke]; #'#91cf60'
  #   c [label = 'Evidence from \\n existing studies']
  # a1 ->b [dir=back]
  # b -> c [dir=back]

  }") #,)

 # height = 500,width = 800)
library(htmltools)
html_print(HTML(digraph1))
save_html(HTML(digraph1), "~/urbanstudy/field_paper/fig/mstage.png")
library(webshot)
webshot(html_print(HTML(digraph1)), "~/urbanstudy/field_paper/fig/mstage.png")

dev.off()
# export_graph(digraph1,
# file_name = "fig/mstage.png",
# file_type = "png")

# digraph1 = DiagrammeRsvg::export_svg(digraph1)
# digraph1 = charToRaw(digraph1) # flatten
# rsvg::rsvg_png(digraph1, "fig/mstage.png")
# png::writePNG(digraph1, "fig/mstage.png", dpi = 144)
```



```{r Mstage,eval=T,out.width='50%', fig.align='center',fig.cap="Multistage Structure"}
knitr::include_graphics("~/urbanstudy/field_paper/fig/mstage.png")
```

@ben-arivaMETHODOLOGYSHORTRANGETRAVEL1977 introduced a hierarchical framework of travel behavior. According to the length of time in travel decision, they divided the relevant factors into three levels. For example, people could change their travel mode choice for each day or each trip. Thus mode choice is a short-term decision
Car ownership belongs medium-term decision since people usually don't purchase or sell a car very often.
Residential location choice is long-term decision because relocation is the most infrequent event than others.

Under this framework, the decisions in longer term can affect the decisions in shorter term,  but not vice versa. (Figure \@ref(fig:Mstage))
For example, the distance to destination is decided by residential location choice and working location choice. And the distance is also a fundamental factor that influences travel mode choice behavior [@munshiBuiltEnvironmentMode2016].
In this way, both household car ownership, travel distance and travel attitudes are treated as intermediate variables connecting between built environment and mode choice in decision models. [@dingExploringInfluenceBuilt2017; @devosIndirectEffectBuilt2021].
A VMT models with stepwise framework is as follow.


\begin{equation}
(\#eq:stepwise)
\mathrm{Y}=\mathbf{X}_\mathrm{L}\boldsymbol{\beta}_\mathrm{L}+\mathrm{X_{M}}{\beta}_\mathrm{M}+\mathbf{X}_\mathrm{S}\boldsymbol{\beta}_\mathrm{S}+\boldsymbol{\varepsilon}
\end{equation}



where $\boldsymbol{\beta}$ are the coefficients with respect to long-term factors $\mathbf{X}_\mathrm{L}$, medium-term $\mathrm{X_{M}}$, and short-term covariates $\mathbf{X}_\mathrm{S}$.
There could be two-way interaction effect between long-term and medium-term variables; three-way interaction effects among long-term, medium-term and short-term variables in the model (Equation \@ref(eq:stepwise)). 

This framework works well for commuting trips because people will not change work place very often. The mobility theories also agree with this pattern.
"commuting trips are stable in time and account for the largest fraction of the total flows in a population." [@vanackerCommutingTripsTours2011].

However, the number of weekdays commute trips in the U.S. are less than one third of total trips in many years [source: @nhts_2009].
For non-work travel purposes, such as shopping, leisure, or socializing, the destination choices are more flexible.

The decision could be one-step. In consideration of all the benefit and cost, The traveler make a decision including the destination, mode and route at the same moment.
It also could be multistep. Starting from a travel demand or purpose, the traveler decides to make a trip then choose the destination, mode, route, and departure time step-by-step from available alternatives based on benefit, cost, and habit.
This process is progressive, iterative, and habitual in real life. Hence, the travel distance could be decided before or after mode or route choices. One structure only can capture one aspect of the process.
The framework selection should suit the research question. 




##  Decision Tree

```{r ,eval=F,echo=F,out.width='100%', fig.align='center'}
# manypoint<- plot(rnorm(30),rnorm(30))
# digraph2 <- export_svg(
grViz("
digraph Tree {
graph[fontname = 'helvetica-bold',  nodesep=0.4, overlap='prism1000']
rankdir = LR
node [shape = oval, style=filled, fillcolor = White, height = 0.75, width = 1, 
      fontname = 'helvetica',fontsize=12, fontcolor = black];

T1[label = 'Travel Choice'];	
  T20[label = 'No'];
node [fillcolor = WhiteSmoke];
  T21[label = 'Yes'];		 		    
 
node [fillcolor ='#fee08b'];
    T31[label = 'Driving'];		    
node [fillcolor = PapayaWhip];
    T32[label = 'Transit'];        
    T33[label = 'Biking'];  
    T34[label = 'Walking'];  
    
node [fillcolor = yellowgreen];
      T41[label = 'Distance'];			    
node [fillcolor = Beige]; #
      T42[label = 'Time'];		    

T1 -> T20   T1 -> T21 -> T31 -> {T41 T42}   T21 -> {T32 T33 T34}
  }") #,)
 # height = 500,width = 800)

```

```{r Tree,eval=T,out.width='50%', fig.align='center',fig.cap="Decision Tree Structure"}
knitr::include_graphics("~/urbanstudy/field_paper/fig/tree.png")
```

The single-step decision frameworks often require some strong assumptions. For example, the principle of utility maximization applied in either mode choice or VMT models is supposed to explain all the observations, including no-trip or no-driving cases.
Here these observation are treated as censored data with negative utilities. (That will leads to Tobit model for VMT.)

In contrast, a Decision Tree structure allows to use a hierarchical structure to fit different observation respectively (Figure \@ref(fig:Tree)). A similar figure can be found in @ewingTrafficGeneratedMixedUse2011 's Figure .1.
This structure is suitable for the studies including both mode choice and distance/time variables [@maModelingTrafficCrash2015; @ewingVaryingInfluencesBuilt2015].
The model will split into three equations \@ref(eq:Tree)
Starting from a travel demand or purpose, the traveler decides to make a trip or not at the first-level dichotomous node. 
A logit or probit model will fit all the data using a suitable model specification.

Then the second layer with polychotomous nodes is about mode choice, which is respect to the multinomial models.
At the bottom layer, a linear (or log-linear) model will only fit the data with positive driving distance (hurdle models; @maModelingTrafficCrash2015; @ewingVaryingInfluencesBuilt2015).
It is remarkable that the covariates set could vary in different layer's models. For example the lifecycle factor could strongly affect the travel frequency but not affect the driving distance significantly.
Therefore, this structure is more flexible and is consistent with real decision process.



\begin{equation}
\begin{split}
E[\mathbf{Y}_{\{yes,no\}}|\mathbf{X_0}]=&\boldsymbol{\mu_0}=g^{-1}(\mathbf{X_0}\boldsymbol{\beta})\\
E[\mathbf{Y}_{\{car,bus,...\}}|\mathbf{X_1},\mathbf{Y}_{\{yes\}}]=&\boldsymbol{\mu_1}=g^{-1}(\mathbf{X_1}\boldsymbol{\gamma})\\
\mathbf{Z}_{\{car\}}=&\mathbf{X}_\mathrm{2}\boldsymbol{\delta} + \boldsymbol{\varepsilon}
\end{split}
(\#eq:Tree)
\end{equation}



where $\mathbf{Y}_{\{yes,no\}}$ is a binary variable of making a trip or not. $\mathbf{Y}_{\{car,bus,...\}}$ is a categorical variable only including the cases of making a trip. $\mathbf{Z}_{\{car\}}$ is a continuous variable represent driving distance among the group of choosing driving. $\mathbf{X}_{\{0,1,2\}}$ means the three equations could have different model specifications and will estimate corresponding coefficients, $\boldsymbol{\beta}$, $\boldsymbol{\gamma}$, and $\boldsymbol{\delta}$.


<!-- -   The types of trip variables and the proposed probability distributions -->
<!--     -   Binary: Choose/not to make trip (Bernoulli/Binomial), -->
<!--     -   Polychotomous: modes (Multinomial) and trajectory (Network) -->
<!--     -   Counts: the number of trips (Poisson or NB), -->
<!--     -   Positive continuous: distance and time (Exponential Family: Normal, Log-normal, Power-law, Gamma, Weibull); -->
<!-- -   The unit: Individual or aggregated (household, areal); -->
<!-- -   The temporal scales: Real-time, hourly, Daily, weekly, monthly, yearly/annual, or decade; -->
<!-- -   The spatial scales: Community, city, region, country-wide/national -->
<!-- -   Other taxonomy/division may include -->
<!--     -   traveler roles (returner or explorer); -->
<!--     -   trip purposes; -->
<!--     -   long/short distance/time of trips. -->
<!-- The trip responses are defined in different units, scales, metrics. -->

## Multi-scales

```{r ,eval=F,echo=F,out.width='100%', fig.align='center',fig.cap=""}
# manypoint<- plot(rnorm(30),rnorm(30))
DiagrammeR::grViz("
digraph Mscale {
graph[fontname = 'helvetica-bold', style=filled, nodesep=0.4, overlap='prism1000']
rankdir = TD
node [shape = oval, style=filled, fillcolor = White, height = 0.75, width = 1, 
      fontname = 'helvetica',fontsize=12, fontcolor = black];

  subgraph cluster_U {
  label = 'Urban Scale \\n (urban-form factors)'; fillcolor= WhiteSmoke; 
     
    node [fillcolor = Beige];
    UT[label = 'Macro \\n Travel \\n Feature'];		 

    subgraph cluster_BE{
    label = 'Neighborhood Scale \\n (built-environmnet factors)'; fillcolor= PapayaWhip; 

      node [fillcolor = palegreen];
      NT[label = 'Meso \\n Travel \\n Pattern'];
		    
	    subgraph cluster_H{
	    label = 'Household Scale \\n (socio-demographic \\n characteristics)'; fillcolor= '#fee08b';
		    
        node [fillcolor = yellowgreen];
        HT[label = 'Micro \\n Travel \\n Behavior'];		    
		    }	
		 }	
	}    
  }")#,
 # height = 500,width = 800)
```

```{r Mscale,eval=T,out.width='50%', fig.align='center',fig.cap="Multi-scales Structure"}
knitr::include_graphics("~/urbanstudy/field_paper/fig/mscale.png")
```

As discussed in previous section, the external factors affect individual's travel behavior at multiple scales (Figure \@ref(fig:Mscale)).
This structure is adapted by many studies in recent years [@ewingTrafficGeneratedMixedUse2011, Fig.3].
Multi-scales studies distinguish the micro-, meso, and macro-scale variables [@dingInfluencesBuiltEnvironment2017;@leeComparingImpactsLocal2020].
According the results, @van2013recent doubt whether VMT is determined by local built environment factors.
The external factors always affect a group of individuals. Therefore, the meso-scale factors like built environment still can be examined. But the social and nature environment will impact all the people living inside the cities and regions. 
Only when the data sources cover many cities or regions, these factors can be involved in the models.(Equation \@ref(eq:multiscale))


\begin{equation}
(\#eq:multiscale)
\mathbf{Y}=\mathbf{X}_\mathrm{U}\boldsymbol{\beta}_\mathrm{U}+\mathbf{X}_\mathrm{N}{\beta}_\mathrm{N}+\mathbf{X}_\mathrm{H}{\beta}_\mathrm{H}+\cdots+\boldsymbol{\varepsilon}
\end{equation}


where $\mathbf{X}_\mathrm{U}$, $\mathbf{X}_\mathrm{N}$, and $\mathbf{X}_\mathrm{H}$ are the covariates at the scales of urban, neighborhood, and household. And $\boldsymbol{\beta}_\mathrm{U}$, $\boldsymbol{\beta}_\mathrm{N}$, and $\boldsymbol{\beta}_\mathrm{H}$ are corresponding coefficients respectively.


## Other Structures

- Mixed Model

Regression models usually assume the fixed effects of covariate on response. In many cases, some variables should be assigned with random effects.
In travel survey data, every observation are nested in some geographic units, such as tract, TAZ, or city [@dingInfluencesBuiltEnvironment2017].
The geographic location often have some nature of artificial feature influencing travel but the factor is unknown or unobserved.
When the data across many different regions, the model need to control the location effect to identify the crossed effect of built environment. For example, the hypothesis is whether density variable has a strong effects on travel in all place.

In spatial analysis, autocorrelation is a common issue which means the observation values in a location will be affected by its neighbors. Mixed model can help to eliminate the neighborhood effects.


\begin{equation}
(\#eq:multiscale)
\mathbf{Y}=\mathbf{X}_\mathrm{H}\boldsymbol{\beta}+\mathbf{X}_\mathrm{N}{\gamma}+\mathbf{X}_\mathrm{U}{\delta}+\boldsymbol{\varepsilon}
\end{equation}


where $\mathbf{X}_\mathrm{U}$, $\mathbf{X}_\mathrm{N}$, and $\mathbf{X}_\mathrm{H}$ are the same as above. $\boldsymbol{\beta}$ is a vector of fixed effects. $\boldsymbol{\gamma}$ and $\boldsymbol{\delta}$ are two vectors of random effects at neighborhood and urban scales. Assume that $\boldsymbol{\gamma}\sim N(\mathbf{0}, \boldsymbol{\Sigma}_\mathrm{N})$ and $\boldsymbol{\delta}\sim N(\mathbf{0}, \boldsymbol{\Sigma}_\mathrm{U})$. And also assume $Cov(\boldsymbol{\gamma},\boldsymbol{\delta})=Cov(\boldsymbol{\gamma},\boldsymbol{\varepsilon})=Cov(\boldsymbol{\delta},\boldsymbol{\varepsilon})=\mathbf{0}$.


- Non-linear models

As @cliftonGettingHereThere2017 said, built environment-travel studies often assume the linearity holds for the density measures and the travel outcome of interest. In practice, the relationship of trip vesus built environment variables may be non-linear or follow a step function with lower and upper threshold. The shape of the curve is highly informative. Recently, scholars have an increasing interest in the non-parameter methods [@dingNonlinearAssociationsZonal2021]. The overall effects of density might be small. But the curve might have a steep rise or sheer drop in some intervals. The inflection points, called effective thresholds, are more attractive and instructional. For example, Using Generalized Additive Model (GAM) [@hastieGeneralizedAdditiveModels1990], @ewingReducingVehicleMiles2020 's study finds some potential points of encouraging shorter driving. His study recommended the suitable activity density (population and employment size/square mile) should be between 10000-25000 according to a center type (Figure \ref{ewing-gam}).
More and more studies reveal the non-linear relationship between population density and VMT [@chengExaminingNonlinearBuilt2020; @dingApplyingGradientBoosting2018].
People can interpret the different trends as trigger effects, ceiling effects, U-shapes, hybrid, or synergy effect.


```{r ewing-gam,eval=T,out.width='50%', fig.align='center',fig.cap="Activity density v.s its smooth function ( Source: Ewing, R. 2021. Webinar: Transportation Benefits of Polycentric Urban Form)"}
knitr::include_graphics("fig/Ewing_gam1.png")
```


\begin{equation}
(\#eq:nonlinear)
\mathrm{Y}=\mathbf{X}_\mathrm{C}\boldsymbol{\beta}_\mathrm{C}+m(\mathbf{X}_\mathrm{E})\boldsymbol{\beta}_\mathrm{E}+\boldsymbol{\varepsilon}
\end{equation}


In the model (Equation \@ref(eq:nonlinear)), $m(\mathbf{X}_\mathrm{E})$ is a proper function of built environment covariates. 

The non-linear methods can better perform goodness-of-fit, but the generated new data are unique and harder to interpret. The non-linear methods can disclose more information from the data. The risk is that their results are more likely to reflect the features of the data itself and cannot be generalized to other cases.

The results of linear and non-linear models cannot be compared because their underlying assumptions of distribution are different. The threshold studies in urban transportation field remain in the early stages. 

# Summary I

- Questions

After reviewing the theories and analysis frameworks, we go back to the real problems. 
Try to imagine a scenario of publish hearing. A bill under consideration is about changing the rule of land use and development (e.g. Oregon legislators passed the first law (HB2001) in the United States legalizing duplexes on city lots in 2019 ^[https://www.oregon.gov/lcd/UP/Pages/Housing-Choices.aspx]). 
A scholar is asked to clarify the potential impact of urban form on travel behavior.
It is widely recognized that less driving means a more healthy, environment-friendly lifestyle.
Two common but distinct questions are: (1). The denser areas make people drive less? (2). People in denser areas drive less? 

Previous discussion tells us that answering Question (1) needs causal inference, which is hard for observational studies.
Some techniques (e.g. Difference in differences (DID)) try to identify the policy implications.
But the transformation of land use is gradual over several years. Under the new law, many factors are changing in the meantime (e.g. real estate market, parking space).
Some uncontrollable factors could also shift the outcomes (e.g. pandemic, autonomous vehicles).

The answer of Question (2) looks more conservative but is still powerful. If the people lived inside UGB or TOD areas really drive less than outside, the policies are successful either because original residents change their behavior or because new residents move in.
Therefore, this paper stays on the studies of association rather than causality. 
The major question is how people's travel behaviors change along with the urban-form factors.

- Factors

Many individual and environmental factors affect travel behaviors. Why people choose the framework of D-variables?
Because the direct intervention on many influencing factors is impossible (e.g. climate, age, income).
Or some interventions have tremendous economic and political costs (e.g. road capacity, fuel price).
D-variables provide a stable and applicable framework for examining the influences of the common planning elements on travel.
The results of study can convert to the guideline and recommended values.
The accessibility-centered proposal are more close to the nature of travel behavior. But this concept is hard to convert to some 'accessible' measurement.

Previous studies show the impact of D-variables are weak. Although this conclusion is controversial during in academia.
This result obviously is not compelling in public hearing.
Some synthesized index from D-variables show much stronger effects on travel. This method cannot tell us what should we do in practice.
But it implies the answer may still hide inside the covariates and is waiting to be able to reveal more meaningful information.

The studies at different scales also tell us that the meaning of one factor could change with the scales.
The neighborhood's density and the city's density are two different predictors.
the words of 'ecological fallacy' could make people think that the higher resolution and more detailed data would give more accurate estimates and are more close to the truth.
However, there might be many truth at the different spatial levels. 
A suitable study sign should select the suitable factors at households, neighborhoods, or city levels based on each specific research questions.


- Goals

Frequency of trip, traffic mode split, and travel distance are three major dependent variables.
The theory of Utility Maximization tell us a trip as an event must have some 'utility' or 'benefit.'
Nobody want to reduce the total number of trips because it reflects the social activity level and is a sign of urban prosperity.
Given a fixed number of trips, people don't object to a sustainable way. More shared trips with shorter distance are desirable.
Hence mode split and driving distance are the targets in this field.

TPB and prospect theory could better explain people's decision and choice.
This fact remind us researchers should not have some high expectations about the role of urban form.
Adding the attitude, habit, intention into the models does improve the goodness of fit.
Individual travel behavior and social travel pattern might be also different topics.
The studies of the former could be applied on micro design while the studies of the later are more important for policy making.

Both of distance-based theories and opportunity-based theories inspire us to rethink the relationship between travel and D-variables.
Density, mixed land use means more opportunities in the same area.
Design (intersection density or proportion of four-way intersections) and distance to transit represent the 'resistance' or cost.
Destination accessibility measures both the resistance and opportunities (e.g. distance to CBD or available jobs within a given travel time).
That might be the reason that some studies find destination accessibility has the strongest influence on travel.
Urban transportation is a connected system. Travel survey-based studies may have some systematic drawbacks on the destination side.

- Paths

Researchers could choose a suitable framework of analysis for their research questions.
Multistage framework can be used on longitudinal studies. Collecting the data of residents relocation, their car ownership, and travel behavior can figure out how these variables change with the built environment over many years.
This type of study may have limitation for necessity and sufficiency but a long-term evidence is often more impressive for public opinion.

A tree structure covering all decision nodes could reveal the whole travel pattern better. Trip decision, mode choice, and driving distance form the travel pattern. Merely looking at one node could be misleading (e.g. a person has many short driving trips. While another take more bus but makes long drive).

A hierarchical framework helps to identify one factor's effective scale on travel.
It matters because each policy has its affecting scope. UBD imposes the radius of urban development. TOD projects change the built environment around the stations. House Bill 2001 releases the restriction on only low-dense communities.

The threshold analysis could find the effective range of one factor. It helps to select the strategic focus areas and makes priority planning for limited public resources.

<!-- All factors related to spatial distributions have the morphological attributes. These attributes directly or indirectly affect travel behavior through some functional ways -->
<!-- such as road network connection -->
<!-- such as distance to transit services.  -->
<!-- This perspective helps people to figure out the influencing path and better understand the underlying mechanism. -->
<!-- another way of understanding the role of land use is morphology versus functionality.  -->



<!-- the demand or utility of travel is the primary node affecting travel choice.  -->

<!-- The available transportation modes is the secondary nodes which allow the traveler compares and chooses among driving and others. Choosing routes and departure time is in the third level. -->

<!-- The driving distance mainly is decided by the travel demand or utility. Only when the demand is not strong enough or having many alternative destinations, the lower levels' choices could change the driving distance. -->

<!-- If one focuses on auto trips. The tree can be simplified to two levels: driving or not. -->

<!-- From the travel behavior perspective, driving distance locates on a terminal node of a trip decision tree. -->

<!-- The tree shows that the first two level could produce some 'zero' driving distance. Should these cases be included in the studies of driving distance? -->

<!-- This discussion demonstrates a hierarchical relationship among driving distance and other trip variables. -->

<!-- In this way, the transit services located on the second internal nodes. -->

<!-- the travel destination may be determined before the decision making or be selected after the traveler comparing several plans. -->


<!--chapter:end:files/04-structure.Rmd-->

# (PART\*) Part II Statistical Methods {-} 

<!-- {.unnumbered}  -->


# Common Methods

In Part I, we discuss that travel as the response has different types of variable such as distance, frequency, and mode choice. When fitting a travel-urban form model, the first step is to choose a suitable model genre.
Chapter 5 introduce the main types of model with respect to the different travel variables.

The fitted models always give some outcomes. But the reliable outcomes require the models to be valid, adequate, and 'healthy'.
Chapter 6 lists several potential issues which often be neglected or omitted in travel-urban form models. 

Researcher never stop trying the new approaches to address the shortcomings in previous studies.
Chapter 7 presents some recent trends in travel-urban form studies that can give some inspiration for future work.

Scholars don't anticipate all studies have the same results, especially for the association between travel and urban form.
But they really want to see a more generalized result and then contribute the policy implication.
The last chapter Meta-Analysis introduces some basic ideas and approaches of meta-analysis and how to deal with publication bias. 


<!-- Part II presents some statistical methods relating to Travel-Urban Form models. -->
<!-- The common statistical tools applied in travel-urban form studies are some essential approaches had been developed for decades.  -->
<!-- Meanwhile, these principles and algorithms are the premise that further discussions continue. -->

<!-- Hence, the contents in this chapter are mostly from several textbooks [@casellaStatisticalInference2002; @montgomeryIntroductionLinearRegression2021;@ravishankerFirstCourseLinear2020] and lecture notes by Dr. Robert Fountain, Dr. Nadeeshani Jayasena, and Dr. Jong Sung Kim, rather than involving the front edge of techniques in recent statistics literature. -->


## For Travel Distance

### Transformations

For continuous response variables, Multiple Linear Regression (MLR) is the proper type of models and Ordinary least squares (OLS) is the corresponding algorithm. In travel-urban form studies, travel distance belong to this category. Other continuous variables, such as travel time, CO$_2$ emission are also usual research interests in transportation.

However, These variables have a common feature that their domain is positive number, not whole real number field. Here may raise a debate of whether zero values are a part of these variables or not.
As mentioned in previous chapter, the logarithm transformation on these variables can convert the positive values to real numbers, and the zero values are excluded automatically. 
Log transformation can also address the issues of linearity and normality.
Hence, Log transform on travel distance is widely used in transportation related research and practice. A resent example is that @alamFactorsAffectingTravel2018 use log-log model for travel demand by transit at MSA level in U.S.
Here gives two functions of log transform from mathematical perspective: variance stabilizing and linearizing.

- Variance Stabilizing

Equality of variance is a primary assumption of the regression model. 
When variance is not constant, the least-squares estimators will not give the minimized variance.
Though the estimation is still unbiased, the standard errors of regression coefficients will be larger and the model becomes insensitive.
@montgomeryIntroductionLinearRegression2021 give several useful variance stabilizing transformations in Table \@ref(tab:transformation).

```{r,eval=T}
library(kableExtra) 
kbl(data.frame(
  `Relationship`=c('$\\sigma^2\\propto E[y]$','$\\sigma^2\\propto (E[y])^2$','$\\sigma^2\\propto (E[y])^3$','$\\sigma^2\\propto (E[y])^4$'),
  `Transformation` = c('$y^{1/2}$','$\\ln(y)$','$y^{-1/2}$', '$y^{-1}$')
)  , booktabs = TRUE, label = 'transformation', digit=2, escape = F, #, align = "llr"
  caption = 'variance stabilizing transformations'
) %>% kable_classic(full_width = F,font_size = 7) 
```

A preliminary study using National Household Travel Survey (NHTS) [@nhts_2009] data finds both the mean and standard deviation of household daily VMT are close to 40.
This relationship supports that the logarithm of $\mathbf{y}$ is a proper choice for variance stabilizing.

- Linearizing

Another fundamental assumption, linearity is also can be addressed by transformation.
If the relationship between response and predictors is linearizable, a suitable transformation can construct a intrinsically linear model.
Several common forms from [@montgomeryIntroductionLinearRegression2021] are shown in Table \@ref(tab:linearlizing).

```{r,eval=T}
library(kableExtra) 
kbl(data.frame(
  `Relationship`=c('$y=\\beta_0\\exp[\\beta_1x]\\varepsilon$',
                   '$y=\\beta_0+\\beta_1\\ln(x)+\\varepsilon$',
                   '$y=\\beta_0x^{\\beta_1}\\varepsilon$',
                   '$y=x/((\\beta_0+\\varepsilon)x+\\beta_1)$'),
  `Transformation` = c('$y^*=\\ln(y),\\varepsilon^*=\\ln\\varepsilon$','$x^*=\\ln(x)$','$y^*=\\ln(y),x^*=\\ln(x),\\varepsilon^*=\\ln\\varepsilon$', '$y^*=1/y,x^*=1/x$'),
  `Linear Form` = c('$y^*=\\ln \\beta_0 +\\beta_1x +\\varepsilon^*$',
                    '$y=\\beta_0 +\\beta_1x^*+\\varepsilon$',
                    '$y^*=\\ln\\beta_0 +\\beta_1x^* +\\varepsilon^*$',
                    '$y^*=\\beta_0 +\\beta_1x^* +\\varepsilon$')
)  , booktabs = TRUE, label = 'linearlizing', digit=2, escape = F, #, align = "llr"
  caption = 'Linearizing transformations'
) %>% kable_classic(full_width = F,font_size = 7) 
```


Comparing these forms, the $log(y)$ transformation also called log-linear model gives a finite value of response $y$ when predictor $x\to  0$. 
While the log-log model ($y'=\ln(y),x'=\ln(x)$) will give an infinite value of $y$ when $x\to  0$. 
This gives a useful hint when one chooses from log-linear and log-log models.

Moreover, the $log(y)$ transformation changes the scale of error term.
Only one term in $\varepsilon$ and $\ln\varepsilon$ can be close to constant mean and normal distributed.
Therefore, residual diagnosis is still a effective way for choosing the proper form of transformation.
Prior theories and experience can also help to make a proper choice.
Recall the Equation \@ref(eq:gravity-law), both Gravity law and Zipf's law imply that a logarithm transformation on VMT is suitable. 

- A brief discussion

In literature, some regression models take logarithm transforms on all variables [@alamFactorsAffectingTravel2018;@leeComparingImpactsLocal2020], some only transform one or a part of variables [@perumalContextualDensityUS2017], while some models keep the original metrics of data [@zhouSelfSelectionHomeChoice2008].
Some studies choose Tobit model to deal with the domain and normality issues [@chatmanDeconstructingDevelopmentDensity2008].
Tobit model assume the travel distance follows a left-censored normal distribution. 
That mean no log transform is needed but a special distribution must be accounted.
Does unobserved negative distance or utility exists?
Tobit model claims a very strong assumption and requires both theoretical and empirical evidences.
Which approach is the proper one? 
Many of studies don't explain their choice and treat log transform as a 'tradition'.
This question should be answered by checking model adequacy, which is presented in next chapter. 

A correct choice of model type may depended on the data and research design.
Obviously, the models with and without log transform have different structures and are not equivalent.
The following question is, do their outcomes are comparable?
This question needs further investigation.
If the answer is no, the relevant meta-analysis or summaries should tread them separately.

### Estimations

- Coefficients

Estimating the effect size of built environment factors on travel is one of the major goals of travel-urban form studies. 
In regression analysis, the values of coefficients represent the effect size.
Least Squares is the mainstream method in the past decades. By *Gauss - Markov theorem*, OLS method itself doesn't require explanatory variables and response variable following normal distribution. If the residuals $\varepsilon$ satisfy $E(\varepsilon) = 0$ and $Var(\varepsilon) = \sigma^2$, the least-squares method will give the unbiased estimators with minimum variance.

<!-- ::: {#g-m .theorem name="Gauss - Markov theorem"}  \@ref(thm:g-m) -->
<!-- For the regression model \@ref(eq:lm) with the assumptions $E(\varepsilon) = 0$, $Var(\varepsilon) = \sigma^2$, and uncorrelated errors, the least-squares estimators are unbiased and have minimum variance when compared with all other unbiased estimators that are linear combinations of the $y_i$. [@montgomeryIntroductionLinearRegression2021] -->

<!-- Another version is that: Under Models II - VII, if $\lambda'\beta$ is estimable and $\hat\beta$ is any solution to the normal equations, then $\lambda'\hat\beta$ is a linear unbiased estimator of $\lambda'\beta$ and, under Model II, the variance of $\lambda'\hat\beta$ is uniformly less than that of any other linear unbiased estimator of $\lambda'\beta$ [@kimLectureNotes2020,IX, Theorem E13, p38]. -->
<!-- ::: -->

Ordinary Least Squares (OLS) method can be used to estimate the coefficients $\boldsymbol{\beta}$.
<!-- in equation \@ref(eq:lm)  -->
The dimension of $\mathbf{X}$ is $n\times p$, which means the data contain $n$ observations and $p-1$ predictors. The $p\times1$ vector of least-squares estimators is denoted as $\hat\beta$ and the solution to the normal equations is $\boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y}$
and $\hat\sigma^2=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta})'(\mathbf{y-X}\boldsymbol{\hat\beta})$

<!-- \begin{equation} -->
<!-- \boldsymbol{\hat\beta}=(\mathbf{X'X})^{-1}\mathbf{X'}\mathbf{y} -->
<!-- (\#eq:lsq-e) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \hat\sigma^2=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta})'(\mathbf{y-X}\boldsymbol{\hat\beta}) -->
<!-- (\#eq:lsq-v) -->
<!-- \end{equation} -->


Here requires $\mathbf{X'X}$ are invertible, that is, the covariates are linearly independent if $\mathbf{X}$ has rank $p$ [@kimLectureNotes2020, V., Definition, p.22].
When the observations are not independent or have unequal variances, the covariance matrix of error is not identity matrix. The assumption of regression model $V[\boldsymbol{\varepsilon}]=\sigma^2\mathbf{I}$ doesn't hold. 
Denote $\mathbf{V}$ is a known $n\times n$ positive definite matrix and $V[\boldsymbol{\varepsilon}]=\sigma^2\mathbf{V}$.
<!-- Then, there exists an $n\times n$ symmetric matrix $\mathbf{K}$ with rank $n$ and $\mathbf{V}=\mathbf{KK'}$. Let -->
<!-- \begin{equation} -->
<!-- \mathbf{z}=\mathbf{K'y},\ \mathbf{B}=\mathbf{K^{-1}X}, \text{and}\ \boldsymbol{\eta}=\mathbf{K'}\boldsymbol{\varepsilon} -->
<!-- \end{equation} -->
<!-- The linear model becomes $\mathbf{z}=\mathbf{B}\boldsymbol{\beta}+\boldsymbol{\eta}$ and $V[\boldsymbol{\eta}]=\sigma^2\mathbf{I}$. -->
<!-- If the model is full rank, that is $rank(\mathbf{X})=p$ then $\mathbf{X'V^{-1}X}$ is invertible. -->
The generalized least squares solution is $\boldsymbol{\hat\beta}_{GLS}=(\mathbf{X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y}$
 and $\hat\sigma^2_{GLS}=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})'\mathbf{V^{-1}}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})$
 
<!-- \begin{equation} -->
<!-- \boldsymbol{\hat\beta}_{GLS}=(\mathbf{X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y} -->
<!-- (\#eq:glsq-e) -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- \hat\sigma^2_{GLS}=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})'\mathbf{V^{-1}}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS}) -->
<!-- (\#eq:glsq-v) -->
<!-- \end{equation} -->


- Standardized coefficients

It is inevitable that the units of covariates $\mathbf{X}$ are very different in many studies. One thing can be done is to standardize the values of coefficients [@zhangHowBuiltEnvironment2012;@leeComparingImpactsLocal2020].
Unit normal scaling or unit length scaling can convert $\hat \beta_j$ to dimensionless regression coefficient, which is called standardized regression coefficients. A simple expression of standardized coefficients is that $\hat b_j= \hat\beta_j\sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}{\sum_{i=1}^{n}(y_{i}-\bar y)^2}}$,$j=1,2,...(p-1)$, and
$\hat\beta_0=\bar y - \sum_{j=1}^{p-1}\hat\beta_j\bar x_j$

<!-- Let -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- z_{ij}=&\frac{x_{ij}-\bar x_j}{\sqrt{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}}\\ -->
<!-- y^{0}_{i}=&\frac{y_{i}-\bar y}{\sqrt{\sum_{i=1}^{n}(y_{i}-\bar y)^2}} -->
<!-- \end{split} -->
<!-- (\#eq:standize) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{\hat b}=&(\mathbf{Z'Z})^{-1}\mathbf{Z'}\mathbf{y^{0}},\ \text{or}\\ -->
<!-- \hat b_j= &\hat\beta_j\sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar x_j)^2}{\sum_{i=1}^{n}(y_{i}-\bar y)^2}},\ j=1,2,...(p-1),\text{ and}\\ -->
<!-- \hat\beta_0=&\bar y - \sum_{j=1}^{p-1}\hat\beta_j\bar x_j -->
<!-- \end{split} -->
<!-- (\#eq:stand-coef) -->
<!-- \end{equation} -->

<!-- Note that $\mathbf{Z'Z}$ correlations matrix. -->

<!-- \begin{equation} -->
<!-- \mathbf{Z'Z}=\begin{bmatrix}  -->
<!-- 1 & r_{12} & r_{13} & \dots & r_{1k} \\   -->
<!-- r_{21} & 1 & r_{23} & \dots & r_{2k} \\   -->
<!-- r_{31} & _{32} & 1 & \dots & r_{3k} \\   -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots \\   -->
<!-- r_{k1} & r_{k2} & _{k3} & \dots & 1  \end{bmatrix},\quad  -->
<!-- \mathbf{Z'}\mathbf{y^{0}}=\begin{bmatrix}  -->
<!-- r_{1y} \\ r_{2y} \\ r_{3y} \\ \vdots \\ r_{ky} \end{bmatrix} -->
<!-- (\#eq:corr-matrix) -->
<!-- \end{equation} -->

<!-- where -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- r_{ij}=&\frac{\sum_{u=1}^{n}(x_{ui}-\bar x_i)(x_{uj}-\bar x_j)}{\sqrt{\sum_{u=1}^{n}(x_{ui}-\bar x_i)^2\sum_{u=1}^{n}(x_{uj}-\bar x_j)^2}}\\ -->
<!-- r_{jy}=&\frac{\sum_{u=1}^{n}(x_{uj}-\bar x_j)(y_{u}-\bar y)}{\sqrt{\sum_{u=1}^{n}(x_{uj}-\bar x_j)^2\sum_{u=1}^{n}(y_{u}-\bar y)^2}} -->
<!-- \end{split} -->
<!-- (\#eq:corr-1) -->
<!-- \end{equation} -->

<!-- Here $r_{ij}$ is the simple correlation between $x_i$ and $x_j$. $r_{jy}$ is the simple correlation between $x_j$ and $y$ -->

- Elasticity

As introduced in previous chapter, elasticity is also commonly used to determine the relative importance of a variable in terms of its influence on a dependent variable. It is generally interpreted as the percent change in the dependent variable induced by a 1% change in the independent variable [@mccarthyTransportationEconomicsTheory2001] Table \@ref(tab:elas-formula).
The values of elasticity are calculated by $e_i=\beta_i\frac{X_i}{Y_i}\approx\frac{\partial Y_i}{\partial X_i}\frac{X_i}{Y_i}$

```{r,eval=T}
library(kableExtra) 
kbl(data.frame(
  `Model`=c('Linear','Log-linear','Linear-log','Log-log','Logit','Poisson','NB'),
  `Marginal Effects` = c('$\\beta$','$\\beta Y_i$','$\\beta\\frac{1}{X_i}$', '$\\beta\\frac{Y_i}{X_i}$','$\\beta p_i(1-p_i)$', '$\\beta\\lambda_{i}$','$\\beta \\lambda_{i}$'),
Elasticity= c('$\\beta\\frac{X_i}{Y_i}$','$\\beta X_i$','$\\beta\\frac{1}{Y_i}$','$\\beta$','$\\beta X_i(1-p_i)$','$\\beta X_i$','$\\beta X_i$')
)  , booktabs = TRUE, label = 'elas-formula', digit=2, escape = F, #, align = "llr"
  caption = 'Elasticity Estimates for Various Functional Forms'
) %>% kable_classic(full_width = F,font_size = 7) 
```


It might be a typo that @ewingTravelBuiltEnvironment2010 use the formula of $\beta \bar X\left(1-\frac{\bar Y}{n}\right)$ for Logit model.
In Poisson model and Negative Binomial model, $\lambda_i=\exp[\mathbf{x}_i'\boldsymbol{\beta}]$ [@greeneEconometricAnalysis2018, eq.18-17,21].
For truncated Poisson model: $\delta_i=\frac{(1-P_{i,0}-\lambda_i P_{i,0})}{(1-P_{i,0})^2}\cdot\lambda_i\beta$ [@greeneEconometricAnalysis2018, eq.18-23].
Hurdle model will give separate marginal(partial) effects [@greeneEconometricAnalysis2018, example 18.20].

- A brief discussion

When a study contains two or more travel-urban form models, the models' responses are the same or similar. Researcher can assume that the observed VMT are random sampled from a large population. They often compare the models' performance by adding or removing one or a few independent variables. The coefficients from the best fitted model would be recommended. That means that the models in a study usually are comparable.
But the models in cross studies could choose different combinations of covariates $\mathbf{X}$ having substantial difference or uncertainties. 
In other words, the value of $\hat \beta_j$ means that, given all other coefficients fixed, for each change of one unit in $x_j$, the average change in the mean of $\mathbf{Y}$.
Since $\boldsymbol{\hat\beta}$ are linear combinations of the response and covariates [@montgomeryIntroductionLinearRegression2021],
these models should not take the consistent estimated coefficients for granted.
Comparing the coefficiants among different models need to check whether their covariates matrix are similar. The framework of D-variables does help to make cross-study analysis. 

Both standardized regression coefficients and elasticites try to make the effect sizes comparable in some way. 
For example, The population densities at tract level in Virginia and DC would have distinct ranges [@zhangHowBuiltEnvironment2012]. Standardized regression coefficients can eliminate the different ranges of data. 
Another example, the unit of population densities in studies could be people per square mile [@alamFactorsAffectingTravel2018] or people per square kilometer [@ingvardsonHowUrbanDensity2018]. Elasticites can eliminate the different units of data.
Another way is to unify the units before fitting the models but gathering the original data from different studies is a huge challenge.
Which measurement of effect size is better for comparison? A simulation test may answer this question.
Some studies sums up the standardized regression coefficients or elasticites of Multiple Linear Regression and called the summation as combined effects [@leeComparingImpactsLocal2020]. 
Although these values are dimensionless, but standardized regression coefficients and elasticites are derived from the value of $\boldsymbol{\hat\beta}$. Is the sum of partial regression coefficient meaningful? It needs some mathematical proof.


### Inference

Point estimation in last section tell us what the effect size is. Statistical inference tell us how it is likely to be true.
Most of travel-urban form studies are significance-centered. In a typical paper on this topic, if the p-value of one factor is small enough, the estimate of that factor would be accepted. 
But p-value is just a piece of inference. Analysis of variance (ANOVA), hypothesis test, and interval estimation provide more complete information.

<!-- Through testing hypotheses, the results -->
<!-- After estimating the effect size, statistical inference infers the properties of population.  -->
<!--  show how likely the estimates can represent the population. And interval estimation provide than point estimates. -->
<!-- This section would introduce some underlying principles.  -->

- Analysis of Variance

Analysis of Variance (ANOVA) is the fundamental approach in regression analysis. Actually, this method analyses the variation in means rather than variances themselves [@casellaStatisticalInference2002, Ch.11]. The basic idea is
<!-- \mathbf{y'y}=&\mathbf{y'Hy}+\mathbf{y'}(\mathbf{I}-\mathbf{H})\mathbf{y}\\ -->
<!-- \mathbf{y'y}=&\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}+\mathbf{y'y}-\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}\\ -->
<!-- \mathbf{y'y}-n\bar y^2=&\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}-n\bar y^2+\mathbf{y'y}-\boldsymbol{\hat\beta}\mathbf{X'}\mathbf{y}\\ -->

\begin{equation}
\begin{split}
\mathrm{SST} =& \mathrm{SSR} + \mathrm{SSE}\\
\sum(y-\bar y)^2=&\sum(\hat y-\bar y)^2+\sum(y-\hat y)^2
\end{split}
(\#eq:ss)
\end{equation}

where $\mathrm{SST}$ is Sum of Squares Total, $\mathrm{SSR}$ is Sum of Squares Regression, and $\mathrm{SSE}$ is Sum of Square Error. 
For Generalized Least Squares method, $\mathrm{SST}=\mathbf{y'V^{-1}y}$, $\mathrm{SSR}= \boldsymbol{\hat\beta'}\mathbf{B'z}=\mathbf{y'V^{-1}X(X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y}$, and $\mathrm{SSE}=\mathrm{SST}-\mathrm{SSR}$.
$\mathrm{SSR}$ represents the part of variance can be explained by the model.
$\mathrm{SSE}=\mathbf{e'e}$ is the unknown part and $\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=\mathbf{y}-\mathbf{X}\boldsymbol{\hat\beta}$.
This process is called variance decomposition.

<!-- Given the estimated coefficients, the model can get the fitted values of response as: -->
<!-- \begin{equation} -->
<!-- \mathbf{\hat y}=\mathbf{X}\boldsymbol{\hat\beta}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'y}= \mathbf{Hy} -->
<!-- (\#eq:fitted-y) -->
<!-- \end{equation} -->
<!-- where $\mathbf{H}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'}$ is hat matrix and $\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=\mathbf{y}-\mathbf{X}\boldsymbol{\hat\beta}=(\mathbf{I}-\mathbf{H})\mathbf{y}$ -->
<!-- Once the linear relationship holds, the response $\mathbf{y}$ can be decomposed to -->


- Hypothesis Test

**Significance of regression** means if the linear relationship between response and predictors is adequate. The hypotheses for testing model adequacy are

\begin{equation}
\begin{split}
H_0:&\quad \beta_0 = \beta_1 = \cdots =\beta_{p-1}=0\\
H_1:&\quad \text{at least one } \beta_j \neq 0,\ j=0,1,...,(p-1)\\
\end{split}
(\#eq:hyp-1)
\end{equation}

By Theorem D14 [@kimLectureNotes2020,XX, p.90], if an $n\times1$random vector $\mathbf{y}\sim N(\boldsymbol{\mu},\mathbf{I})$, then
$\mathbf{y'y} \sim \chi^2(n,\frac12\boldsymbol{\mu'\mu})$
Recall the assumption of $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$.
By the additive property of $\chi^2$ distribution, $\frac{MSE}{\sigma^2}=\frac{\mathbf{y'(I-H)y}}{(n-p)\sigma^2} \sim \chi^2_{(n-p)}$ and $\frac{MSR}{\sigma^2}=\frac{\mathbf{y'Hy}}{(p-1)\sigma^2} \sim \chi^2_{(p-1)}$.
Though $\sigma^2$ is usually unknown, by the relationship between $\chi^2$ and $F$ distributions,

\begin{equation}
F_0=\frac{MSE}{MSR} \sim F_{(p-1),(n-p),\lambda}
\end{equation} 

where $\lambda$ is the non-centrality parameter. It allows to test the hypotheses given a significance level $\alpha$. If test statistic $F_0>F_{\alpha,(p-1),(n-p)}$, then one can reject $H_0$.

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \frac{MSE}{\sigma^2}=&\frac{\mathbf{y'(I-H)y}}{(n-p)\sigma^2} \sim \chi^2_{(n-p)}\\ -->
<!-- \frac{MSR}{\sigma^2}=&\frac{\mathbf{y'Hy}}{(p-1)\sigma^2} \sim \chi^2_{(p-1)} -->
<!-- \end{split} -->
<!-- (\#eq:msemsr) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \mathbf{y'y} \sim \chi^2(n,\frac12\boldsymbol{\mu'\mu}) -->
<!-- (\#eq:chisq) -->
<!-- \end{equation} -->

**Significance of coefficients** is to test a specific coefficient, the hypothesis is H$_0$: $\beta_j =0$ and H$_1$: $\beta_j \neq 0$.
$\boldsymbol{\hat\beta}$ is a linear combination of $\mathbf{y}$. Based on the assumption of $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$, it can be proved that $\boldsymbol{\hat\beta}\sim N (\boldsymbol{\beta},\sigma^2(\mathbf{X'X})^{-1})$ and

\begin{equation}
t_0=\frac{\hat\beta_j}{se(\hat\beta_j)}=\frac{\hat\beta_j}{\sqrt{\hat\sigma^2C_{jj}}} \sim t_{(n-p)}
\end{equation} 

where $C_{jj}$ is the element at the $j$ row and $j$ column of $(\mathbf{X'X})^{-1}$. If $|t_0|< t_{\alpha/2,(n-p)}$, then the test failed to reject the $H_0$, this predictor can be removed from the model. This test is called partial or marginal test because the test statistic for $\beta_j$ depends on all the predictors in the model.

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- H_0:&\quad \beta_j =0\\ -->
<!-- H_1:&\quad \beta_j \neq 0\\ -->
<!-- \end{split} -->
<!-- (\#eq:hyp-2) -->
<!-- \end{equation} -->

- Confidence Intervals

Above results can also construct the confidence interval for each coefficient. A $100(1-\alpha)$ confidence interval for $\beta_j$ is $\hat\beta_j-t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}\le \beta_j \le \hat\beta_j+t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}$.

<!-- \begin{equation} -->
<!-- \hat\beta_j-t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}}\le \beta_j \le \hat\beta_j+t_{\alpha/2,(n-p)}\sqrt{\hat\sigma^2C_{jj}} -->
<!-- \end{equation} -->

- A brief discussion

This section shows that statistical inference relies on some probability distributions.
Hence, it requires more conditions than least squares methods. 
Checking model adequacy is a necessary step before reaching any conclusion.

ANOVA is an worthwhile method but is rarely seen in travel-urban form studies. 
@serbanicaSustainableCitiesCentral2017 use a two-way ANOVA to "compare the effects of country group, population growth and city size on green performance." 
@laneTAZlevelVariationWork2011 applies multivariate analysis of covariance (MANCOVA) on examining how "the variation in proportional changes in driving is related to variation in the covariates."
This study demonstrates that coefficients are not the only measurement of influencing factors.
ANOVA may explain how the effects appear or disappear in various spatial scales.
When the variance structure changing along the scales, observing the dynamic of $SSR$ of D-variables is an interesting topic.
@gelmanAnalysisVarianceWhy2005 shows that ANOVA is important for hierarchical models.


Statistician have proved that p-value itself is not a sufficient evidence for hypothesis test [@hubbardWhyValuesAre2008;@halseyFickleValueGenerates2015] and it should not be the only criteria for statistical inference [@wassersteinASAStatementPValues2016].
Confidence intervals (CI) is a better measurement which can exclude the values unlikely existing in population [@ranstamWhyPvalueCulture2012].
It can represent the uncertainty better than standard error because $se$ also depends on the sample size.
Most of travel-urban form studies provide $se$ values of estimates. A few of them give the confidence intervals. 
It calls for some empirical or simulated studies to show how CI can tell more about the effect size. 



## For Tirp Generation

The frequency of trip or ridership is a count variable. The observed counts of trips $Y_i,...,Y_n$ are random variable aggregated over differing numbers of individual or household with support $Y=\in\{0,1,2,...\}$.
The trips as events occur randomly in a day or other time.
An usual assumption is that count data follow a Poisson or negative binomial distribution. 

- Poisson Regression

The probability mass function (pmf) of Poisson distribution and its canonical form is 

\begin{equation}
Pr(Y=y) = \frac{e^{-\mu}\mu^y}{y!}=\exp[\log(\mu) y-\mu](y!)^{-1}
\end{equation}

So Poisson distribution has a simple link function as

\begin{equation}
\begin{split}
g(\mu_i)&=\log\mu_i=\eta_i=\mathbf{x}'\boldsymbol{\beta}\\
g^{-1}(\eta_i)&=\exp[\eta_i]=\mu_i=\exp[\mathbf{x}'\boldsymbol{\beta}]\\
\end{split}
(\#eq:pois-link)
\end{equation}

And Poisson distribution has the property of $E[y_i]=Var[y_i]=\mu_i$ as the systematic component.
By taking log transform, the non-negative parameter space mapping to real number.
It also convert the multiplicative relationship among predictors to additive.
The value of coefficient $\beta_j$ means that per unit change in predictor $x_j$ leads to the expected change in the log of the mean of response.
Another interpretation is that the mean of response would multiple $\exp[\beta_j]$ by per unit change in $x_j$.
Iteratively reweighed least squares method (IRLS) can solve the log-linear Poisson model.
The key correction step is $\hat{\eta_i}^{(1)}=\hat{\eta_i}^{(0)} + \frac{y_i-\hat\mu_i^{(0)}}{\hat\mu_i^{(0)}}$.
The diagonal weight matrix is $w_{ii}=\hat\mu_i^{(0)}$

- Negative Binomial Regression

A restriction of Poisson Distribution is that the mean and variance should be equal or proportional.
In many count data, the inequality of them is called overdispersion.
Overdispersion is very common in trip frequency data. It could be because the daily trips are not independent and homogeneous. They are a mixture of several different purposes or several persons in a household.
By adding a new parameter, mixture rate can construct a Poisson mixture model and address the overdispersion.
Suppose an unobserved random variable follow a gamma distribution $Z\sim Gamma(r,1/r)$ where $r$ is the shape parameter. The pdf is

\begin{equation}
f(z)=\frac{r^r}{\Gamma(r)}z^{r-1}\exp[-rz],\quad z>0
\end{equation}

It has $E[Z]=1$ and $Var[Z]=1/r$. 
Then a mixture model can be denote as a conditional distribution $Y|Z\sim Pois(\mu Z)$ for some $\mu>0$ and
$E[Y]=\mu$ and $Var[Y]=\mu+\frac{\mu^2}{r}$.
It is called Poisson-Gamma distribution who can represents the inequality of mean and variance.
If $r$ represent the given number of success and $y$ represent the observed number of failure in a sequence of independent Bernoulli trails. Then the success probability  is $p=r/(r+\mu)$
Recall that $\Gamma(r+y)=\int_0^\infty z^{r+y-1}\exp[-z]dz$, it can be proved that $Y$ follow a negative binomial distribution.

<!-- \begin{equation} -->
<!-- E[Y]=E[E[Y|Z]]=E[\mu Z]=\mu E[Z]=\mu -->
<!-- \end{equation} -->
<!-- and -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- Var[Y]&=E[Var[Y|Z]] + Var[E[Y|Z]]\\ -->
<!-- &=E[\mu Z]+Var[\mu Z]\\ -->
<!-- &=\mu E[Z] + \mu^2Var[Z]\\ -->
<!-- &=\mu+\frac{\mu^2}{r} -->
<!-- \end{split} -->
<!-- \end{equation} -->



<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- p(y)&=\int_0^\infty p(y|z)\cdot f(z)dz\\ -->
<!-- &=\int_0^\infty \frac{(\mu z)^y\exp[-\mu z]}{y!}\cdot\frac{r^r}{\Gamma(r)}z^{r-1}\exp[-rz]dz\\ -->
<!-- &=\frac{r^r\mu^y}{y!\Gamma(r)}(\mu+r)^{-y-r}\int_0^\infty [(\mu+r)z]^{y+r-1}\exp[-(\mu+r)z]d(\mu+r)z\\ -->
<!-- &=\frac{\Gamma(r+y-1)}{\Gamma(y+1)\Gamma(r)}(\frac{r}{\mu+r})^r(\frac{\mu}{\mu+r})^y\\ -->
<!-- &={{r+y-1}\choose{r-1}}p^r(1-p)^y, \quad y=0,1,2,... -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- However, negative binomial distribution is non-exponential families. Using maximum likelihood method and log link, the coefficients can be estimated. -->

Quasi-Poisson Model is another simple way for overdispersion. It introduces a dispersion parameter $\phi$. The Poisson model has $Var[Y|\eta]=\phi\mu$ where $\phi>1$. The estimated $\phi$ is
$\hat\phi=\frac{1}{n-p}\sum\frac{(Y_i-\hat\mu_i)^2}{\hat\mu_i}$
The extra parameter can  be estimated by maximum likelihood.

- Zero-inflated and Hurdle Models

In trip frequency data, there are often more no-trip observations than Poisson and negative binomial distribution expected. One problem in previous planning studies is to manipulate data by replacing zero values with one [@ewingVaryingInfluencesBuilt2015]. But the meaning and mechanism of 'No participate' are essentially different with that of intensive of participate [@greeneEconometricAnalysis2018, 18.4.8].

Zero-inflated model and hurdle model can address this issue.
Both of them assume the data arise from two mechanisms.
For example, @zhangHouseholdTripGeneration2019 use the Zero-inflated Negative Binomial model to examine the influences of built environment on trip generation.
In Zero-inflated Poisson/negative binomial model, both of two mechanisms generate zero observations.  
The first mechanism produce the excess zeros with $\pi_i=Pr(Y_i=0)$. The rest zero and positive values are generated by the second mechanism $f(y;\mathbf{x}_i,\boldsymbol\beta)$ of Poisson or negative binomial pmfs.

\begin{equation}
f(y_i;\pi_i,\mu_i)=\begin{cases}\pi_i+(1-\pi_i)f(0;\mu_i)&y_i=0\\
(1-\pi_i)f(y_i;\mu_i)&y_i>0\\
\end{cases}
(\#eq:zi)
\end{equation}

The two link functions are

\begin{equation}
\begin{split}
g_0(\pi_i)=&\mathbf{w}'_i\boldsymbol\gamma\\
g_1(\mu_i)=&\mathbf{x}'_i\boldsymbol\beta\\
\end{split}
\end{equation}


Note that the two mechanisms could have different covariates and coefficients. 
But both of $\pi_i$ and $\mu_i$ appear in two equations and have to be evaluated jointly. Newton-Raphson algorithm or EM algorithm can deal with this question.

Hurdle models is another type of two-step models.
Hurdle models assume that all zero observations are generated by the first mechanism. Hence the first mechanism is not depend on $\mathbf{x}_i$ and $\boldsymbol\beta$. 
A challenge is that ordinary Poisson or negative binomial distribution does contain zero values. Here use a truncated distribution to address this issue.

\begin{equation}
f(y_i;\pi_i,\mu_i)=\begin{cases}\pi_i&y_i=0\\
(1-\pi_i)\frac{f(y;\mu_i)}{1-f(0;\mu_i)}&y_i>0\\
\end{cases}
(\#eq:hurdle)
\end{equation}

where $f(0;\mu_i)= \exp[-\mu_i]$ in Poisson model and  $f(0;\mu_i)= (\frac{r}{\mu_i+r})^r$ in negative binomial model.

- A brief discussion

Because the logarithm on response is similar with log-linear model,  @choiAnalysisMetroRidership2012 take log transform on both side of equation and compare the performance between Poisson regression and log-log models (they call it as multiplicative model). They think "the Poisson model ... reflects the varying elasticity of the dependent variable according to the level of independent variables."
They purpose log-log model is better for greater F-statistic and adjusted $R^2$ than Poisson model. But the test statistic is not a good measurement for comparing models with different structures. $R^2$ is only a piece of evidence for goodness of fit.
@wangDetransformationBiasNonlinear2018 's study shows that the predictions of log-transform model may have significant bias when conducting detransformation.
The comparison between log-transform and Poisson models needs to go back to the properties of data and underlying mechanism.
The detransformation bias may due to the inappropriate model structure.
Although the two types of model have similar forms of equation but they perform two distinct types of randomization. A convincing comparison still call for the adequacy checking.

There are three conditions for Poisson process:
First, as a stochastic process, the probability of at least one event happened in a time interval is proportional to the length of the interval.
Second, the probability of two or more event happened in a small time interval is close to zero.
Third, in disjoint time intervals, the count numbers of trip should be independent.
In real life, a traveler can not make two trips at the same time so the second condition holds.
But a household with two worker and two student might have four trip at the same time every morning. Hence, individual count data is more valid than household's when using Poisson distribution. 
The independency of count number among differing time interval may not valid too. The daily trips often belong to a trip chain and require more information at a micro level.

Negative binomial regression has the same link function (Equation \@ref(eq:pois-link)) with Poisson models.
For the advantage of addressing overdispersion, there are more travel-urban form studies choosing negative binomial regression than Poisson models [@ewingVaryingInfluencesBuilt2015]. Some studies found the estimated coefficients are similar in two types of models [@chatmanDeconstructingDevelopmentDensity2008].
@dillPredictingTransitRidership2013 also report that count data models have no obvious advantages in prediction. 
A research about interval estimates may disclose their difference.

@ewingVaryingInfluencesBuilt2015 firstly apply the hurdle models on travel-urban form study. But their article is not for testing the advantage of hurdle model. 
The two-step models can better express the decision process discussed in Part I.
It is worth to compare the performance between hurdle, Tobit, and replacing-0-with-1 models in the future.



## For Mode Choice

Mode choice is the classical topic in travel studies.
One can choose to taking a trip or not, driving or active modes. These discrete response variables cannot be denoted by continuous variables.
Generalized Linear Models (GLM) allow the response following more general distributions than normal.
GLMs (equation \@ref(eq:glm)) include three components. 
Systematic component $\eta=\mathbf{X}\boldsymbol\beta$ has a similar form with ordinary linear models but without error term. $\boldsymbol\beta$ are unknown coefficients.
Random component $E[Y]=\mu$ specifies the probability distributions of $Y$, which could have a pdf or pmf from an exponential family.
Link function $g(\cdot)$ connects the systematic component and random component together.

- Binomial Response

When a traveler choose to make a trip or not, the decision follows a Bernoulli distribution. The probability is denoted by $Pr(\text{choice}=\text{Yes})=\pi$ and $Pr(\text{choice}=\text{No})=1-\pi$. 
For $n$ number of decisions under the same $\pi$, let $Y$ represents the count of choosing 'Yes' and follow a binomial distribution $Bin(n,\pi)$.
For many travelers with different $\pi$, one has $Y_i\sim Bin(n_i,\pi_i)$, that is a binary response data. The number of total observation $N=\sum_{i=1}^n n_i$. The pmf of binomial distribution is 

\begin{equation}
Pr(Y_i = y_i) =  {{n_i}\choose{y_i}}  \pi_i^{y_i} (1-\pi_i)^{n_i-y_i}
\end{equation}


It is clear that the random component is $E[y_i]=\pi_i$ and systematic component $\eta_i=\mathbf{X}'_i\boldsymbol\beta$. 
$\pi$ is the probability between zero and one. but the log odds of success $\eta_i$ can take any real number. The canonical form of binomial distribution is

\begin{equation}
Pr(Y_i = y_i) = \exp\left[\log(\frac{\pi_i}{1-\pi_i})y_i+n_i\log(1-\pi_i)\right]{{n_i}\choose{y_i}}
\end{equation}


The canonical link function in logit models can transform the probability to the range of real number. In this one-to-one mapping, a probability $\pi_i>1/2$ will give a positive $\eta_i$ and a negative $\eta_i$ correspond to a $\pi_i$ less than one half.

\begin{equation}
\begin{split}
g(\pi_i)&=\log\frac{\pi_i}{1-\pi_i}=\eta_i\quad\text{Logit function}\\
g^{-1}(\eta_i)&=\frac{\exp[\eta_i]}{1+\exp[\eta_i]}=\pi_i\quad\text{Logistic function}\\
\end{split}
(\#eq:logit-link)
\end{equation}

<!-- ### Probit Models (Opt.) -->

- Multinomial Response
 
For categorical response such as travel mode choice, a traveler has more than two alternatives including driving, transit, biking and walking. The generalized logistic regression can address these polychotomous data.
The mode choice $Y_i$ follows the mutinomial distribution with $J$ alternatives. 
Denote the probability of $i$th traveler chooses the $j$th mode, then $\pi_{ij}=Pr(Y_i=j)$.
And the pmf of multinomial distribution is 

\begin{equation}
Pr(Y_{i1}=y_{i1}, ..., Y_{iJ}=y_{iJ})= {n_i \choose y_{i1},..., y_{iJ} }
\pi_{i1}^{y_{i1}} \cdots \pi_{iJ}^{y_{iJ}}
\end{equation}


When the data exclude the people without trip, the several modes exhaust all observations and mutually exclusive. That is $\sum_{j=1}^J\pi_{ij}=1$ for each $i$. Once $J-1$ parameters are evaluated, the rest one will be determined automatically. That means $\pi_{iJ}=1-\pi_{i1}-\cdots-\pi_{i,J-1}$. 
The random component is $\mu_i=n_i\pi_{ij}$ and the systematic component is $\eta_{ij}=\mathbf{X}_i'\boldsymbol\beta_j$

\begin{equation}
\begin{split}
g^{-1}(\eta_{ij})&=\frac{\exp[\eta_{ij}]}{\sum_{k=1}^J\exp[\eta_{ik}]}=\pi_{ij}\\
g(\pi_{ij})&=\log\frac{\pi_{ij}}{\pi_{iJ}}=\eta_{ij}\\
\end{split}
(\#eq:mlogit-link)
\end{equation}

@mcfaddenConditionalLogitAnalysis1973 proposed the Discrete Choice Models which is also called multinomial/conditional logit model.
This model introduces $U_{ij}$ as the random utility of $j$th choice. Then based on Utility Maximum theory, 

\begin{equation}
\pi_{ij}=Pr(Y_i=j)=Pr(\max(U_{i1},...,U_{iJ})=U_{ij})
\end{equation}

Here $U_{ij}=\eta_{ij}+\varepsilon_{ij}$ where the error term follows a standard Type I extreme value distributions. The reason is that the difference between two independent extreme value distributions has a logistic distribution. Hence, it can still be solved by logit models.
The expected utility depend on the characteristics of the alternatives rather than that of individuals.
Let $\mathbf{Z}_j$ represents the characteristics of $j$th alternative, one has $\eta_{ij}=\mathbf{Z}_i'\boldsymbol\gamma$.
Combining the two sources of utility together, a general form of utility is $\eta_{ij}=\mathbf{X}_i'\boldsymbol\beta_j+\mathbf{Z}_i'\boldsymbol\gamma$


- A brief discussion

Multinomial logistic models are widely used in mode choice questions.
An alternative is the multinomial probit model witch assumes the error terms $\boldsymbol\varepsilon\sim MVN(\mathbf{0},\Sigma)$ where $\Sigma$ is a correlation matrix. The related application can be found in @chakourExaminingInfluenceStop2016.

Logistic models are not robust when the probability of $\pi$ is close to zero or one. 
For mode choice questions, the proportions of walking, biking, and transit are much smaller than that of driving.
In logistic models, the goal is to estimate the unknown vector of parameters $\boldsymbol\beta$ for the known covariates $\mathbf{X}_i$. 
But in the systematic component, $\eta_i$ is unobserved. Ordinary Linear Regression doesn't work in this case. 
Fortunately, the link function in logit models has a close form.
Iteratively Reweighted Least Squares method (IRLS) (Lawson 1961) can get the solution. 

<!-- One iteration of this approach includes four steps. -->
<!-- The first step starts from the current estimated coefficients $\boldsymbol{\hat\beta}^{(0)}$. Then the current estimates of $\hat{\eta_i}^{(0)}=\mathbf{x}_i'\boldsymbol{\hat\beta}^{(0)}$ and $\hat \pi_i^{(0)}=\frac{\exp[\hat\eta_i^{(0)}]}{1+\exp[\hat\eta_i^{(0)}]}$. -->
<!-- But current $\hat{\eta_i}^{(0)}$ is different with the true $\eta_i$. -->
<!-- The second step will update the current estimates by adding a correction term. -->
<!-- Using the first two terms of Taylor series, -->

<!-- \begin{equation} -->
<!-- \hat{\eta_i}^{(1)}=\hat{\eta_i}^{(0)} +(y_i-\hat\mu_i^{(0)})\cdot \frac{d\hat\eta_i^{(0)}}{d\hat\mu_i^{(0)}} -->
<!-- \end{equation} -->

<!--  Since $E[Y_i]=\mu_i=n_i\pi_i$, it is also easy to get  -->

<!-- \begin{equation} -->
<!-- \frac{d \eta_i}{d \mu_i}=\frac{1}{n_i}\cdot\frac{d \eta_i}{d \pi_i}=\frac{1}{n_i}\left(\frac{1}{\pi_i}+\frac{1}{1-\pi_i}\right)=\frac1{n_i\pi_i(1-\pi_i)} -->
<!-- \end{equation} -->
<!-- Therefore, -->
<!-- \begin{equation} -->
<!-- \hat{\eta_i}^{(1)}=\hat{\eta_i}^{(0)} + \frac{y_i-n_i\hat\pi_i^{(0)}}{n_i\hat\pi_i^{(0)}(1-\hat\pi_i^{(0)})} -->
<!-- \end{equation} -->

<!-- The third step is to calculate the diagonal weight matrix $\mathbf{W}$ in the Fisher scoring algorithm. -->
<!-- It is known that the binomial distribution has $Var[Y_i]=n_i\pi_i(1-\pi_i)$.  -->

<!-- \begin{equation} -->
<!-- w_{ii}=\left[Var[Y_i](\frac{d \eta_i}{d \mu_i})^2\right]^{-1}= -->
<!-- n_i\hat\pi_i(1-\hat\pi_i) -->
<!-- \end{equation} -->

<!-- The final step improves estimate of $\boldsymbol{\hat\beta}^{(1)}$ using the weighted least-squares method -->

<!-- \begin{equation} -->
<!-- \hat{\boldsymbol{\beta}}^{(1)}=\mathbf{(X'WX)^{-1}}\mathbf{X'W}\boldsymbol{\hat{\eta}}^{(1)} -->
<!-- \end{equation} -->

<!-- The four steps will repeat until the procedure convergence. And the result gives -->

<!-- \begin{equation} -->
<!-- Var[\hat{\boldsymbol{\beta}}]=\mathbf{(X'WX)^{-1}} -->
<!-- \end{equation} -->

In IRLS algorithm, when the probability and sample size of one mode is small (e.g. $\hat\pi_i=0.05$), it would be assigned a small weight. "The standard error is artificially compressed, which leads us to overestimate the precision of the proportion estimate." [@lipseyPracticalMetaanalysis2001, chap. 3]
Sometimes, researcher can combine several modes such as walking and biking to active mode and relief this issue.
Otherwise, one has to looking for other algorithm, such as data augmentation by Markov chain Monte Carlo (MCMC) to get the more stable estimates.

<!--chapter:end:files/05-methods.Rmd-->


# Several Issues

This chapter goes through several common issues in modeling and discuss the potential risks and remedies.
Ignoring these issues could lead to severe biased estimates or spurious relationships.
In a travel-urban form paper, the part of methodology usually includes data, variables, models, and results. Before publication, the researcher must have done a lot of work: trying any possible data sources, conducting variable selection and completing model validation.
These works often are not shown in the paper.
Therefore, the topics in this chapter are potential issues. 
A suitable literature review or convincing criticism requires to gather the original data and replicate the models in the published paper.

## Assumptions

- Additive and linearity

For regression models, relationship between the explanatory variables should be additive.
The initial relationship could be multiplicative or exponential. log transform can covert the multiplicative relationship to additive [@choiAnalysisMetroRidership2012].
The proper specification requires some theoretical and empirical supports.
Gravity Law discloses that travel distance has a multiplicative (inverse) relationship with the 'masses' of two places. The 'masses' related variable such as population size can be added to the regression model as a additive factor. 

Some built environment and socioeconomic factors may have interaction effects. For example, the high density in developed countries have different effects on travel to the developing countries [@ewingDoesCompactDevelopment2017].
Exploring the two-way or even higher order interaction effects is not common in travel-urban form studies.
@leeComparingImpactsLocal2020 examine the interaction effects between population weighted density (PWD) and D-variables at tract level. Other possible paired interactions are not considered in this study.

Linearity means the function of independent variables is compatible with addition and scaling. 
That is $f(x+y)=f(x)+f(y)$ and $f(a\cdot x)=a\cdot f(x)$.
When the assumption of linearity still doesn't hold after transformation, the study should look for other non-linear models and corresponding approaches. 
Does the regression models show the linear property after log transformation?
There are more recent studies start to pay attention to this topic [@ewingReducingVehicleMiles2020;@dingNonlinearAssociationsZonal2021]. More discussion of the non-linear relationship is placed in the next chapter.


- Independent Identically Distributed (IID)

Another essential assumption is that random error are Independent Identically Distributed (IID). Random error is also called residual, which refer to the difference between observed $\mathbf{y}$ and fitted $\mathbf{\hat y}$.
That is $\mathbf{e}=\mathbf{y}-\mathbf{\hat y}$, where $\mathbf{\hat y}$ are the linear combinations of predictors $\mathbf{X}$. Residuals represent the part can not be explained by the model.

<!-- \begin{equation} -->
<!-- \mathbf{e}=\mathbf{y}-\mathbf{\hat y} -->
<!-- (\#eq:residual) -->
<!-- \end{equation} -->

'Identical' means that random errors should have zero mean and constant variance. The expected value, the variances, and the covariances among the random errors are the first- and second-moment of residuals. 
That is $E(\varepsilon) = 0$ and $Var(\varepsilon) = \sigma^2$.
The homogeneity of variance is also called homoscedasticity. 
To satisfy this assumption, some studies chose a subset such as VMT by nonwork purposes [@chatmanDeconstructingDevelopmentDensity2008], bus ridership by time of day, or school children's metro ridership [@liuInfluenceBuiltEnvironment2018]. So it also depends on the research design.



<!-- \begin{equation} -->
<!-- E(\varepsilon) = 0, \quad Var(\varepsilon) = \sigma^2 -->
<!-- (\#eq:identical) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- Cov[\varepsilon_i,\varepsilon_j] = 0,\quad i\neq j -->
<!-- (\#eq:indenpendent) -->
<!-- \end{equation} -->

'Independent' requires the random errors are uncorrelated. That is $Cov[\varepsilon_i,\varepsilon_j] = 0$,$i\neq j$
In the stage of survey design, researchers try to collect the data by random sampling.
The studies using secondary data usually believe every observations are independent.
But in travel-urban form studies, many dataset cover all the neighbored units in a region. In this situation, the independent assumption often doesn't hold.


- Normality

The common words in travel-urban form literature are that "We use logarithm transform on travel variables to address the issues of normality and linearity."
Evidence has demonstrated that travel distance and frequency are not Normal distributed. The Zipf's law also prove that travel distance follows a power distribution. Using logarithm transformations, the skewed distribution can be converted to an approximate normal distribution.
Meanwhile, some scholars assume the observed travel data are left censored and choose Tobit regression [@chatmanHowDensityMixed2003; @boarnetWalkingUrbanDesign2008].

Actually, the normality requirement for response is a misunderstanding. Neither response variable nor predictor is required to be normal. 
Normality is the requirement for residuals.
Note that least squares method itself needs zero mean and constant variance rather than normality. 
When conducting hypothesis test and infering confidence intervals, the required assumption is that the  response conditional on covariates is normal, that is $\mathbf{y|x}\sim N (\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$. Maximum Likelihood Methods also requires this assumption.
There are some quantitative methods which can examine normality of the transformed distributions.

- A brief Discussion

In urban studies, the geographic data often have both independent and identical issues, which are called the spatial autocorrelation and heterogeneity [@zhangEvaluationSpatialAutocorrelation2009].
That means, the observation in one location be affected by the neighbored areas for some unknown reasons. And the observations from city to city or region to region are generated by distinct mechanisms.
Once the conditions of IID are satisfied, the Gauss - Markov theorem proves that least-square method could give the minimum-variance unbiased estimators (MVUE) or called the best linear unbiased estimators (BLUE). These conditions are not strict and make regression method widely applicable.

When the sample size is small, the circumstance of violating normality assumption would make the inference misleading. 
As @lumleyImportanceNormalityAssumption2002 pointed out, if the sample size is large enough such as $n\ge 100$, the t-test and OLS still can give the asymptotically unbiased estimates of mean. But for some long-tailed data, median regressions might be appropriate, that also depends on the research question to be asked.
Few paper in travel-urban form literature show their normality test in the published version and investigate the influences.


## Adequacy

The estimation and inference themselves can not demonstrate a model's performance.
If the primary assumptions is violated, the estimations could be biased and the model could be misleading. These problems can also happen when the model is not correctly specified. Making a proper diagnosis and validation are the first thing we need to do when fitting some models.

<!-- "There are different types of model specification errors in regression analysis: " -->
<!-- (1) necessary independent variable(s) is (are) missing from the model (i.e., omission error) (Greene 2012, 96--8, Section 4.3.2); -->
<!-- (2) irrelevant independent variable(s) is (are) added to the model (i.e., commission error) (Greene 2012, 98, Section 4.3.3); -->
<!-- (3) independent variables have an incorrect functional form (Ramsey 1969; Thursby and Schmidt 1977; Hausman 1978; Davidson and MacKinnon 1981); -->
<!-- (4) variables in the model are not accurately measured (i.e., measurement error) (Wooldridge 2015, 287--92, Section 9.4); and -->
<!-- (5) the model is not stand-alone but instead belongs to a system of simultaneous equations (Ramsey 1969; Hausman 1978). -->

- Residuals Analysis

The major assumptions, both IID and normality are related to residual. 
Residual diagnosis is an essential step for modeling validation.
There are several scaled residuals can help the diagnosis. 
Since $MSE$ is the expected variance of error $\hat\sigma^2$ and $E[\varepsilon]=0$, standardized residuals ($d_i=\frac{e_i}{\sqrt{MSE}}=e_i\sqrt{\frac{n-p}{\sum_{i=1}^n e_i^2}}$, $i=1,2,...,n$) should follow a standard normal distribution.

Recall random error $\mathbf{e}=\mathbf{y}-\mathbf{\hat y}=(\mathbf{I}-\mathbf{H})\mathbf{y}$ and hat matrix $\mathbf{H}=\mathbf{X}(\mathbf{X'X})^{-1}\mathbf{X'}$.
Let $h_{ii}$ denote the $i$th diagonal element of hat matrix.
Studentized Residuals can be expressed by $r_i=\frac{e_i}{\sqrt{MSE(1-h_{ii})}}$, $i=1,2,...,n$.
It is proved that $0\le h_{ii}\le1$.
An observation with $h_{ii}$ closed to one will return a large value of $r_i$. The $x_i$ who has strong influence on fitted value is called leverage point.
Ideally, the scaled residual have zero mean and unit variance. Hence, an observation with $|d_i|>3$ or $|r_i|>3$ is a potential outlier.

Predicted Residual Error Sum of Squares (PRESS) can also be used to detect outliers.
This method predicts the $i^{th}$ fitted response by excluding the $i^{th}$ observation and examine the influence of this point.
The corresponding error $e_{(i)}=e_{i}/(1-h_{ii})$ and $V[e_{(i)}]=\sigma^2/(1-h_{ii})$.
Thus, if $MSE$ is a good estimate of $\sigma^2$, PRESS residuals is equivalent to Studentized Residuals.
$\frac{e_{(i)}}{\sqrt{V[e_{(i)}]}}=\frac{e_i/(1-h_{ii})}{\sqrt{\sigma^2/(1-h_{ii})}}=\frac{e_i}{\sqrt{\sigma^2(1-h_{ii})}}$.

Residual plot can show the pattern of the residuals against fitted $\mathbf{\hat y}$.
If the assumptions are valid, the shape of points should like a envelope and be evenly distributed around the horizontal line of $e=0$ (Figure \@ref(fig:resid-plot) left panel).
A funnel shape in residual plot shows that the variance of error is a function of $\hat y$ (Figure \@ref(fig:resid-plot) right panel). A suitable transformation to response or predictor could stabilize the variance.
A curved shape means the assumption of linearity is not valid (Figure \@ref(fig:resid-plot) middel panel). It implies that adding quadratic terms or higher-order terms might be suitable.
Residual plot is an essential tool for regression model diagnosis. But it is rarely seen travel-urban form literature.

```{r resid-plot, fig.dim=c(12,4),out.width='100%',fig.show='hold',fig.cap="Three examples of Residual plot"}
set.seed(1234)
n=1000
data <- data.frame(
        x.p1= rpois(n,2),
        x.p2 = rpois(n,4),
        x.n1= rnorm(n,2,1),
        x.n2 = rnorm(n,4,1)
        )
data <- data %>% mutate( y.n= (1 + 2*x.p1 + 2*x.p2 + rnorm(n)), # as.integer
                         y.p= rpois(n,1 + 2*x.n1 + 2*x.n2))
m1 <- lm(y.n ~ x.p1 + x.p2,data)
m2 <- lm(log(y.n)~ x.p1 + x.p2, data %>% filter(y.n >0)) 
m3 <- lm(log(y.n)~ log(x.p1) + log(x.p2), data %>% filter(x.p1 >0, x.p2 >0))
par(mfrow = c(1, 3))
plot(m1,1);plot(m2,1);plot(m3,1)
```


A histogram of residuals can check the normality assumption. In the histogram, probability distribution of VMT is usually highly right-skewed. While the log-transformed VMT is more close to a bell curve.
A better way is a normal quantile -- quantile (QQ) plot of the residuals.
An ideal cumulative normal distribution should plot as a straight line.
Only looking at the $R^2$ and p-values cannot disclose this feature. 

```{r qq-plot, fig.dim=c(12,4),out.width='100%',fig.show='hold',fig.cap="Three examples of Normal Q-Q Plot"}
par(mfrow = c(1, 3))
plot(m1,2);plot(m2,2);plot(m3,2)
```



- Goodness of fit

Coefficient of Determination $R^2$ is a proportion to assess the quality of fitted model. This measurement tell us how good the model can explain the data.

\begin{equation}
R^2 =\frac{SSR}{SST}=1-\frac{SSE}{SST}
(\#eq:rsq)
\end{equation}

when $R^2$ is close to $1$, the most of variation in response can be explained by the fitted model. Although $R^2$ is not the only criteria of a good model, it is often available in most published papers. Recall the discussion in Part I, the aggregated data will eliminate the difference among individuals, households, or neighborhoods. In the new variance structure, $SSE$ will be much less than disaggregated model. The $R^2$ in many disaggregate studies are around 0.3, while the $R^2$ in some aggregate studies can reach 0.8. A seriously underfitting model's outputs could be biased and unstable.

A fact is that adding predictors into the model will never decrease $R^2$. 
If a VMT-urban form model added many predictors but adjusted $R^2$ is still low, the association between travel distance and built environment might be spurious.
$R^2$ also cannot reflect the different number of parameters in the models. Adjusted $R^2$ can address this issue by introducing degree of freedom. The degree of freedom denotes the amount of information required to know.

\begin{equation}
\begin{split}
df_T =& df_R + df_E\\
n-1=&(p-1)+(n-p)
\end{split}
(\#eq:df)
\end{equation}

Then, the mean square (MS) of each sum of squares (SS) can be calculated by $MS=SS/df$. The mean square error $MSE$ is also called as the expected value of error variance $\hat\sigma^2=MSE=SSE/(n-p)$. $n-p$ is the degree of freedom. Then adjusted $R^2$ is

\begin{equation}
R_{adj}^2 = 1-\frac{MSE}{MST} = 1-\frac{SSE/(n-p)}{SST/(n-1)}
(\#eq:rsq-adj)
\end{equation}

Another similar method is $R^2$ for prediction based on PRESS.
Recall the PRESS statistic is the prediction error sum of square by fitting a model with $n-1$ observations.

\begin{equation}
PRESS = \sum_{i=1}^n(y_i-\hat y_{(i)})^2= \sum_{i=1}^n\left(\frac{e_i}{1-h_{ii}}\right)^2
(\#eq:press)
\end{equation}


A model with smaller PRESS has a better ability of prediction. The $R^2$ for prediction is

\begin{equation}
R_{pred}^2 = 1-\frac{PRESS}{MST}
(\#eq:rsq-pred)
\end{equation}




- Heterogeneity and Autocorrelation

For spatio-temporal data, the observations often have some relationship over time or space.
When the assumption of constant variance is violated, the linear model has the issue of heterogeneity.
When the assumption of independent errors is violated, the linear model with serially correlated errors is called autocorrelation.
Spatial heterogeneity and spatial autocorrelation are two typical phenomenon in urban studies. All the neighboring geographic entities or stages could impact each other, or sharing the similar environment. 
Failing to deal with spatial heterogeneity could produce fake significance in hypothesis test and lead to systematically biased estimates.
Although the estimation of coefficients could be unbiased when there is spatial autocorrelation in regression models, the estimated error variance would be biased and make misleading significance tests [@zhangEvaluationSpatialAutocorrelation2009].

Here is a simple case of heterogeneous model.
Recall Generalized least square estimates 
<!-- \@ref(eq:glsq-e) and \@ref(eq:glsq-v) -->
, if the residuals are independent but variances are not constant, a simple linear model becomes $\boldsymbol{\varepsilon}\sim MVN(\mathbf{0},\sigma^2\mathbf{V})$ where $\mathbf{V}$ is a diagonal matrix with $v_{ii}=x^2_i$.
Then $\mathbf{X'V^{-1}X}=n$ and the weighted least squares solution is $\hat\beta_{1,WLS}=\frac1n\sum_{i=1}^{n}\frac{y_i}{x_i}$ and $\hat\sigma^2_{WLS}=\frac1{n-1}\sum_{i=1}^{n}\frac{(y_i-\hat\beta_{1}x_i)^2}{x_i^2}$.
In this case, the OLS estimates of coefficients are still unbiased but no longer efficient.
The estimates of variances are biased. The corresponding hypothesis test and confidence interval would be misleading.

<!-- \begin{equation} -->
<!-- \mathbf{V}=\begin{bmatrix}  -->
<!-- x_1^2 & 0 & \dots & 0 \\   -->
<!-- 0 & x_2^2 & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & x_n^2 \end{bmatrix},\quad  -->
<!-- \mathbf{V}^{-1}=\begin{bmatrix}  -->
<!-- \frac1{x_1^2} & 0 & \dots & 0 \\   -->
<!-- 0 & \frac1{x_2^2} & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & \frac1{x_n^2} \end{bmatrix} -->
<!-- (\#eq:hete-matrix) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \hat\beta_{1,WLS}=\frac1n\sum_{i=1}^{n}\frac{y_i}{x_i} -->
<!-- (\#eq:hete-e) -->
<!-- \end{equation} -->

<!-- and -->

<!-- \begin{equation} -->
<!-- \hat\sigma^2_{WLS}=\frac1{n-1}\sum_{i=1}^{n}\frac{(y_i-\hat\beta_{1}x_i)^2}{x_i^2} -->
<!-- (\#eq:hete-v) -->
<!-- \end{equation} -->

If the data is aggregated to a upper level, it is the cases of geographic modifiable areal unit problem (MAUP) discussed in previous chapter.
Let $u_j$ and $v_j$ are the response and predictors of $j$th household in a neighborhood. $n_i$ is the sample size in each neighborhood. Then $y_i=\sum_{j=1}^{n_i}u_j/n_i$ and $X_i=\sum_{j=1}^{n_i}v_j/n_i$. 
Then $\mathbf{X'V^{-1}X}=\sum_{i=1}^nn_ix_i^2$ and the WLS estimate of $\beta_1$ is $\hat\beta_{1,WLS}=\frac1n\frac{\sum_{i=1}^{n}n_ix_iy_i}{\sum_{i=1}^{n}n_ix_i^2}$ and $V[\hat\beta_{1,WLS}]=\frac{\sigma^2}{\sum_{i=1}^{n}n_ix_i^2}$.
There are three procedures, Bartlett's likelihood ratio test, Goldfeld-Quandt test, or Breusch-Pagan test which can be used to examine heterogeneity [@ravishankerFirstCourseLinear2020, 8.1.3, pp.288-290]

<!-- In this case, -->
<!-- \begin{equation} -->
<!-- \mathbf{V}=\begin{bmatrix}  -->
<!-- \frac1{n_1} & 0 & \dots & 0 \\   -->
<!-- 0 & \frac1{n_2} & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & \frac1{n_n} \end{bmatrix},\quad  -->
<!-- \mathbf{V}^{-1}=\begin{bmatrix}  -->
<!-- n_1 & 0 & \dots & 0 \\   -->
<!-- 0 & n_2 & \dots & 0 \\   -->
<!-- \vdots & \vdots & \ddots & \vdots \\   -->
<!-- 0 & 0 & \dots & n_n \end{bmatrix} -->
<!-- (\#eq:agg-matrix) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \hat\beta_{1,WLS}=\frac1n\frac{\sum_{i=1}^{n}n_ix_iy_i}{\sum_{i=1}^{n}n_ix_i^2} -->
<!-- (\#eq:agg-e) -->
<!-- \end{equation} -->
<!-- and -->
<!-- \begin{equation} -->
<!-- V[\hat\beta_{1,WLS}]=\frac{V[\sum_{i=1}^{n}n_ix_iy_i]}{(\sum_{i=1}^{n}n_ix_i^2)^2}=\frac{\sum_{i=1}^{n}n_i^2x_i^2\sigma^2/n_i}{(\sum_{i=1}^{n}n_ix_i^2)^2}=\frac{\sigma^2}{\sum_{i=1}^{n}n_ix_i^2} -->
<!-- (\#eq:agg-v) -->
<!-- \end{equation} -->

Take a case of single dimension autocorrelation for example, it assumes the model have constant variance. That is $E[\varepsilon]=0$. 
But $Cov[\varepsilon_i,\varepsilon_j]=\sigma^2\rho^{|j-i|}$, $i,j=1,2,...,n$ and $|\rho|<1$
This is a linear regression with autoregressive order 1 (AR(1)). 
The estimates of $\boldsymbol{\hat\beta}$ is the same with the GLS solutions, which are
$\boldsymbol{\hat\beta}_{GLS}=(\mathbf{X'V^{-1}X})^{-1}\mathbf{X'V^{-1}}\mathbf{y}$ and 
$\widehat{V[\boldsymbol{\hat\beta}_{GLS}]}=\hat\sigma^2_{GLS}(\mathbf{X'V^{-1}X})^{-1}$,
where $\hat\sigma^2_{GLS}=\frac1{n-p}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})'\mathbf{V^{-1}}(\mathbf{y-X}\boldsymbol{\hat\beta}_{GLS})$.
The variance-covariance matrix $\mathbf{V}$ is also called Toeplitz matrix.

<!-- \begin{equation} -->
<!-- \mathbf{V}=\begin{bmatrix}  -->
<!-- 1 & \rho & \rho^2 & \dots & \rho^{n-1} \\   -->
<!-- \rho & 1 & \rho & \dots & \rho^{n-2} \\   -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots \\   -->
<!-- \rho^{n-1} & \rho^{n-2} & \rho^{n-3} & \dots & 1 \end{bmatrix},\quad  -->
<!-- \{\mathbf{V}^{-1}\}_{ij}=\begin{cases}  -->
<!-- \frac{1}{1-\rho^2} & \text{if } i=j=1,n \\   -->
<!-- \frac{1+\rho^2}{1-\rho^2} & \text{if } i=j=2,...,n-1 \\   -->
<!-- \frac{-\rho}{1-\rho^2} & \text{if } |j-i|=1 \\   -->
<!-- 0  & \text{otherwise} \end{cases} -->
<!-- (\#eq:auto-matrix) -->
<!-- \end{equation} -->

It is can be verified that $\boldsymbol{\hat\beta}_{GLS}\le\boldsymbol{\hat\beta}_{OLS}$ always holds and they are equal when $\mathbf{V=I}$ or $\rho=0$. It proves that $\boldsymbol{\hat\beta}_{GLS}$ are the best linear unbiased estimators (BLUE).
This case can be extended to miltiple regression models and the autocorrelation of a stationary stochastic process at lag-k.
Durbin-Watson test is used to test the null hypothesis of $\rho=0$.

- A brief discussion

Only reporting the $R^2$ and p-values cannot tell us whether the model is valid or not.
Residual analysis can detect the ill-condition models and should not be ignored in regression studies.

For non-OLS model, such as logistic regression, $R^2$ doesn't exist. Pseudo $R^2$s are used to evaluate the goodness-of-fit. McFadden's $R^2$, Count $R^2$, Efron's $R^2$ are some common Pseudo $R^2$ with different interpretations. For example McFadden's $R^2$ use the ratio of the log likelihood to reflect how better is the full model than the null model. When examining the effects of built-environment factors on VMT at multiple spatial levels, @hongHowBuiltenvironmentFactors2014 use a Bayesian version of adjusted $R^2$ for multilevel models which proposed by @gelmanBayesianMeasuresExplained2006.
Therefore, cross comparing $R^2$ and various Pseudo $R^2$ is meaningless. Pseudo $R^2$s on different data are also not comparable.
Next question is what is a satisfactory model fit for different categories.

Some techniques can deal with spatial heterogeneity and autocorrelation and improve the model performance. More discussions of addressing spatial effects are placed on the next chapter.



## Multicollinearity

Multicollinearity or near-linear dependence refers to the models with highly correlated predictors. When data is generated from experimental design, the treatments $X$ could be fixed variables and be orthogonal. But travel-urban form model is observational studies and nothing can be controlled as in lab. It is known that there are complex correlations among the built-environment predictors themselves.
Although, the basic IID assumptions do not require that all predictors $\mathbf{X}$ are independent, when the predictors are near-linear dependent, the model is ill-conditioned and the least-square estimators are unstable.

- Variance Inflation

Multicollinearity can make the variances inflated and impact model precision seriously. If some of predictors are exact linear dependent, the matrix $(\mathbf{X'X})^{-1}$ is symmetric but non-invertible. By spectral decomposition of symmetric matrix, $\mathbf{X'X}=\mathbf{P'\Lambda P}$ where $\Lambda=\text{diag}(\lambda_1,...,\lambda_p)$, $\lambda_i$'s are eigenvalues of $\mathbf{X'X}$, $\mathbf{P}$ is an orthogonla matrix whose columns are normalize eigenvectors. Then the total-variance of $\boldsymbol{\hat\beta}_{LS}$ is $\sigma^2\sum_{j=1}^p1/\lambda_j$.
If the predictors are near-linear dependent or nearly singular, $\lambda_j$s may be very small and the total-variance of $\boldsymbol{\hat\beta}_{LS}$ is highly inflated.
For the same reason, the correlation matrix using unit length scaling $\mathbf{Z'Z}$ will has a inverse matrix with inflated variances. That means that the diagonal elements of $(\mathbf{Z'Z})^{-1}$ are not all equal to one. The diagonal elements are called **Variance Inflation Factors**, which can be used to examine multicollinearity. The VIF for a particular predictor is examined by $\mathrm{VIF}_j=\frac{1}{1-R_j^2}$,
where $R_j^2$ is the coefficient of determination by regressing $x_j$ on all the remaining predictors.

<!-- \begin{equation} -->
<!-- \mathrm{VIF}_j=\frac{1}{1-R_j^2} -->
<!-- (\#eq:vif) -->
<!-- \end{equation} -->

A common approach is to drop off the predictor with greatest VIF and refit the model until all VIFs are less than 10. However, dropping off one or more predictors will lose many information which might be valuable for explaining response. Due to the complexity among predictors, dropping off the predictor with the greatest VIF is not always the best choice. Sometimes, removing a predictor with moderate VIF can make all VIFs less than 10 in the refitted model. Moreover, there is not an unique criteria for VIF value. When the relationship between predictor and response is weak, or the $R^2$ is low, the VIFs less than 10 may also affect the ability of estimation dramatically.
Orthogonalization before fitting the model might be helpful. Other approaches such as principal components regression, ridge regression, etc. could deal with multicollinearity better.

- Principal Components Regression

Principal Components Regression (PCR) is a dimension reduction method which projecting the original predictors into a lower-dimension space.
It still uses a singular value decomposition (SVD) and get $\mathbf{X'X}=\mathbf{Q\Lambda Q}'$
$\mathbf{Q}$ are the matrix who columns are orthogonal eigenvectors of $\mathbf{X'X}$. $\Lambda=\text{diag}(\lambda_1,...,\lambda_p)$ is decreasing eigenvalues with $\lambda_1\ge\lambda_1\ge\cdots\ge\lambda_p$. Then the linear model can transfer to
$\mathbf{y} = \mathbf{XQQ}'\boldsymbol\beta + \varepsilon = \mathbf{Z}\boldsymbol\theta + \varepsilon$,
where $\mathbf{Z}=\mathbf{XQ}$, $\boldsymbol\theta=\mathbf{Q}'\boldsymbol\beta$. 
$\boldsymbol\theta$ is called the regression parameters of the principal components.
$\mathbf{Z}=\{\mathbf{z}_1,...,\mathbf{z}_p\}$ is known as the matrix of principal components of $\mathbf{X'X}$. 
Then $\mathbf{z}'_j\mathbf{z}_j=\lambda_j$ is the $j$th largest eigenvalue of $\mathbf{X'X}$.
PCR usually chooses several $\mathbf{z}_j$s with largest $\lambda_j$s and can eliminate multicollinearity. 
Its estimates $\boldsymbol{\hat\beta}_{P}$ results in low bias but the mean squared error $MSE(\boldsymbol{\hat\beta}_{P})$ is smaller than that of least square $MSE(\boldsymbol{\hat\beta}_{LS})$.



- Ridge Regression

Least squares method gives the unbiased estimates of regression coefficients. 
However, multicollinearity will lead to inflated variance and make the estimates unstable and unreliable.
To get a smaller variance, a tradeoff is to release the requirement of unbiasedness.
@hoerlRidgeRegressionBiased1970 proposed ridge regression to address the nonorthogonal problems.
The estimates of ridge regression are $\boldsymbol{\hat\beta}_{R}=(\mathbf{X'X}+k\mathbf{I})^{-1}\mathbf{X'}\mathbf{y}$,
where $k\ge0$ is a selected constant and is called a biasing parameter. When $k=0$, the ridge estimator reduces to least squares estimators.

<!-- \begin{equation} -->
<!-- \boldsymbol{\hat\beta}_{R}=(\mathbf{X'X}+k\mathbf{I})^{-1}\mathbf{X'}\mathbf{y} -->
<!-- (\#eq:ridge-e) -->
<!-- \end{equation} -->


<!-- Denote $\boldsymbol{\hat\beta}_{R}$ are biased estimates but its variance is small enough. -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathrm{MSE}(\boldsymbol{\hat\beta}_{R})&=E[\boldsymbol{\hat\beta}_{R}-\boldsymbol{\beta}]^2=\mathrm{Var}[\boldsymbol{\hat\beta}_{R}]+\mathrm{Bias}[\boldsymbol{\hat\beta}_{R}]^2\\ -->
<!-- &<\mathrm{MSE}(\boldsymbol{\hat\beta}_{LS})=\mathrm{Var}[\boldsymbol{\hat\beta}_{LS}] -->
<!-- \end{split} -->
<!-- \end{equation} -->


<!-- When $\mathbf{X}$ is nonsingular and $(\mathbf{X'X})^{-1}$ exists, the ridge estimator is a linear transformation of $\boldsymbol{\hat\beta}_{LS}$. That is $\boldsymbol{\hat\beta}_{R}=\mathbf{Z}_k\boldsymbol{\hat\beta}_{LS}$ where $\mathbf{Z}_k=(\mathbf{X'X}+k\mathbf{I})^{-1}\mathbf{X'X}$ -->

<!-- Recall the total-variance of $\boldsymbol{\hat\beta}_{LS}$ is $\sigma^2\sum_{j=1}^p1/\lambda_j$. -->
<!-- The total-variance of $\boldsymbol{\hat\beta}_{R}$ is  -->

<!-- \begin{equation} -->
<!-- \mathrm{tr}(\mathrm{Cov}[\boldsymbol{\hat\beta}_{R}])=\sigma^2\sum_{j=1}^p\frac{\lambda_j}{(\lambda_j+k)^2} -->
<!-- \end{equation} -->

<!-- Thus, introducing $k$ into the model can avoid tiny denominators and eliminate the inflated variance. -->
<!-- Choosing a proper value of $k$ is to keep the balance of $\mathrm{MSE}$ and $\mathrm{Bias}$. -->
<!-- The bias in $\boldsymbol{\hat\beta}_{R}$ is  -->

<!-- \begin{equation} -->
<!-- \mathrm{Bias}(\boldsymbol{\hat\beta}_{R})^2=k^2\boldsymbol{\beta}'(\mathbf{X'X}+k\mathbf{I})^{-2}\boldsymbol{\beta} -->
<!-- \end{equation} -->

<!-- Hence,increasing $k$ will reduce $MSE$ but make greater $bias$. -->
<!-- Ridge trace is a plot of $\boldsymbol{\hat\beta}_{R}$ versus $k$ that can help to select a suitable value of $k$. -->
<!-- First, at the value of $k$, the estimates should be stable. Second, the estimated coefficients should have proper sign and reasonable values. Third, the $SSE$ also should has a reasonable value. -->

<!-- Ridge regression will not give a greater $R^2$ than least squares method. Because the total sum of squares is fixed. -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathrm{SSE}(\boldsymbol{\hat\beta}_{R})&=(\mathbf{y-X}\boldsymbol{\hat\beta}_{R})'(\mathbf{y-X}\boldsymbol{\hat\beta}_{R})\\ -->
<!-- &=(\mathbf{y-X}\boldsymbol{\hat\beta}_{LS})'(\mathbf{y-X}\boldsymbol{\hat\beta}_{LS})+(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})'\mathbf{X'X}(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})\\ -->
<!-- &=\mathrm{SSE}(\boldsymbol{\hat\beta}_{LS})+(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})'\mathbf{X'X}(\boldsymbol{\hat\beta}_{LS}-\boldsymbol{\hat\beta}_{R})\\ -->
<!-- &\ge \mathrm{SSE}(\boldsymbol{\hat\beta}_{LS}) -->
<!-- \end{split} -->
<!-- \end{equation} -->

The advantage of ridge regression is to obtain a suitable set of parameter estimates rather than to improve the fitness. It could have a better prediction ability than least squares.
It can also be useful for variable selection. The variables with unstable ridge trace or tending toward the value of zero can be removed from the model.
In many case, the ridge trace is erratic divergence and may revert back to least square estimates. 
[@jensenSurrogateModelsIllconditioned2010a;@jensenVariationsRidgeTraces2012] proposed surrogate model to further improve ridge regression. Surrogate model chooses $k$ depend on matrix $\mathbf{X}$ and free to $\mathbf{Y}$. 

<!-- Using a compact singular value decomposition (SVD), the original can be decomposed to maxtix$\mathbf{X}=\mathbf{PD_{\xi}Q}'$. $\mathbf{P}$ and $\mathbf{Q}$ are orthogonal. The columns of $\mathbf{P}$ and $\mathbf{Q}$ are left-singular vectors and right-singular vectors of $\mathbf{X}$. -->
<!-- It satisfies $\mathbf{P'P}=\mathbf{I}$ and $\mathbf{D}_{\xi}=\text{diag}(\xi_1,...,\xi_p)$ is decreasing singular values. Then $\mathbf{X}_k=\mathbf{PD}((\xi_i^2+k_i)^{1/2})\mathbf{Q}'$ and  -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{X'X}=&\mathbf{QD}_\xi^2\mathbf{Q}'\\ -->
<!-- \mathbf{X}_k'\mathbf{X}_k=&\mathbf{Q(D_\xi^2+K)}\mathbf{Q}'\quad\text{generalized surrogate}\\ -->
<!-- \mathbf{X}_k'\mathbf{X}_k=&\mathbf{QD}_\xi^2\mathbf{Q}'+k\mathbf{I}\quad\text{ordinary surrogate} -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- and the surrogate solution $\boldsymbol{\hat\beta}_{S}$ is -->

<!-- \begin{equation} -->
<!-- \mathbf{Q(D^2_{\xi}+K)Q}'\boldsymbol{\hat\beta}_{S}=\mathbf{X}_k=\mathbf{QD}((\xi_i^2+k_i)^{1/2})\mathbf{P}'\mathbf{y} -->
<!-- (\#eq:surrogate-e) -->
<!-- \end{equation} -->

<!-- Jensen and Ramirez proved that $\mathrm{SSE}(\boldsymbol{\hat\beta}_{S})< \mathrm{SSE}(\boldsymbol{\hat\beta}_{S})$ and surrogate model's canonical traces are monotone in $k$. -->

- Lasso Regression

Ridge regression can be understood as a restricted least squares problem. Denote the constraint $s$, the solution of ridge coefficient estimates satisfies

\begin{equation}
\min_{\boldsymbol\beta}\left\{\sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_j\right)^2\right\}\text{ subject to } \sum_{j=1}^p\beta_j^2\le s
\end{equation}

Another approach is to replace the constraint term $\sum_{j=1}^p\beta_j^2\le s$ with $\sum_{j=1}^p|\beta_j|\le s$. This method is called lasso regression. 

<!-- \begin{equation} -->
<!-- \min_{\boldsymbol\beta}\left\{\sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_j\right)^2\right\}\text{ subject to } \sum_{j=1}^p|\beta_j|\le s -->
<!-- \end{equation} -->

Suppose the case of two predictors, the quadratic loss function creates a spherical constraint for a geometric illustration, while the norm loss function is a diamond. The contours of $\mathrm{SSE}$ are many expanding ellipses centered around least square estimate $\hat\beta_{LS}$. Each ellipse represents a $k$ value. 
If the restriction $s$ also called 'budget' is very large, the restriction area will cover the point of $\hat\beta_{LS}$. That means $\hat\beta_{LS}=\hat\beta_{R}$ and $k=0$.
When $s$ is small, the solution is to choose the ellipse contacting the constraint area with corresponding $k$ and $\mathrm{SSE}$.
Here lasso constraint has sharp corners at each axes. When the ellipse has a intersect point on one corner, that means one of the coefficient equals zero. But it will not happen on ridge constraint.
Therefore, an improvement of lasso with respect to ridge regression is that lasso allow some estimates $\beta_j=0$. It makes the results more interpretative. Moreover, lasso regression can make variable selection.


- A brief discussion

Many studies such as @alamFactorsAffectingTravel2018, check the multicollinearity issue after fitting the models. 
Using PCR method, some disaggregated travel models' $R^2$ can be over 0.5 [@hamidiLongitudinalStudyChanges2014;@tianTrafficGeneratedMixedUse2015]. 
But the limitation is that the principal components are hard to interpret the meaning.
The results of PCR may just describe the data themselves and they are reproducible but not replicable for other data.

Some research further investigate which variable plays the primary role.
Using meta-regression, @gimRelationshipsLandUse2013 find that accessibility to regional centers is the primary factor affecting travel behavior, while other D-variables are conditional or auxiliary factors.

@handyEnoughAlreadyLet2018 says most of the D-variable models have moderate multicollinearity issue and suggest to replace 'Ds' with 'Accessibility' framework.
A question is, does the multicollinearity disappear in the new 'A' framework?
Based on @handyPlanningAccessibilityTheory2005 's suggestion, @proffittAccessibilityPlanningAmerican2019 create an accessibility index by regression tree. It seems like multicollinearity is not the reason of choosing "D" or "A".

To address multicollinearity, ridge regression, surrogate model, and lasso regression have provided plenty of choices.
@tsaoEstimableGroupEffects2019 contribute another approach to handle multicollinearity which still uses ordinary least squares regression. 
Hence these 'traditional' methods are implementable and interpretative. More important is that they are comparable.
Researcher maybe don't need to rush into some more complex methods or post studies.


```{r,eval=F}
# Compute k-means with k = 3
set.seed(123)
res.km <- kmeans(scale(iris[, -5]), 3, nstart = 25)
# K-means clusters showing the group of each individuals
fviz_cluster(res.km, data = iris[, -5],
             palette = "set3", #c("#2E9FDF", "#00AFBB", "#E7B800")
             geom = "point", pointsize=1, # "text"
             ellipse = T,ellipse.type ="t",   #'confidence'"euclid""convex"
             ellipse.level = 0.75, ellipse.alpha = 0.1,
             ggtheme = theme_bw(), main="Models with trip response variable",xlab="data", ylab="models"
             )
```

```{r,eval=F}
res.km <- kmeans(scale(USArrests), 4, nstart = 25)
fviz_cluster(res.km, data = USArrests,
             palette = "set3", #c("#2E9FDF", "#00AFBB", "#E7B800")
             geom = "point", pointsize=1, # "text"
             ellipse = T,ellipse.type ="t",   #'confidence'"euclid""convex"
             ellipse.level = 0.75, ellipse.alpha = 0.1,legend = "none",
             ggtheme = theme_bw(), main="Models with trip response variable",xlab="data", ylab="models"
             )
```

```{r modelcluster, eval=F, out.width='50%',fig.cap="A diagram of model cluster"}
# Dimension reduction using PCA
res.pca <- prcomp(USArrests,  scale = TRUE) #iris[, -5]
# Coordinates of individuals
ind.coord <- as.data.frame(get_pca_ind(res.pca)$coord)
# Add clusters obtained using the K-means algorithm
res.km <- kmeans(scale(USArrests), 4, nstart = 25)
ind.coord$cluster <- factor(res.km$cluster)
# Add Species groups from the original data sett
# ind.coord$Species <- iris$Species
eigenvalue <- round(get_eigenvalue(res.pca), 1)
variance.percent <- eigenvalue$variance.percent
p <- ggscatter(ind.coord,x = "Dim.1", y = "Dim.2",
  color = "cluster", palette = "npg", title="Find some proper Model clusters",
  ellipse = TRUE, ellipse.type = "t", #"convex"
  ellipse.border.remove=T, ellipse.level = 0.85,
  shape = "cluster", #"Species"
  size = 1.5,legend = "none", ggtheme = theme_bw(),
  xlab = "Various scale or scope",ylab = "Various probability distribution",
) + # theme(axis.text = element_blank()) +
  stat_mean(aes(color = cluster), size = 8,shape = c(1,2,0,3))

ggpar(p,tickslab=F,ticks=F)
```


## Variables Selections

For Multiple Linear Regression, variables selection is an essential step.
But in travel-urban form literature this step often doesn't take up much space.
This section introduces several fundamental methods and discuss the related issues at the end.

- Selecting Procedure

Suppose the data has $10$ candidate predictors. There will be $1024$ possible models. 
All Possible Regressions fit all $2^p$ models using $p$ candidate predictors. Then one can select the best one based on above criteria.
For high-dimension data, fitting all possible regressions is computing intensive and exhaust the degree of freedom.
In practice, people often choose other more efficient procedures such as best-subset selection. Given a number of selected variables $k\le p$, there could be $p\choose k$ possible combinations. By fitting all $p\choose k$ models with $k$ predictors, denote the best model with smallest $SSE$, or largest $R^2$ as $M_k$.
For each $k=1,2,...,p$, there will be $M_0,M_1,...,M_p$ models. The final winner could be identified by comparing PRESS,

Stepwise selection include three methods: forward selection, backward elimination, and stepwise regression.
**Forward selection** starts from null model with only intercept. In each step of this procedure, a variable with greatest simple correlation with the response will be added into the model. If the new variable $x_1$ gets a large $F$ statistic and shows a significance effect on response, the second step will calculate the partial correlations between two sets of residuals. One is from the new fitted model $\hat y=\beta_0+\beta_1x_1$. Another one is the model of other candidates on $x_1$, that is $\hat x_j=\alpha_{0j}+\alpha_{1j}x_1$, $j=2,3,...,(p-1)$. Then the variable with largest partial correlation with $y$ is added into the model.
The two steps will repeat until the partial $F$ statistic is small at a given significant level.
**Backward elimination** starts from the full model with all candidates.
Given a preselected value of $F_0$, each round will remove the variable with smallest $F$ and refit the model with rest predictors.
Then repeat to drop off one variable each round until all remaining predictors have a partial $F_j>F_0$.
**Stepwise regression** combines forward selection and backward elimination together. During the forward steps, if some added predictors have a partial $F_j<F_0$, they also can be removed from the model by backward elimination. 
It is common that some candidate predictors are correlated. 
At the beginning, a predictor $x_1$ having greater simple correlation with response was added into the model.
However, along with a subset of related predictors were added, $x_1$ could become 'useless' in the model. In this case, backward elimination is necessary for achieving the best solution. 

Lasso regression can also help dropping off some variables. 
When reducing variance, lasso allow the least squares estimates shrinking towards zero. This method is called shrinkage.

- Model Evaluation Criteria

**Coefficient of determination** $R^2$is a basic measure of model performance. It has known that adding more predictor always increases $R^2$. So the subset regression will stop to add new variables when the change of $R^2$ is not significant.
The improvement of $R^2_{adj}$ is that it is not a monotone increasing function. So one can select a maximum value on a convex curve.
Maximizing $R^2_{adj}$ is equivalent to minimizing residual mean square $\mathrm{MSE}$
When prediction of the mean response is the interest, $R^2_{pred}$ based on prediction mean square error (PRESS) statistic is more preferred. PRESS is useful for selecting from two competing models.

**Akaike Information Criterion (AIC)** is a penalized measure using maximum entropy. 
AIC will decrease when adding extra terms into the model. Then one can justify when the model can stop adding the new terms. $\mathrm{AIC}=n\ln\left(\frac1n \mathrm{SSE} \right)+ 2p$.
**Bayesian information criterion (BIC)** is the extension of AIC. $\mathrm{BIC}=n\ln\left(\frac{1}{n} \mathrm{SSE} \right)+ p\ln(n)$ @schwarzEstimatingDimensionModel1978 proposed a version of BIC with higher penalty for adding predictors when sample size is large.

Beside above criteria, **Mallows $C_p$ statistic** is an important criteria related to the mean square error.
Suppose the fitted subset model has $p$ variables and expected response $\hat y_i$. $\mathrm{SSE}(p)$ is the total sum square error including two variance components.
$\mathrm{SSE}$ is the true sum square error from the 'true' model, while the sum square bias is $\mathrm{SSE}_B(p)=\sum_{i=1}^n(E[y_i]-E[\hat y_i])^2= \mathrm{SSE}(p) - \mathrm{SSE}$.
Then Mallows $C_p=\frac{\mathrm{SSE}(p)}{\hat\sigma^2} - n + 2p$.
If the supposed model is true, $\mathrm{SSE}_B(p)=0$, it gives $E[C_p|\mathrm{Bias}=0] = \frac{(n-p)\sigma^2}{\sigma^2}-(n-2p)=p$
Hence, a plot of $C_p$ versus $p$ can help to find the best one from many points. The proper model should have $C_p\approx p$ and smaller $C_p$ is preferred.
$C_p$ is often increase when $\mathrm{SSE}(p)$ decrease by adding predictors. A personal judgment can choose the best tradeoff between smaller $C_p$ and smaller $\mathrm{SSE}(p)$.

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- C_p=&\frac{1}{\hat\sigma^2}( \mathrm{SSE}_B(p) + \sum_{i=1}^n\mathrm{Var}[\hat y_i] )\\ -->
<!-- =&\frac{1}{\hat\sigma^2}( \mathrm{SSE}(p) - \mathrm{SSE} + \sum_{i=1}^n\mathrm{Var}[\hat y_i] )\\ -->
<!-- =&\frac{1}{\hat\sigma^2}( \mathrm{SSE}(p) - (n-p)\hat\sigma^2 + p\hat\sigma^2 )\\ -->
<!-- =&\frac{\mathrm{SSE}(p)}{\hat\sigma^2} - n + 2p -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \mathrm{AIC}=n\ln\left(\frac1n \mathrm{SSE} \right)+ 2p -->
<!-- (\#eq:aic) -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \mathrm{BIC}=n\ln\left(\frac{1}{n} \mathrm{SSE} \right)+ p\ln(n) -->
<!-- (\#eq:bic) -->
<!-- \end{equation} -->

-  A brief discussion

The original data sources often include more than one hundred variables such as NHTS, ACS, LEHD, and EPA's Smart Location Database.
It is hard to conduct the systematic variable selections for all of them.
In travel-urban form literature, the variables selections are mainly based on the background knowledge and research design.
The framework conceived by D-variables allows researchers to add new candidates they favorite and compare the results.
@astonStudyDesignImpacts2020 purpose that, in addition to D-variables, five explanatory variables are most common number in travel-urban form studies. 
Hence, for a target model with 10 covariates, above systematic methods of variables selection are still applicable.

For travel-urban form questions, researchers should also consider a target level of goodness-of-fit through variables selection.
Due to data limitation or other reasons, one may only use a subset of the true predictors to fit the model, which is called **underfitting**.
In social studies, underfitting is common because the social activities are very complex. Usual data collection could miss some important factors such as social network. Some psychological factors such as attitude or habit are hard to be quantified. 
Sometimes, the coefficients of determination are very low (e.g. $R^2_{adj} = 0.0088$ [@laneTAZlevelVariationWork2011], $R^2_{adj} = 0.085$ [@gordonNeighborhoodAttributesCommuting2004, p.27]).

In contrast, One may fit a model with extra irrelevant factors. **Overfitting** model fits the data too closely and may only capture the random noise.
Or the extra factors are accidentally related to the response in this data. 
Some travel-urban form studies may have the risk of overfitting. For example, @leeComplementaryPricingLand2013 's study applies two-stage least squares (2SLS) method and get $R^2 = 0.96$. Sometimes high $R^2$ may due to some specific research design or data source (e.g. $R^2 = 0.979$ [@zhaoWhatInfluencesMetro2013], $R^2 = 0.952$ [@cerveroDirectRidershipModel2010]).
Without suitable validation, many overfitting models produce false positive relationship and perform badly in prediction.


<!-- Suppose the true model is $\mathbf{y}=\mathbf{X}\boldsymbol\beta +\boldsymbol\varepsilon=\mathbf{X}_1\boldsymbol\beta_1 + \mathbf{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon$. $\mathbf{X}$ is full rank $r(\mathbf{X})=r =r_1+r_2$, $E[\boldsymbol\varepsilon]=0$, and $Cov[\boldsymbol\varepsilon]= \sigma^2\mathbf{I}_n$. -->
<!-- The normal equation $\mathbf{X'X}\boldsymbol\beta=\mathbf{X'y}$ can be rewrite as  -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{X}_1'\mathbf{X}_1\boldsymbol\beta^0_1+\mathbf{X}_1'\mathbf{X}_2\boldsymbol\beta^0_2&=\mathbf{X}_1'\mathbf{y}\\ -->
<!-- \mathbf{X}_2'\mathbf{X}_1\boldsymbol\beta^0_1+\mathbf{X}_2'\mathbf{X}_2\boldsymbol\beta^0_2&=\mathbf{X}_2'\mathbf{y}\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- Let $\mathbf{P}_i=\mathbf{X}_i(\mathbf{X}_i'\mathbf{X}_i)^{-}\mathbf{X}'_i$, $i=1,2$, and -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{M}_1=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{X}_2\\ -->
<!-- \mathbf{M}_2=&\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{X}_2 -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- Then, -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \boldsymbol\beta^0_1=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1(\mathbf{y}-\mathbf{X}_2\boldsymbol\beta^0_2)\\ -->
<!-- \boldsymbol\beta^0_2=&[\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{X}_2]^{-}\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{y}=\mathbf{M}^{-}_2\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{y}\\ -->
<!-- \hat\sigma^2=&\frac{1}{n-r}(\mathbf{y}-\mathbf{X}_1\boldsymbol\beta^0_1-\mathbf{X}_2\boldsymbol\beta^0_2)'(\mathbf{y}-\mathbf{X}_1\boldsymbol\beta^0_1-\mathbf{X}_2\boldsymbol\beta^0_2) -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- - Underfitting -->

<!-- If the fitted model $\mathbf{y}=\mathbf{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon$ doesn't contain $\mathbf{X}_2$ and $\boldsymbol\beta_2$, the least squares solutions are -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \boldsymbol\beta^0_{1,H}=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{y}\\ -->
<!-- \hat\sigma^2_{1,H}=&\frac{1}{n-r_1}\mathbf{y}'(\mathbf{I}-\mathbf{P}_1)\mathbf{y} -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- It is clear that $\boldsymbol\beta^0_{1,H}$ and $\hat\sigma^2_{1,H}$ are biased estimates because -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- E[\boldsymbol\beta^0_{1,H}]=&(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{X}_1\boldsymbol\beta_1+(\mathbf{X}'_1\mathbf{X}_1)^{-}\mathbf{X}'_1\mathbf{X}_2\boldsymbol\beta_2\\ -->
<!-- =&\mathbf{H}\boldsymbol\beta_1+\mathbf{M}_1\boldsymbol\beta_2 -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- and -->

<!-- \begin{equation} -->
<!-- E[\hat\sigma^2_{1,H}]=\sigma^2 + \frac{1}{n-r_1}\boldsymbol\beta'_2\mathbf{X}'_2(\mathbf{I}-\mathbf{P}_1)\mathbf{X}_2\boldsymbol\beta_2 -->
<!-- =\sigma^2 + \frac{1}{n-r_1}\boldsymbol\beta'_2\mathbf{M}\boldsymbol\beta_2 -->
<!-- \end{equation} -->

<!-- $E[\boldsymbol\beta^0_{1,H}]=\boldsymbol\beta_1$ and $E[\hat\sigma^2_{1,H}]=\sigma^2$ only when $\boldsymbol\beta_2=0$ or $\mathbf{M}_1=0$. The later is $\mathbf{X}_1\perp\mathbf{X}_2$ or $\mathbf{X}'_1\mathbf{X}_2=0$. -->

<!-- Since $\mathbf{\hat Y}_{0,H}=\mathbf{X}_{0,1}\boldsymbol\beta^0_{1,H}$, $\mathbf{\hat Y}_{0,H}$ is also biased unless $\boldsymbol\beta_2=0$ or $\mathbf{X}_1$ is orthogonal to $\mathbf{X}_2$. -->

<!-- Denote $MSE_{H}$ as the error mean squares of underfitting model. $MSE=\text{Var-cov}[\boldsymbol{\hat\beta}]+\text{Bias}\cdot\text{Bias}'$. Then -->


<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- MSE_{H}=&\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-} + \mathbf{M}_1\boldsymbol\beta_2\boldsymbol\beta_2'\mathbf{M}'_1\\ -->
<!-- MSE=&\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-} + \mathbf{M}_1Cov[\boldsymbol\beta_2^0]\mathbf{M}'_1\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- Since $Cov[\boldsymbol\beta_2^0]-\boldsymbol\beta_2\boldsymbol\beta_2'$ is a  positive semidefinite matrix (p.s.d.), $MSE\ge MSE_{H}$ always holds. -->


<!-- - Overfitting -->

<!-- That is, the true model is $\mathbf{y}=\mathbf{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon$ -->
<!-- and the fitted model is $\mathbf{y}=\mathbf{X}_1\boldsymbol\beta_1 + \mathbf{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon$. -->

<!-- This case implies $\boldsymbol\beta_2=0$. Then all above estimates are unbiased. -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- E[\boldsymbol\beta^0_{1,H}]=&\mathbf{H}\boldsymbol\beta_1+\mathbf{M}_1\boldsymbol\beta_2=\mathbf{H}\boldsymbol\beta_1\\ -->
<!-- E[\hat\sigma^2_{1,H}]=&\sigma^2 + \frac{1}{n-r_1}\boldsymbol\beta'_2\mathbf{M}\boldsymbol\beta_2=\sigma^2\\ -->
<!-- MSE_{H}=&\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-} + \mathbf{M}_1\boldsymbol\beta_2\boldsymbol\beta_2'\mathbf{M}'_1=\sigma^2(\mathbf{X}'_1\mathbf{X}_1)^{-}\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->




<!-- ## Other Topics -->

<!-- ### Bayesian approaches (Opt.) -->

<!-- ### SEM (Opt.) -->

<!-- Another attempt tries the method of structural equation modeling (SEM). The two studies capture higher elasticities of per capita VMT with respect to density (-0.38 and -0.238) [@cerveroEffectsBuiltEnvironments2010; @ewingStructuralEquationModels2014]. -->


<!-- In general, modeling is a case-by-case work. Researchers may have their preferred model by weighing the sensitivity and robustness even given the same hypothesis and data. -->
<!-- The published papers usually don't show the results of diagnosis and validation. -->
<!-- Under this circumstance, compare or summarize these outcomes are unreliable. -->

<!--chapter:end:files/06-issues.Rmd-->

# New Trends

There are many new trends in travel-urban form studies. This chapter selects spatial effects and non-linear relationship and introduces their basic idea and application.


## Controlling Spatial Effects

Recently there are more data sources including spatial information at small scale such as Block Group or traffic analysis zone (TAZ).
That allow researchers to identify or control the spatial effects.
Several categories of models such as multilevel model, mixture models, and mixed models are related to spatial effects. People may get confused by these concepts. This section tries to figure out their principle and meaning.

- Multilevel models

Multilevel models (as called hierarchical linear models) is applied on the data with hierarchical structure, that means the population is also hierarchical [@hoxMultilevelAnalysisTechniques2017].
The observed cases inside a subgroup are identical. The overall population is a mixture of many subgroups. 
It is the exact circumstances of travel-urban form studies. Individual travel behavior depends on the person's socioeconomic characteristics within this household, within this neighborhood, within this city and region.

A simple case of hierarchical models is that the model specification includes multi-scales factors.
It is increasingly used in travel-urban form studies [@schwanenImpactMetropolitanStructure2004;@ewingVaryingInfluencesBuilt2015;@parkNotParkingLots2019].


- Nested data models

In some research design, the spatial related factors are nested arrangement (which is different with the Nested Logit Models in structural choice analysis [@schmidheinyEquivalenceLocationChoice2011;@chuImplementationNewNetwork2018]).
Crossed effects versus nested effects is a dichotomy in experimental design [@montgomeryDesignAnalysisExperiments2017]. 

Crossed effects means that every levels of factor $a_{1},a_{2},...,a_{n}$ co-occurs with every levels of factor $b_{1},b_{2},...,b_{m}$.
There could be $mn$ levels of interaction effect between $a$ and $b$, that is $a_{1}b_{1},a_{1}b_{2},...,a_{n}b_{m}$.
There is at least one observation in any specific combination of categories.
A level of factor $a$ applied on the cases will refer to the same treatment.
For example, a household with/without child and with/without vehicle have crossed effects. All households can be assigned to the four categories. All households in one category have the same characteristics on parenthood and vehicle ownership.

Nested arrangement is also called hierarchical design. The levels of factor $a_{ij}$ nested under the levels of factor $B_i$, $i=1,2,..,m$, $j=1,2,...,n_i$. That means some levels of $a_{i1},a_{i2},...,a_{in_i}$ only occurs with one level of factor $B_i$.
In other words, all levels of $a_{ij}$ nested under of $B_i$ are unique.
There would not be interaction effect between $a$ and $B$. Some combinations of categories are not represented.
Take @mcneilRevisitingTODsHow2020 's study for example, two TOD programs, Center Commons and Broadway Vantage are nested in East Portland group, which further nested in Portland Region. Each program may has an unique effect on residents' travel behavior.
Nested data models is uncommon in travel-urban form studies because relevant studies usually don't investigate the interaction effects. @leeComparingImpactsLocal2020 add the two-way interaction terms between population weighted density (PWD) at Urban Area level and the D-variables at census tract level into their models. They find the cross-level interaction effects are highly significant.
Other paired or higher-order interactions are not considered in their study.

- Mixed models

When the standard regression model has more than one error term, the model includes both fixed effects and random effects, which is called mixed models.
The general form is $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\mathbf{Z}\boldsymbol{\delta}+\boldsymbol{\varepsilon}$.
where $\boldsymbol{\delta}$ represents the random effects by assuming $\boldsymbol{\delta}\sim N(\mathbf{0}, \sigma_\delta^2\mathbf{I})$ and $Cov(\boldsymbol{\delta},\boldsymbol{\varepsilon})=\mathbf{0}$.
$\sigma_\delta^2$ is the extra sources of variability in addition to $\sigma^2$. 
Random effects usually are related to categorical variables. 
$\mathbf{Z}$ is a $n\times q$ indicator matrix. $q$ refers to the levels of factors.
In mixed models, ordinary least squares method ignore the impact of the random effects.
When the grouped data is balanced, the generalized least square method is equivalent to ordinary method. 
The sum of square error is $SSE=\mathbf{y'[Z(Z'Z)^{-1}Z'-X(X'X)^{-1}X']y}$.
When the sample sizes in every groups are unequal (e.g. the travel data are divided by urban areas, TAZs, or neighborhoods),
Restricted Maximum Likelihood (REML) is an iterative approach which can deal with the variability among the groups.

<!-- \begin{equation} -->
<!-- \mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\mathbf{Z}\boldsymbol{\delta}+\boldsymbol{\varepsilon} -->
<!-- (\#eq:mix) -->
<!-- \end{equation} -->
<!-- Each iteration of REML involves two steps.  -->
<!-- The first step ignores the random effect. Only fixed effects $\boldsymbol{\hat\beta}^{(t)}$ are estimated using ordinary least squares method. Then, the results can construct a set of estimated residuals $\mathbf{\hat e}^{(t)}$. -->

<!-- In the second step, using maximum likelihood method to obtain the estimated variance components $(\hat\sigma^2_\delta)^{(t)}$ and the updated variance of $y$. -->

<!-- Plugging the new variance back to step one can get the updated fixed effects and residuals. -->
<!-- This procedure continues until the results being convergence. -->

<!-- Recall that maximum likelihood estimates assumes the observations follow a normal distribution. -->
<!-- Therefore, the mixed model with REML method has stricter requirement of the data or proper transformations. -->

The dichotomy of fixed effects and random effects is not decided by the factors themselves.
In research design, the study is interested in some factors and try to estimate corresponding coefficients.
These factors are assigned as having fixed effects on the response.
When these factors are categorical variables, the levels of the factor are chosen to test the differences among these specific levels. The chosen levels should exhaust the population and the fixed effects across cases are constant. The socioeconomic factors such as gender, race, lifecycle and D-variables are all fiexed effects.

When a travel-urban form study involve the spatial factors such as TAZ/Block Group, tract, or county/city, which often be assigned as random effect.
People have already known theses factors contribute a part of variance in the model and have significant impacts on response. By the principle of ANOVA, adding these terms into the model can improve the accuracy of model.
Assigning random effects is because of these factor have too many levels which can not be exhausted.
Or these levels represent comprehensive unknown mechanisms which have no explanatory value.



- Mixture models

If the pattern or mechanism is missing, the models using multilevel data are often called mixture models.
From frequentist perspective, a finite-dimensional mixture of $K$ components has a set of $K$ mixture weights and a set of $K$ parameters.
From Bayesian perspective, both of the weights and parameters follow the corresponding prior distributions.
Expectation maximization (EM) algorithm and Markov chain Monte Carlo (MCMC) are two methods which can solve the problem of mixture decomposition.
This situation is not common in urban studies. Because most of spatial data are collected by geographic units and have clear boundaries.

-  Geographically weighted regression

As discussed in last chapter, spatial heterogeneity and spatial autocorrelation are common issues in travel-urban form studies, especially at traffic analysis zone (TAZ) level.
The two problems are both because of the spatial dependency. The neighbored units could impact each other or share a common environment factor.
Geographically weighted regression (GWR) is a traditional technique to capture the spatial instability.
This approach is an application of the weighted least squares methods by involving location information as spatial variables such as latitude and longitude.

@cardozoApplicationGeographicallyWeighted2012 has shown that GWR models have better performance than ordinary least squares (OLS) for predicting the transit ridership at Madrid Metro station. 
Other studies further try some extended version of GWR.
@liuInfluenceBuiltEnvironment2018 's study on ridership find that geographically weighted Poisson regression (GWPR) models give smaller AIC than global GWR.
@chenDiscoveringSpatiotemporalImpacts2019 replace the metric of Euclidean distance (ED) with Minkowski distance (MD) in GWR models. 
@chenNonlinearEffectsBuilt2021 continue examining the models' performances among GWR, support vector machine (SVM), and Random Forest. Using 10-fold cross-validation, they find a hybrid method combining Random Forest and multiple local models can account the spatial heterogeneity and improve the predictive ability.


- A brief discussion

Various types of spatial effects would determine the model structures with corresponding data and research design.
For example, it should be careful that the nested effects are not obvious in some cases.
For example, population density should be a crossed factor because a density value (e.g. 1000 persons per square miles) is exactly the same in any cities. 
However, it is possible that a city has many high-density neighborhoods (such as the mean of residential density in Washington, DC is 7015 persons/sq.mi.), while another city only has low-density neighborhoods (such as the mean of residential density in Richmond-Petersburg and Norfolk-Virginia Beach in Virginia is 1950 persons/sq.mi.) [@zhangHowBuiltEnvironment2012].
In this case, the effects of density with respect to city may not be crossed.

Both mixed models and GWR methods become more widely used in travel-urban form studies. The essence of these tools is still to solve the IID issues. The correct way to use them is to make the methods matching the research design from the start. For example, the number of TAZs is usually very large and each TAZ's effects would not be the research interest [@dingNonlinearAssociationsZonal2021]. 
How about the urban areas? There are more than 400 urban areas in U.S. 
If the study wants to get a generalized result, urban areas would be assigned as a random effect [@hongHowBuiltenvironmentFactors2014;@leeComparingImpactsLocal2020].
But if the study does want to estimate each urban areas' effect on travel behavior, they still can treat it as a fixed effect [@ewingReducingVehicleMiles2020].



## Capturing Non-Linear Relationship

Sometimes, the mechanism of a factor are different in different parts of the range of $\mathbf{X}$.
@cliftonGettingHereThere2017 points out the assumption of linearity in many studies actually doesn't hold. A step function could express the relationship between VMT and urban density better.
A common example is the impact of income on travel distance. Research found that low-income and high-income households have longer travel distance than middle class households but the underlying reasons are different.
High-income families have less constrains on driving decision than middle class so they drive more.
However low-income families have stronger constrains than middle class because their homes are often far away from their workplaces or cheap grocery stores. In @zhangHouseholdTripGeneration2019 's study on trip generation, they choose a simplified way to deal with the nonlinearity. The cut of \$50,000 is chosen as the household income threshold and creates a indicator variable 'medium-to-high level of income.'
The similar things could happen on age, population density and other factors. 
In recent years, many researcher begin to pay attention to the non-linear relationship in travel-urban form studies.
In addition to step function, there are several ways which can capture the non-linear feathers in regression models.

- Polynomial Regression

An implication of Gravity Law is that the interaction between $m_1$ and $m_2$ (Equation \@ref(eq:gravity-law) should be considered. That is the attributes of origin and destination can collectively affect travel behavior.
This involves the second-order polynomial regression models $y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_{11}x_1^2+\beta_{22}x_2^2+\beta_{12}x_1x_2+\varepsilon$.
It has known that the distribution of VMT follow a decreasing exponential curve. Logarithm transformation can make the model at first order and convert the multiplicative relationship to additive.
Keeping the order of the model as low as possible is a general rule.
Because adding high-order terms could produce ill-conditioned $\mathbf{X'X}$ matrix or strong multicollinearity between $x$ and $x^2$.
Only if the curve still exists after transformation, the second-order terms could be added to the model.

<!-- \begin{equation} -->
<!-- y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_{11}x_1^2+\beta_{22}x_2^2+\beta_{12}x_1x_2+\varepsilon -->
<!-- (\#eq:polym) -->
<!-- \end{equation} -->

- Basis Functions

High-order models is a global structure of non-linearity. That means this function should hold on the whole range of $x$.
Using basis functions can avoid the weakness of a global structure. By dividing the range of $x$ into many segments, then a set of fixed and known functions can be applied to a variable $X$.
The points of the coefficients change are called **knots**.

**Step functions** is a special case of basis functions. Here the basis functions are a set of indicator functions. 
The idea is to convert a continuous variable, such as income into an ordered categorical variable. Let the cut-points are $c_1,c_2,..., c_k$ in the range of $X$, then the new variables are
Then the linear model is $y=\beta_0+\beta_1C_1(x)+\beta_2C_2(x)+\cdots+\beta_{k}C_k(x)+\varepsilon$
Note that $C_0(x)$ don't need to appear in the model because it is treated as the reference level.
Step functions divide the whole curve into many bins. This action could miss the trend of curve.
The choice of proper breakpoints is also a challenge.

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- C_0(x)=&\mathbb{I}_{x<c_1}\\ -->
<!-- C_1(x)=&\mathbb{I}_{c_1\le x<c_2}\\ -->
<!-- C_2(x)=&\mathbb{I}_{c_2\le x<c_3}\\ -->
<!-- \cdots\\ -->
<!-- C_k(x)=&\mathbb{I}_{c_k\le x}\\ -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- y=\beta_0+\beta_1C_1(x)+\beta_2C_2(x)+\cdots+\beta_{k}C_k(x)+\varepsilon -->
<!-- (\#eq:stepf) -->
<!-- \end{equation} -->



If the range of $x$ is divided into segments, each segment can fit a polynomial model. This method is called **piecewise polynomial** fitting or **spline**.
Adding constrain can make the fitted curves being continues. And the additional constrains can make the first and second derivatives of the piecewise polynomial continues.
A cubic polynomial with $k$ knots can add a truncated power basis function as
$h(x,c_i)=(x-c_i)^3_+=\begin{cases}(x-c_i)^3&\text{if }x>c_i\\0&\text{otherwise}\end{cases}$.
Then the spline with continuous first and second derivatives is
$y=\sum_{j=0}^3\beta_{0j}x^j+\sum_{i=i}^k\beta_{k}h(x,c_i)+\varepsilon$.
By computing the MSEs of every models with different number of knots, cross validation can be used to examine the best number of knots.
 When adding more knots, the value of MSE decrease. The optimal choice is the minimum number of knots with respect to a "small enough" MSE.

Regression spline is very flexible so have the risk of overfitting. The fitted curve can go though all of the $y_i$ without constrains. Denote the function $g(x)$ represent the constraints. **Smoothing spline** can be expressed as
$\arg\min_{g}\left\{\sum_{i=1}^n(y_i-g(x_i)^2)+\lambda\int g''(t)^2dt \right\}$,
where $\lambda$ is a nonegative tuning function. The left term of $\sum_{i=1}^n(y_i-g(x_i)^2)$ is the quadratic loss function. Loss reduction can improve the fitness of model.
The right term of $\lambda\int g''(t)^2dt$ is a penalty term. $g''(t)$ is the second derivative of function $g(\cdot)$ measure the amount of slope changing. If the fitted curve is very wiggly, the value of penalty term will be very large.
Therefore, smoothing spline tries to find the trade-off of loss and penalty by adjusting $\lambda$.
Using the leave-one-out (LOO) cross validation, the best value of $\lambda$ can be verified to minimize the $SSE$ and achieve the bias-variance balance.

- Non-parameter Regression

Non-parameter regression is an approach using a model-free basis for prediction. The basic idea is to select a set of neighborhood points inside a window defined by a bandwidth $b$. Then calculate $S=[w_{ij}]$, a weighting matrix. The smoother estimate of the $i$th response is $\mathbf{\tilde y}=\mathbf{Sy}\quad \text{or } \tilde y_i=\sum_{j=1}^n w_{ij}y_j$.

There are two common types of smoother, kernel and local regression.
The kernel smoother uses a weighted average for the estimation. And the kernel functions satisfy $K(t)\ge 0,\forall t$, $\int_{-\infty}^{+\infty}K(t)dt=1$, and $K(-t)=K(t)$.
The Kernel Regression is $w_{ij}=\frac{K(\frac{x_i-x_j}{b})}{\sum_{k=1}^n K(\frac{x_i-x_k}{b})}$.
For local weighted regression, the neighborhood points inside the span can fit locally a regression line or a hyperplane rather than a constant. 

- Generalized Additive Models

Generalized additive models (GAM) is an extension of Generalized Linear Models (GLM) which apply basis functions on several predictors [@hastieGeneralizedAdditiveModels1990; @woodGeneralizedAdditiveModels2017].
GAM provide a flexible framework because each variable $X_j$ has a separate basis function $f_j(\cdot)$. The basis could be any non-linear functions including polynomial regression, steps, splines, local regression, and others. The whole model adds every variables' contribution together in the end.
A general form is $y=\beta_{0}+\sum_{j=1}^{p-1}f_j(x_j)+\varepsilon$.
Fitting GAM will estimate each function by holding the remaining variables fixed. This procedure will repeat many time to update the estimations until convergence.
Interaction terms can also be added to the model.

- A brief discussion

The first benefit of above semi-parameter methods is to improve the models' fitness. @dingNonlinearAssociationsZonal2021 apply spline basis function on built-environment variables. The value of WAIC (widely applicable information criterion) in the new model is smaller than regular logit model.

The second benefit is that these fitted functions keep the regression models' structure. In a study of mode choice in Chicago, @zhouBikesharingTaxiModeling2019 find that, by simply adding polynomial interaction terms, the performance of multinomial logit models are as good as some machine learning algorithm (e.g. support vector machine, decision tree, and neural network). When choosing different methods in a study, the simpler and robust method is always more preferred.

The third important benefit is that these tools can help to identify the non-linear features.
"Built environment variables are effective only within a certain range." [@wuExaminingThresholdEffects2019]
In @zhaoPredictionBehavioralAnalysis2020 's study, the piece-wise utility functions reveal that passengers are time sensitive when waiting time is less than five minutes (i.e. it has a steeper slope than longer waiting time).
Thus, researchers can interpret these features as threshold effects or synergistic effects ^["threshold effect is an effect in a dependent variable that does not occur until a certain level, or threshold, is reached in an independent variable."(APA Dictionary of Psychology); "Synergistic effects are nonlinear cumulative effects of two active ingredients with similar or related outcomes of their different activities, or active ingredients with sequential or supplemental activities." (Delivery System Handbook for Personal Care and Cosmetic Products, 2005)].
The outcomes of nonlinear effects are more likely to convert to policy implication.
More researchers direct their attention to GAM rather than synthesized indices [@ewingReducingVehicleMiles2020;@parkGuidelinesPolycentricRegion2020].
Three recent IPEN studies ([the International Physical Activity and the Environment Network](http://www.ipenproject.org/IPEN_adult.html)) employed Generalized additive mixed models (GAMMs) to examine curvilinear effects of objective and perceived urban form factors on active travel. [@christiansenInternationalComparisonsAssociations2016; @kerrPerceivedNeighborhoodEnvironmental2016; @cerinObjectivelyassessedNeighbourhoodDestination2018].
As more relevant studies published, the systematic comparison and summary of threshold and other effects would be possible. 


## Other Topics

There are much more advanced methods can be applies in travel-urban form models.
@wangDetransformationBiasNonlinear2018 the Monte Carlo simulation to examine the detransformation bias in log-linear model.
Bayesian approaches is an alternative to frequentist methods [@zhangHowBuiltEnvironment2012; @hongHowBuiltenvironmentFactors2014].
But both of the two studies use the non-informative prior such as uniform distribution. If more studies could investigate more suitable distributions for the common parameters of D-variables, that will enhance the power of Bayesian method in travel-urban form studies.


Machine learning methods are becoming more popular in recent years.
The common methods include Support vector machines (SVM), Naive Bayes (NB),	Neural networks (NN), and Tree-based methods (e.g. Boosting trees (BOOST), Bagging trees(BAG), Classification and regression trees(CART), Random forest(RF)).
These methods usually have better predictive accuracy than traditional methods.
For example, using random forest method, both of models' mean absolute error (MAE) and root mean square error (RMSE) are smaller than regular linear models' results [@chengExaminingNonlinearBuilt2020].
Random forest can evaluate each independent variable's contribution in the fitted models. It also can disclose the the nonlinear features of built-envrionment factors on travel demand and mode choice [@yanUsingMachineLearning2020;@xuIdentifyingKeyFactors2021].
Gradient boosting decision trees (GBDT) is a popular tree-based method recently because it outperforms other methods on prediction precision [@dingApplyingGradientBoosting2018;@wangSynergisticThresholdEffects2020;@zhangNonlinearEffectAccessibility2020].

It doesn't mean the more complex models are always recommended.
@zhouBikesharingTaxiModeling2019 's study of travel choices finds that multi-layer neural network has similar predicting accuracy as simple machine learning method such as random forest. The reason might be that the data sample size in travel-urban form studies is not large enough to show the advanced algorithm's advantage.

Usually, the models' interpretability is the shortcoming of machine learning approaches.
Recent travel-urabn form studies often try both traditional and machine-learning methods [@zhouBikesharingTaxiModeling2019;@zhaoPredictionBehavioralAnalysis2020].
One interpreting method is to evaluate the ranking of variable importance. As discussed in previous chapter, standardized coefficients in traditional regression models can show which independent variable has strongest influence on response than others. Random forest and neural network also can give the ranks of all covariates if the matrix is non-singular. 
Another method is the partial dependence plot.
@zhaoPredictionBehavioralAnalysis2020 compare multinomial logit models and several machine learning approaches and find the effect of travel time by transit on mode choice is close to linear. while the waiting time and rideshare are two factors with nonlinearies. Then based on this results, the traditional regression models equipped with nonlienar functions can also give accurate and interpreatble estimates.



<!-- Common analysis techniques have difficulty to address high dimension data. -->

<!-- ### SEM (Opt.) -->

<!-- Another attempt tries the method of structural equation modeling (SEM). The two studies capture higher elasticities of per capita VMT with respect to density (-0.38 and -0.238) [@cerveroEffectsBuiltEnvironments2010; @ewingStructuralEquationModels2014]. -->

<!-- In general, modeling is a case-by-case work. Researchers may have their preferred model by weighing the sensitivity and robustness even given the same hypothesis and data. -->
<!-- The published papers usually don't show the results of diagnosis and validation. -->
<!-- Under this circumstance, compare or summarize these outcomes are unreliable. -->

<!-- Expected finding, implications, and limitations -->

<!--chapter:end:files/07-trends.Rmd-->

---
editor_options: 
  markdown: 
    wrap: sentence
---

# Meta-Analysis

Why do researchers need meta-analysis?
In short, meat-analysis could help to get more generalized effect sizes, to evaluate the impacts of study design on results, and to examine the publication bias in published literature.
Narrative reviews, systematic reviews, and meta-analyses are three type of evidence synthesis based on previous studies [@cuijpersMetaanalysesMentalHealth2016].
Traditional narrative reviews give an overview of a research field based on both relevant studies and author's opinions such as @ajzenTheoryPlannedBehaviour2011 's review of TPB and @marsQualitativeResearchTravel2016 's study of qualitative methods for activity-travel decisions.
The research questions and conclusions could be raised during or after reading the literature, but it is also possible that the author searches more evidences for proving the points after the conclusions had been clear in this review.
How to choose and narrate the evidences is highly flexible.

Systematic reviews require to define some clear rules for the review process. An example is @gandiaAutonomousVehiclesScientometric2019a 's review of autonomous vehicles literature.
After determining the research question, the formal review should have fixed standards on study selection and include all valid evidences. 

Meta-analysis can be looked as a special case of systematic reviews.
Each chosen studies in meta analysis contributes an observation for a predefined dataset.
The results in meta-analysis must be quantitative and be generated by statistical methods.
The methodology of systematic reviews tries to make the evidences being objective and reproducible.
meta-analysis further make the analysis process being transparent and reproducible.
Two good meta-analysis examples are @gardnerPsychologicalCorrelatesCar2008 's study of psychological correlates on car use and @semenescu30YearsSoft2020 's study of soft travel interventions on car use over the last 30 years.
The discussion in this chapter focus on the five travel-urban form meta-analysis [@ewingTravelBuiltEnvironment2010; @stevensDoesCompactDevelopment2017;@gimRelationshipsLandUse2013;@astonStudyDesignImpacts2020;@astonExploringBuiltEnvironment2021]

## Effect Sizes

Effect sizes are the outcome variables of interest in meta-analysis.
To make sure the effect sizes across studies are identical, an uniform measurement with the same meaning is essential.
A few important standards also include computable, reliable, and interpretable [@lipseyPracticalMetaanalysis2001; @higginsCochraneHandbookSystematic2019].
Effect Sizes describe both the direction and magnitude of the relationship.
In single group designs, the measurements of effect sizes include means, proportions, and correlations.
In control group designs, that include (Standardized) mean differences, risk and odds ratios.

-   Correlations

While means and proportions describe a single variable, correlations disclose the connection between two variables.
That is often the interest of research.
The support of correlation is from -1 to +1.
By a convention proposed by @cohenStatisticalPowerAnalysis1988, a correlation $r\le 0.10$ is small and the the two variables might be independent.
A correlation $r\ge 0.50$ is large and implies the two variables are dependent.
In social studies, many correlations are not over $0.3$ but it implies a substantial change of daily life.
Hence the 'large' or 'small' is context dependent.

Pearson product-moment correlation and its standard errors are calculated by $r_{xy} = \frac{\sigma^{2}_{xy}}{\sigma_x \sigma_y}$ and $SE_{r_{xy}} = \frac{1-r_{xy}^2}{\sqrt{n-2}}$.
To conduct some regression analysis, the correlation can be transferred into Fisher's $z$ which has a range of real number and is asymptotically normal distributed.
That is $z = 0.5\log_{e}\left(\frac{1+r}{1-r}\right)$ and $SE_{z} = \frac{1}{\sqrt{n-3}}$.
For one continuous variable and one categorical variable, point-biserial correlation is calculated by ${r_{pb}}= \frac{\sqrt{p_1(1-p_1)}(\bar{x_1}-\bar{x_2})}{s_x}$, where $\bar x_1$ is the mean of the continuous variable given the first level of the categorical variable $y$, and $\bar x_2$ is the mean given the secodn level of $y$.
$p_1$ is the proportion of observations that fall into the first level of $y$, and $s_x$ is the standard deviation of $x$.
When the proportions are closed to 0 or 1, The point-biserial correlation could has a restricted range [@bonettPointbiserialCorrelationInterval2020].

-   Standardized mean differences (SMD)

In control group designs, several methods can reduce the bias of effects size.
When the sample size is small, Hedges' $g$ can correct the standardized mean differences (SMD) by $g = \text{SMD} \times (1-\frac{3}{4n-9})$.
Another method is repeat measurement of the same case within a short time and under a stable environment.

A small correlation between the two variables may due to the highly restricted range.
For example, U.S. cities may represent the cities in low-density, developed countries and can not represent all cities in the world.
That could be the reason of detecting a weak relationship between VMT and urban density in the U.S.
In some cases, a small range is suitable such as excluding the observations with no-trip or extremely long trip distance.
Hence, the analysis of range restriction is crucial.
Research should investigate the variable ranges from literature or more general information.
Then define a proper restriction for the current research design.

If the restriction is meaningless and is not intended, a correction term $U$ is the ratio of standard deviation between the unrestricted population and restricted variable.
That is $U= \frac{s_{unres}}{s_{res}}$ where the value of $s_{unres}$ is based on relevant representative studies.
Then $r^*_{xy} = \frac{U\cdot r_{xy}}{\sqrt{(U^2-1)r_{xy}^2+1}}$.

-   Pooling Effect Sizes

As dicussed in previous chapter, the fixed-effect models assume the true effect size is a fixed value in all studies.
$\hat\theta_k = \mu + \varepsilon_k$ The random-effect models assume the effect sizes have a variance.
$\hat\theta_k = \mu + \zeta_k + \varepsilon_k$, where $\hat\theta_k$ is the estimate of effect size of study $k$, $\mu$ is the true mean of effect for all studies.
$\varepsilon_k$ is the random error and $\varepsilon\sim N(0,\sigma^2)$.
$\zeta_k$ is the random effect in study $k$ and $\zeta\sim N(0,\tau^2)$.
$\tau^2$ is the heterogeneity variance.

Then a weighted average effect size can get from $\hat\theta = \frac{\sum^{K}_{k=1} \hat\theta_kw_k}{\sum^{K}_{k=1} w_k}$, where $w_k = 1/s^2_k$ for fixed-effect models and $w_k = 1/(s^2_k+\tau^2)$ for random-effect models.
In random-effects models, the variance of the distribution of true effect sizes $\tau^2$ should be added into the denominator.

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- \hat\theta_k =& \mu  + \varepsilon_k \text{fixed effect}\\ -->

<!-- \hat\theta_k =& \mu + \zeta_k + \varepsilon_k \text{random effect} -->

<!-- \end{split} -->

<!-- (\#eq:fixran) -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \hat\theta = \frac{\sum^{K}_{k=1} \hat\theta_kw_k}{\sum^{K}_{k=1} w_k} -->

<!-- \end{equation} -->

<!-- \begin{equation} -->

<!-- \begin{split} -->

<!-- w_k = 1/s^2_k &\text{fixed effect}\\ -->

<!-- w_k = 1/(s^2_k+\tau^2) &\text{random effect} -->

<!-- \end{split} -->

<!-- (\#eq:weight) -->

<!-- \end{equation} -->

-   Meta-Regression

Similar with other regression analysis, Meta-regression chooses the effects sizes as the response, the characteristics of studies as predictors, for example the year, location, or language of study conducted.
Some meta-regression analysis select the attributes of research design as predictors.
For example, @stevensDoesCompactDevelopment2017 controls the effect of "controlling for residential self-selection".
In addition, @astonExploringBuiltEnvironment2021 add published years and "controlling for regional accessibility" as explanatory variables in meta-regression.

The model used in meta-regression usually assumes the added predictors have a fixed effect.
The random terms include the random errors $\epsilon_k$ and cross-study heterogeneity $\zeta_k$.
Hence it is a mixed-effects model.
That is $\hat\theta_k = \theta + \sum_{j=1}^{(p-1)}\beta_j x_{jk} + \epsilon_k+\zeta_k$.
Weighted least squares (WLS) is used in meta-analysis to address the different standard error of effect size.
Usually, the sample size of meta-regression is small and there is no extra information for cross validation.
There is no clear theoretical mechanism to explain the ralationship between effects sizes and paper's properties.
Hence, researcher should be temperance in adding more predictors.

Subgroup analysis are a special case of meta-regression.
When the added predictors are categorical variables, it means all observations inside a group have a shared effect.
If the group levels exhaust all possible levels of population, the group effects are looked as fixed.
If the group levels are just drawn from a large amount of levels, then the group effects are random and the observations in all groups share a common variance $\tau^2$.

-   A brief discussion

In @astonStudyDesignImpacts2020 's meta-analysis, correlations $r$ in literature are chosen as the response variable.
All of the z-scores, t statistics, and p-values in selected studies are converted to correlation.
That means the domain of response is $r\in[-1, 1]$.
The same thing also happens on elasticities [@stevensDoesCompactDevelopment2017; @astonExploringBuiltEnvironment2021].
When conducting meta-regression, Choosing Fisher's z score as the response might be the better choice.

@ewingTravelBuiltEnvironment2010 's meta-analysis uses weighed average elasticities to represent the pooling effect sizes.
The limitation of their data is lack of consistent standard error estimates from collected studies.
So they use sample size as an approximation of precision.

@gimRelationshipsLandUse2013 uses Hedges' g as the resonpse, "which represents the magnitude of the relationship between each of the five land use variables and travel behavior".
But by definition, "Hedge's g statistic is used to measure the effect size for the difference between means"(APA Dictionary of Psychology). 
Hence, using Hedges' g for controlled (quasi-)experimental studies [@semenescu30YearsSoft2020] is more reasonable.
The above questions can be checked by getting their working data.
Otherwise, a simulating test could give us some answers.

## Cross-study Heterogeneity

Cross-study heterogeneity describe the extent of variant of effect sizes with in a meta-analysis.
In social studies, cross-study heterogeneity is common and random-effects model usually is anticipated.
A high heterogeneity shows that the studies may contain two or more groups with different true effect.
A very high heterogeneity imply the overall effect is meaningless and the two or more groups should not be analyzed together.
Hence the degree of cross-study heterogeneity should always be reported in meta-analysis.
@ruckerUndueRelianceI22008 suggest to distinguish two types of heterogeneity.
**Baseline or design-related heterogeneity** means the population or research design has substantial difference.
That is the 'apples and oranges' problem.
While **statistical heterogeneity** reflects the magnitude of precision of effect size and is acceptable in meta-analysis.
Considering the random-effects model, $\tau^2$ is the variance of the true effect sizes.
The 95% confidence interval of the expected effect sizes is $\hat\mu \pm t_{K-1, 0.975}\sqrt{SE_{\hat\mu}^2+\hat\tau^2}$.

-   Cochran's Q

Cochran's $Q$ can be used to check whether the variation in the studies is reasonable [@cochranMethodsStrengtheningCommon1954].
If the random error is the only source of effect size differences, the value of $Q$ should not get an excess variation than expected.

```{=tex}
\begin{equation}
\begin{split}
Q = \sum^K_{k=1}w_k(\hat\theta_k-\hat\theta)^2\quad\text{where}&\quad w_k=1/s^2_k\quad\text{fixed effect}\\
Q = \sum_{k=1}^{K} w_k (\hat\theta_k-\hat\mu)^2\quad\text{where}&\quad w_k=1/(s^2_k+\tau^2)\quad\text{random effect}
\end{split}
\end{equation}
```
where $\hat\theta_k$ is the predicted effect on study $k$, $\hat\theta$ is the estimate of overall effect in fixed-effect model, $\hat\mu$ is the estimate mean of overall effect, $\tau^2$ is the variance of overall effect, $w_k$ is the weight calculated by the study's precision.
Cochran assume $Q$ approximately follow a $\chi^2$ distribution with $K-1$ degrees of freedom.
$K$ is the total number of studies in meta-analysis.
The null hypothesis is no heterogeneity.
Either increasing the number of studies $K$ or the sample size of each study can increase $Q$ value.
Therefore, only $Q$ can not be a sufficient evidence of heterogeneity.

-   $I^2$ Statistic and $H^2$ Statistics

Both $I^2$ and $H^2$ are based on Cochran's $Q$ [@higginsQuantifyingHeterogeneityMetaanalysis2002].
If $Q$ follows a $\chi^2$ distribution with $K-1$ degrees of freedom, then $E[Q]=K-1$ when there is no heterogeneity.
And $Q-(K-1)$ is the exceeded part of variation.
$I^2$ represents the percentage of the exceeded part in the effect sizes.
That is

```{=tex}
\begin{equation}
I^2 = \frac{Q-(K-1)}{Q}
\end{equation}
```
A conventional standard is that $I^2\le$ 25% means low heterogeneity, $I^2\ge$ 75% means substantial heterogeneity.
When $Q$ value is smaller than $K-1$, then let $I^2=0$.
Compared to $Q$, $I^2$ is not sensitive to the changes of number of studies.

$H^2$ is the direct ratio of observed variation over the expected variance.
When $H^2\le1$, there is no cross-study heterogeneity.
$H^2>1$ indicates the presence of cross-study heterogeneity.
$H^2$ is also increases along with the number of studies.

```{=tex}
\begin{equation}
H^2 = \frac{Q}{K-1}
\end{equation}
```

-   Outlier and leverage points.

When one or more studies have a large absolute value of residual, which is three times or more of standard deviation, these points are called outlier.
The observations with unusual predictors values could strongly influence the model and corresponding estimates.
These cases are called leverage points.

Both outlier and leverage point could change the modeling results.
But these are not sufficient evidences for removing these points.
Especially, deleting some cases is not acceptable if it tries to make the results more significant or have larger effect size.
The criteria should still be based on the research question.
Researcher needs to reexamine all available information to decide if these cases are not valid for this study.

- A brief Discussion

The five available meta-analysis of travel-urban form studies describe the selecting and screening process in details. 
Strictly filtering unsuitable cases can help to eleminate heterogenerity. 
But only @gimRelationshipsLandUse2013 uses Q-statistics to conduct homogeneous test for effect sizes.
Unfortunatelly, in his paper, only "conectivity-travel relationship" fails to reject the hypothesis of homogeneous.
$I^2$ Statistic and $H^2$ Statistics are absence in the five meta-analysis. In contract, @semenescu30YearsSoft2020 provide clear results of Q-test and $I^2$ for the 17 interventions variables to inspect heterogeneity.

For outlier issues, @astonStudyDesignImpacts2020 and @astonExploringBuiltEnvironment2021 choose 1.5 standard deviations and remove 22 and 42 outliers respectively.
The outlier criteria are also absence in other three studies. 
In the response to @stevensDoesCompactDevelopment2017 by @ewingDoesCompactDevelopment2017, they say "Guerra's elasticity looks like an outlier that probably should have been dropped from the sample," and "Mexico City is likely not the only atypical outlier in Stevenss sample frame,..." The sampling criteria in the five meta-analysis seem quite different.

<!-- ### Forest Plots -->

## Publication Bias


It is known that the studies with significant findings have greater opportunity for publishing.
This phenomenon will distort the findings, often overestimate the effect sizes, or overlook the negative effects.
That is called publication bias.

In meta-analysis the available studies, which usually refer to the published papers, are only a small part of all studies describing one research field.
The rest part, the unpublished studies for many different reasons, can be looked as missing data in statistics.
There are three types of missing data: Missing completely at random (MCAR) means the observed and unobserved events occur randomly and independent.
For MCAR data, the estimates are unbiased.
Missing at random (MAR) means the missingness is not random, but some variables can fully account the reason of missing.
By addressing the influencing factors, MAR data still can give unbiased estimates.
Missing not at random (MNAR) means the variable related to the reason of missing is not available.

There is no doubt that the published papers are not random selected and can not represent the population of studies.
Actually, there is not way to verify and solve the MNAR problems in meta-analysis.
It has to assume the type of missing is MAR and the missing events associates with some available information.

There are several common reasons of missing in meta-analysis [@pageInvestigatingDealingPublication2021; @pagePRISMA2020Explanation2021].
The first reason is questionable research practices (QRPs).
That means that researchers have bias when analyzing and reporting their findings [@simonsohnSpecificationCurveAnalysis2020].
For example, one type of QRP, p-hacking is repeating the trial until a significance level of $p<0.05$ appeared.
Changing the hypothesis after knowing the results is another type of QRP [@kerrHARKingHypothesizingResults1998].
By dropping off the "unfit" hypotheses, the conclusion will be biased and is not reproducible.

In the published papers, the studies with insignificant or negative results are often low cited and are easily omitted by studies selection.
That is called citation bias.
Time-lag bias refers to the studies with significant results are available earlier than others.
Language bias means that non-English studies are systematically neglected.
Sometimes, a study with exciting results could be used in several published papers.
For example, two published papers with the same authors use the same data and get similar results [@zhaoWhatInfluencesMetro2013; @zhaoAnalysisMetroRidership2014].
That is called multiple publication bias and it will reinforce the overestimated effects.

Published bias often refer to all of these biases happened before or after publication.
Their common feature is that the studies' results are the reason of missing.
For examining the risk of publication bias and mitigating the publication bias, two categories of methods are standard error based and p-value based.

### Standard Error-based Methods

The key assumption of these methods is that the effect's standard errors are related to studies' publication bias.
Standard error is also interpreted as study's precision.

-   Small-Study Effect

Small-study effect methods assume that a small study has greater standard error, overestimated effects, and larger publication bias [@borensteinIntroductionMetaAnalysis2021, ch. 30].
It is true that the studies with small sample size will give the effects with larger uncertainty.
The published small studies often have high effect sizes.
While, large studies often involve more resources, longer time, and some "big names" in a field.
Therefore, this method believe that publication bias has stronger influence on the small studies and the estimates in large studies are more close to the true values.

The funnel plot provide a graphic way to recognize the publication bias.
In a scatter plot of effects sizes (x-axis) versus standard errors (y-axis), each point represents a study (Figure \@ref(fig:funnelplot1)).
Ideally, the pattern should like a symmetric upside-down funnel or pyramid.
The top part with small standard errors is tight and the bottom part spread over.
All of the points should evenly distributed around the vertical line of mean effects.
But for the publication bias exists, the observed studies would concentrate at one side.
That also implies the mean effect could be a offset of the true effect.

Publication bias is not the only reason of asymmetric pattern.
cross-study heterogeneity also leads to asymmetric plot for the different true effects.
The contour-enhanced funnel plots [@petersContourenhancedMetaanalysisFunnel2008] adds more useful information and help to distinguish publication bias from other forms of asymmetry.
The regions of significance levels (e.g. $p< 0.1$, $p< 0.05$, and $p< 0.01$) in the plot shows how close the point cluster to the significance edge.
If the available studies lie around the edge, the true effect is likely to be zero.

```{r funnelplot1, echo=F, message=F, out.width="75%", fig.dim = c(8, 6),fig.cap="Contour-Enhanced Funnel Plot (adapted from @harrer2021doing)."}
# devtools::install_github("MathiasHarrer/dmetar")
library(meta)
library(dmetar)
data(ThirdWave)
m.gen <- metagen(TE = TE,
                 seTE = seTE,
                 studlab = Author,
                 data = ThirdWave,
                 sm = "SMD",
                 comb.fixed = FALSE,
                 comb.random = TRUE,
                 method.tau = "REML",
                 hakn = TRUE,
                 title = "Third Wave Psychotherapies")

col.contour = c("gray75", "gray85", "gray95")

# Generate funnel plot (we do not include study labels here)
funnel.meta(m.gen, xlim = c(-0.5, 2),
            contour = c(0.9, 0.95, 0.99),
            col.contour = col.contour)
# Add a legend
legend(x = 1.6, y = 0.01, 
       legend = c("p < 0.1", "p < 0.05", "p < 0.01"),
       fill = col.contour)
# Add a title
# title("Contour-Enhanced Funnel Plot (Harrer et al., 2021)")
```

-   Egger's regression test

Egger's regression test [@eggerBiasMetaanalysisDetected1997] can help to quantify the extent of asymmetry in funnel plot.
The simple linear model is $\frac{\hat\theta_k}{SE_{\hat\theta_k}} = \beta_0 + \beta_1 \frac{1}{SE_{\hat\theta_k}}$.
In Egger's test, the intercept $\hat\beta_0$ evaluate the funnel asymmetry.
If the hypothesis $\hat\beta_0=0$ is rejected, then Egger's test shows the plot is asymmetric.

-   Peters' Regression Test

For binary response, Peters' regression test [@petersComparisonTwoMethods2006] use a weighted simple linear model $\log\psi_k =  \beta_0 + \beta_1\frac{1}{n_k}$
where $\log\psi_k$ represent the log transformation on odds ratios, risk ratios, or proportions.
the predictor is the inverse of the sample size $n_k$ in $k$th study.
When fitting the model, each $1/n_k$ is assigned a weight $w_k$, depending on its event counts in treatment group $a_k$ and control group $c_k$, non-event counts in treatment group $c_k$ and control group $d_k$. That is $w_k = \frac{1}{\left(\dfrac{1}{a_k+c_k}+\dfrac{1}{b_k+d_k}\right)}$.
Peters' test uses $\beta_1$ instead of the intercept to test asymmetry.
When the test rejects the hypothesis of $\beta_1 = 0$, the asymmetry may exist.
For small sample size $K<10$, Eggers' or Peters' test may fail to identify the asymmetry [@sterneRecommendationsExaminingInterpreting2011].

-   Trim and Fill Method

Duval and Tweedie trim and fill method [@duvalTrimFillSimple2000] is a technique of eliminating publication bias.
It is an data imputation method by repeating two steps.
The first step of trimming tries to identify the outliers and reevaluate the estimates.
In the second step of filling, the trimmed points are adjusted by the expected bias and mirror to the opposite side.
Then the mean effect is recalculated based on all points.
This method is based on a strong assumption that the publication bias is the only reason of asymmetry.
It will fail when the cross-study heterogeneity is large [@simonsohnPcurveKeyFiledrawer2014].

-   PET-PEESE

PET-PEESE method [@stanleyMetaregressionApproximationsReduce2014] includes two parts of the precision-effect test (PET) and the precision-effect estimate with standard error (PEESE) Both of them are simple linear model with response of effect size and predictor of standard error. That is 
$\theta_k = \beta_0 + \beta_1\mathrm{SE}_{\theta_k}$ (PET) and $\theta_k = \beta_0 + \beta_1\mathrm{SE}_{\theta_k}^2$ (PEESE).
And the weights are still the inverse of the variance $w_k= 1/s_k^2$.
In the PET part, intercept $\hat\beta_0$ is used to examine whether the effect size is zero.
If the hypothesis of $\beta_0=0$ is rejected in PET model, then use the expected $\hat\beta_0$ in PEESE model as the corrected effect size.
the PET-PEESE method does not perform well for the meta-analysis with small sample size or high cross-study heterogeneity [@carterCorrectingBiasPsychology2019].

-   Rcker's Limit Meta-Analysis Method

limit meta-analysis by @ruckerTreatmenteffectEstimatesAdjusted2011 tries to shrink the publication bias by adding an adjusting term.
$\hat\theta^*_k =  \hat\mu + \sqrt{\dfrac{\tau^2}{SE^2_k + \tau^2}}(\hat\theta_k - \hat\mu)$
where $\hat\theta^*_k$ is adjusted expected effect size of study $k$.
$\hat\theta_k$ is the original expected effect size of study $k$.
$\hat\mu$ is the expected value of mean effects.
$SE^2_k$ is still the observed variance of $k$ and $tau^2$ is the cross-study variance (Figure \@ref(fig:rucker)).

```{r rucker, echo=F, message=F,fig.dim = c(8, 4),fig.cap="An example of Rcker's Limit Meta-Analysis (adapted from @harrer2021doing)."}
library(metasens)
# Create limitmeta object
lmeta <- limitmeta(m.gen)
# dev.off() 
par(mfrow = c(1,2))
# Funnel with curve
funnel.limitmeta(lmeta, xlim = c(-0.5, 2))
# Funnel with curve and shrunken study estimates
funnel.limitmeta(lmeta, xlim = c(-0.5, 2), shrunken = TRUE)
```

The gray curve shows that the magnitude of expected bias is a monotone increasing function of standard error.
Every observations' effects are adjusted by this curve.

### P value-based Methods

-   P-Curve

Since the studies with significant results (p-value < 0.05) tend to be published, either due to the authors or reviewers, the distribution of p-values in the published papers should be exceptional.
P-Curve methods is straightforward by examining whether the p-values in selected studies follows a "reasonable" distribution.

In a simulation test, @harrer2021doing show that the simulated p-values by sampling from a standard normal distribution follows a exponential distribution (Figure \@ref(fig:pcurve)).
When the true effect size is large, the distribution is highly right skewed.
Along with the effect size decreasing , the distribution has a longer and longer tail until it becomes an uniform distribution.
while another influencing factor is the sample size $n$.
Larger $n$ will lead to shorter tail.

Based on this assumption, the distribution of p-value in p-hacking studies will be left skewed.
P-curve only shows that the distribution of p-value associates with effect size and sample size.
But the true distribution is still unknown.

```{r pcurve, fig.dim = c(12, 4), message=F, echo=F, out.width="100%", fig.align="center", fig.cap="P-curves for varying study sample size and true effect (adapted from @harrer2021doing)."}
library(rlang)
# Define helpers
pdist = function(x, lower.tail) pnorm(x, lower.tail=lower.tail)
plotter = function(x, var){
  
  x %>% filter(!!enquo(var) < 0.05) %>% pull(!!enquo(var)) %>% 
    hist(breaks = 20, plot = F) -> h
  h$density = h$counts/sum(h$counts)*100
  plot(h, freq = F, main = NULL, ylab = "Percentage", 
       xlab = bquote(italic("p")~"value"), col = "gray")
  
  d = strsplit(deparse(substitute(var)), "_") %>% unlist() %>% nth(2)
  n = strsplit(deparse(substitute(var)), "_") %>% unlist() %>% nth(3)
  d = as.numeric(d)/10
  title(bquote(theta~"="~.(d)~~~"n = "~.(n)))
} 

# # Simulate
# dat = list()
# for (i in 1:1e5){
# 
#   dat.inner = list()
#   for (n in c(20, 50, 100)){
# 
#   z = mean(rnorm(n, 0, 1))*sqrt(n)
#   dat.inner[[paste0("s_0_", n)]] = z
# 
#   z = mean(rnorm(n, .2, 1))*sqrt(n)
#   dat.inner[[paste0("s_2_", n)]] = z
# 
#   z = mean(rnorm(n, .5, 1))*sqrt(n)
#   dat.inner[[paste0("s_5_", n)]] = z
#   }
# 
#   dat[[i]] = unlist(dat.inner)
# }
# 
# do.call(rbind, dat) %>%
#   apply(., 2, function(x) 2*pmin(pdist(x, TRUE), pdist(x, FALSE))) %>%
#   data.frame() -> simdat.p
# saveRDS(simdat.p, "~/urbanstudy/field_paper/data/simdat.p.rds")
simdat.p <- readRDS("~/urbanstudy/field_paper/data/simdat.p.rds")

par(mfrow = c(1,3)) #, cex.main = 2, bg="#FFFEFA"
simdat.p %>% plotter(s_0_20)
# simdat.p %>% plotter(s_0_50)
# simdat.p %>% plotter(s_0_100)
simdat.p %>% plotter(s_2_20)
# simdat.p %>% plotter(s_2_50)
# simdat.p %>% plotter(s_2_100)
simdat.p %>% plotter(s_5_20)
# simdat.p %>% plotter(s_5_50)
# simdat.p %>% plotter(s_5_100)
```

-   Test for Right-Skewness

It is hard to know the true distribution of p-values but it is easy to test whether the effect size equals zero or not.
Here converts the p-value to a proportion of pp-value as $pp=p/\alpha$ where $\alpha$ is the significance level.
Using Fisher's method, the null hypothesis is no right-skewness. Then $\chi^2_{2K} = -2 \sum^K_{k=1} \log(pp_k)$
If the null hayothesis is rejected, that means the effect exists.

-   Test for 33% Power (flatness of the p-curve)

Test for 33% power starts from another direction.
Based on the properties of the non-central distribution of $F$, $t$, or $\chi^2$, the null hypothesis is that a small effect exists, or the p-curve is slightly right skewed.
The 33% power is a rough threshold.
Statistical power $1-\beta$ means a probability of correctly rejecting the null hypothesis.
It is equivalent with a 66% probability of Type II error, also called "false negative".

<!-- For example, in a paired t-test for normal distributed data, if the alternative hypothesis is true and the true difference $\theta=\theta_0$. -->
<!-- Then the power is -->

<!-- ```{=tex} -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- &Pr(\frac{\hat\theta-0}{SE}>\Phi^{-1}(0.95)|\theta=\theta_0)\\ -->
<!-- =&Pr(\frac{\hat\theta-\theta}{SE}>1.64-\frac{\theta}{SE}|\theta=\theta_0)\\ -->
<!-- =&1-Pr(\frac{\hat\theta-\theta}{SE}<1.64-\frac{\theta}{SE}|\theta=\theta_0)>0.33 -->
<!-- \end{split} -->
<!-- (\#eq:33power) -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- When the sample size is large, $\frac{\hat\theta-\theta}{SE}$ asymptotically follow a standard normal distribution. -->
<!-- Then -->

<!-- ```{=tex} -->
<!-- \begin{equation} -->
<!-- \frac{\theta}{SE}>1.64-\Phi^{-1}(0.66)\approx 1.2324 -->
<!-- \end{equation} -->
<!-- ``` -->
When the right-skewness test cannot reject $\theta=0$, then the flatness test may reject that the effect size is large.
If both of the two tests are not significant, then the evidence is insufficient for any conclusion.
Note that the flatness test depends on how to define a small value of $\theta$.
Another alternative method is the Kolmogorov-Smirnov (KS) test by comparing a sample with a reference probability distribution.

-   Selection Models

Selection Model supposes a probability density function $f(\theta)$ can reflect the true distribution of effect sizes without publication bias.
The background assumption still is the observed effect sizes $\theta_k \sim N(\mu,\sigma^2+\tau^2)$, sampling error $\sigma^2$, and cross-study heterogeneity variance $\tau^2$.
By assuming a weight function $w(p_k)$ of p-value $p_k$ can represent the mechanism of publication bias, then the adjusted function $f^*(\theta)$ should be consistent with the observed data.
That is $f^*(\theta_k) = \frac{w(p_k)f(\theta_k)}{\int w(p_k) f(\theta_k) d\theta_k}$.

A straightforward choice of $w(p_k)$ is a **step function** [@hedgesEstimatingEffectSize1996].
Since $\alpha_{1,2,3}=0.05, 0.1, 0.5$ is the common critical values, the step function uses them as cut points and divide the range of p-value $p\in(0,1)$ into four segments.
Using maximum likelihood, the probability for each segment can be estimated such as $w_{1,2,3,4}=1,0.8,0.6,0.35$, and the best fitted $f^*(\theta)$ can recover the true function $f(\theta)$ and unbiased estimates of $\hat\mu$ and $\hat\tau^2$ (Figure \@ref(fig:stepcurve)). 

```{r stepcurve, warning = FALSE, message=F, echo=F, fig.dim = c(8, 6), fig.align="center", out.width="60%",fig.cap="Selection model based on a step function (adapted from @harrer2021doing)."}
library(ggplot2)
df = data.frame(x = c(0, 0.025, 0.05, 0.5, 1),
                y = c(1, 0.8, 0.6, 0.35, 0.35))
ggplot(data = df, aes(x = x, y = y)) +
  geom_step(cex = 1) +
  geom_vline(xintercept = 0.025, linetype = "dotted", color = "gray30") +
  geom_vline(xintercept = 0.05, linetype = "dotted", color = "gray30") +
  geom_vline(xintercept = 0.5, linetype = "dotted", color = "gray30") +
  geom_vline(xintercept = 1, linetype = "dotted", color = "gray30") +
  annotate("text", x = 0.105, y = 0.9, label = bquote(a[1]~"="~0.025), hjust = "left",
           color = "gray30") +
  annotate(geom = "curve", x = 0.105, y = 0.9, xend = 0.02, yend = 1.02, 
           curvature = .2, arrow = arrow(length = unit(2, "mm")), linetype = "solid",
           color = "gray30") +
  annotate("text", x = 0.15, y = 0.7, label = bquote(a[2]~"="~0.05), hjust = "left",
           color = "gray30") +
  annotate(geom = "curve", x = 0.15, y = 0.7, xend = 0.05, yend = 0.82, 
           curvature = .2, arrow = arrow(length = unit(2, "mm")), linetype = "solid",
           color = "gray30") +
  annotate("text", x = 0.6, y = 0.5, label = bquote(a[3]~"="~0.5), hjust = "left",
           color = "gray30") +
  annotate(geom = "curve", x = 0.6, y = 0.5, xend = 0.5, yend = 0.62, 
           curvature = .2, arrow = arrow(length = unit(2, "mm")), linetype = "solid",
           color = "gray30") +
  # annotate("text", x = 0.8, y = 0.45, label = bquote(a[4]~"="~0.35), hjust = "left",
  #          color = "gray30") +
  # annotate(geom = "curve", x = 0.8, y = 0.45, xend = 0.75, yend = 0.35, 
  #          curvature = .2, arrow = arrow(length = unit(2, "mm")), linetype = "solid",
  #          color = "gray30") +
  theme_classic() +
  ylab(bquote("Probability of Selection ("~omega~")")) +
  xlab("p-value") + 
  scale_x_continuous(breaks = c(0, 0.025, 0.05, 0.5, 1), expand = c(0, 0), limits = c(0, 1.05),
                     labels = c("0", "0.05", "0.1", "0.5", "1")) +
  scale_y_continuous(breaks = c(0, 0.35, 0.6, 0.8, 1), expand = c(0,0), limits = c(0, 1.1), 
                     labels = c("0%", "35%", "60%", "80%", "100%")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.background = element_rect(fill = "#FFFEFA", color = "#fbfbfb"),
        panel.background = element_rect(fill = "#FFFEFA"))
  
```

**Three-parameter model** assumes only one cut-point [@mcshaneAdjustingPublicationBias2016].
That is $\alpha=0.025$, which means p-value $=0.05$ in a two-sides test.
Then the weight function only has two segments.
The three parameters inlude the true effect $\mu$, the cross-study heterogeneity variance $\tau^2$, and the probability of the second segment $w_2$.

**Fixed weights selection model** completely assigns all the cut points and their weight [@veveaPublicationBiasResearch2005].
This method allows to fit a very flexible selection model.
On the contrary, one can also assume a global monotonic decreasing distribution such as half-normal, logistic, negative-exponential, etc.

All of these methods are based on the assumed distribution under publication bias being correct.
Without sufficient cases and test, identifying the true distribution in one specific field is difficult.
Especially, when cross-study heterogeneity is high, such as $I^2\approx$ 75%, these approaches are not reliable [@vanaertConductingMetaAnalysesBased2016].
Therefore, when the meta-analyses of travel-urban form studies show the high heterogeneity, it is hard to verify and eliminate the possible publication bias.

```{r pubtab, eval=F,echo=F, message=F}
dat <- data.frame(
  Method = c("Trim-and-Fill","PET-PEESE","Limit Meta-Analysis","P-Curve","Selection Models"),
  Advantages = c(
    "Very heavily used in practice. Can be interpreted by many researchers.",
    "Based on a simple and intuitive model. Easy to implement and interpret.",
    "Similar approach as PET-PEESE, but explicitly models cross-study heterogeneity.",
    "Has been shown to outperform other methods when its assumptions are met.",
    "Can potentially model any kind of assumed selection process. The three-parameter selection model has shown good performance in simulation studies."),
  Disadvantages = c(
    "Often fails to correct the effect size enough, for example when the true effect is zero. Not robust when the heterogeneity is very large; often outperformed by other methods.",
    "Sometimes massively over- or underestimates the effect. Weak performance for meta-analyses with few studies, low sample sizes, and high heterogeneity.",
    "Performance is less well studied than the one of other methods. May fail when the number of studies is very low (<10) and heterogeneity very high.",
    "Works under the assumption of no heterogeneity, which is unlikely in practice. Requires a minimum number of significant effect sizes. Less easy to interpret and communicate.",
    "Only valid when the selection model describes the publication bias process adequately. Assume that other small-study effects are not relevant. Can be difficult to interpret and requires background knowledge."
  )
)

kable(dat %>% mutate_all(linebreak), "html", booktabs = T, escape = FALSE, 
      align = "l", longtable = T, 
      caption = "Methods to estimate the true effect size corrected for publication bias: Overview of advantages and disadvantages.(Harrer et al., 2021,Table 9.1)") %>% 
  kable_styling(latex_options = c("repeat_header"), 
                bootstrap_options = c("condensed", "striped"),
                font_size = 15) %>% 
  row_spec(0, bold=TRUE) %>% 
  column_spec(1, width = "2cm", bold = T) %>% 
  column_spec(2, width = "4cm") %>% 
  column_spec(3, width = "5cm") %>%
  column_spec(1, italic= FALSE) 
```



- A brief discussion

@ewingTravelBuiltEnvironment2010 try to minimize publication bias by adding gray literature.
@stevensDoesCompactDevelopment2017 employs PEESE method to adjust the "selective reporting bias."
@astonStudyDesignImpacts2020 choose Egger's regression test to detect publication bias and use meta-regression to correct the estimates (here I cannot identify what method they use). It is strange that they don't mention publication bias in their second meta-analysis [@astonExploringBuiltEnvironment2021].
The discussion about publication bias is also absence in @gimRelationshipsLandUse2013 's study.
The only funnel plot is found in @semenescu30YearsSoft2020 meta-analysis of soft intervention on car use (Figure \@ref(fig:funnelplot2)).

```{r funnelplot2,eval=T,out.width='75%', fig.align='center',fig.cap="A example of funnel plot [@semenescu30YearsSoft2020]"}
knitr::include_graphics("~/urbanstudy/field_paper/fig/semenescu30YearsSoft2020.jpg")
```


A high-quality meta-analysis needs to define a suitable research question. 
@astonExploringBuiltEnvironment2021 and @semenescu30YearsSoft2020 select the checklist of
Preferred Reporting Items for Systematic Reviews and Meta-Analyses [(PRISMA)](http://www.prisma-statement.org/) as their guidance.
Other scholars give more suggestions and plans. 
In FINER framework, a research question should be "feasible, interesting, novel, ethical, and relevant" [@hulleyDesigningClinicalResearch2013, Ch.2, Cummings et al.].
PICO framework [@mattosSystematicReviewMetaanalysis2015] emphasize the eligible criteria on "population, intervention, control group or comparison, and outcome."
The American Psychological Association (APA) gives a guidance of inclusion and exclusion criteria, coding procedures, and statistical methods in the Meta-Analysis Reporting Standards (MARS).
In a recent paper, @pigottMethodologicalGuidancePaper2020 propose some methodological standards and recommended analysis plan for meta-analyses in social science.


@cuijpersMetaanalysesMentalHealth2016 suggests the information extracted from the selected studies should include studies' characteristics, effect-size related values, and study quality information.
The values of sample size, estimated effect size and standard deviation are essential contents.
In addition to the data locations, collecting years, and methods, the papers' published years, authors, etc. can also be the explanatory variables in meta-analysis.
For randomized controlled trials, the Cochrane Risk of Bias tools are used to evaluate study qualities [@higginsCochraneHandbookSystematic2019; @sterneRoBRevisedTool2019].
But the consistent and transparent study quality assessments are not common in social studies [@hohnPrimaryStudyQuality2019].

Meta-analysis has more strict requirement on data collection than traditional reviews.
The inclusion criteria must be defined carefully.
Meta-analysis needs the studies' results are identical then can synthesize some general results.
The selected studies should have comparable research design including data sources, methods, and outcomes.
Otherwise, the synthesizing the studies without common properties are meaningless, that is called the 'apples and oranges problem'.
Yet, all published papers have unique contributions and have some differences.
What is the allowed variant among studies depends on the research questions of meta-analysis.
For example, if a research wants to find the relationship between travel behavior and influencing factors at individual level, the studies with aggregated travel response at city/region level should be excluded.
And vice versa because individual behavior and social behavior are different questions.
Therefore, in meta-analysis of urban studies, there is particular concern about modifiable areal unit problem (MAUP).


The results of meta-analyses are the derivations of available studies.
Thus, if most of the studies have cognitive errors such as the geocentric model, meta-analysis itself can not reject them and will actually reinforce them.
People should not have too high expectation of meta-analysis.
If most of the studies have poor quality or are systematically biased, meta-analysis will also be flawed inevitably, which is called "garbage in, garbage out" problem.
That is why assessing study quality is critical in study selection.

Another type of limitation is that the available studies can not represent the 'true' population.
Ideally, the observed studies are produced randomly and independently in a field but it never happens in reality.
Publication bias, is also called "file drawer" problem means that researchers tend to submit the studies with positive, exciting, or significant results.
Journal also tend to publish such kinds of articles.
Thus, the available studies overrepresent a specific subgroup systematically and the results are biased.
Some statistical techniques try to reduce the bias to a certain extent.

The "researcher agenda" problem is another threaten. It means that the researchers themselves are the important influencing factors in meta-analysis.
During searching studies and analyzing data, they try to stay objective but may always have some personal preferences.
Given the same data, researchers' undisclosed opinions or unintentional choices could lead the analysis to vary conclusions [@silberzahnManyAnalystsOne2018].
Hence, post hoc ("after this") analysis or data dredging should be avoid.
A prior analysis makes the results more valid and reliable.




<!-- ### Advanced Methods -->

<!-- #### Multilevel Meta-Analysis -->

<!-- #### Structural Equation Modeling Meta-Analysis -->

<!-- #### Network Meta-Analysis -->

<!-- #### Bayesian Meta-Analysis -->

<!-- ### Summary -->

<!--chapter:end:files/09-Meta-Analysis.Rmd-->



# Summary {#summary}

Part II starts from the first question in modeling, what type of model we should choose?
Then the following question is, do this method is correct?
The last question would be, how can we do better?
Recent literature usually focus on the third question.
But for any research, the first two questions are not obvious or self-evidence.
Thus, the this paper makes efforts to review some fundamental knowledge.

For the first question, existing theories have given some hints for sorting the travel data and models according to their properties.
But in literature, there are some still contradictory cases need further test and comparison (e.g. ordinary linear models with log-transform on count data v.s. count data models).
Each method has the suitable application and limitations. For example, PCA become less used in the last three years because it is hard to interpret.

Several common issues are presented to remind the second questions.
Derivation and test are two ways of proving a statistical method. In the literature, we can only see the author's choices of data and methods. Without replicating the models, it is hard to assess the methods.
The real world have no perfect data. Although some diagnosis and validation methods can aid us to find some problems, the results would not tell us what should we do automatically. The only way is to try many data, methods, and check the results repeatedly.

Spatial effects are heeded in recent studies. For heterogeneous issues, GWR and aggregation represent two distinct solutions.
GWR tries to capture the local dynamic among the spatial data and require intensive computation.
The aggregated methods try to eliminate heterogeneity. As the sample sizes inside the aggregating areas increasing, the variance of estimated coefficients will decrease. 
"Ecological fallacy" sounds like a mistake. Small scale could also distract the researchers by unimportant details.
Suitable aggregated data and models structure should match the research question.

Capturing the nonlinear feathers is also popular in recent studies. 
From the perspective of policy implementation, public may only concern about some effective ranges of variables.
For example in rural area or CBD, population density would not be a useful tool for intervention.
Threshold effects or synergistic effects are also interesting because they can tell planner and policy makers how to select effective combination from the toolbox.
There always are new methods being developed. 
This paper gives a brief on some advanced methods such as machine learning. These methods provide additional tools for examining the results and would not replace traditional method at present.

<!-- Though this paper, a gain is that now I can identify what the category one method belongs to.  -->
<!-- If some methods such as machine learning is required, I can make further study on this topic.  -->

<!-- Critically discussing the statistical methods in literature is a huge challenge. -->
<!-- Most of the time, I try to understand what the methods are and mainly focus on the classic approaches. -->
<!-- I often doubt whether my commons is correct or misunderstand the author's meaning or the method itself. -->
<!-- It is necessary to read these cited articles third time and more. -->

-   Meta-Analysis in travel-urban form studies

The meta-analysis in travel-urban form studies faces more exacting challenges.
The same on many social studies, travel-urban form studies use observation design.
They cannot satisfy the requirement of randomized controlled trials (RCT).
The urban environment factors have many unknown and complex influences on travel behavior.
They are not controllable as in laboratory.
All the traveler lives in many kinds of urban environment.
There is no control group for measuring the between-group mean difference.
Most of travel survey only capture a moment of travel pattern.
There is no repeat measurement for evaluating the random error.
These shortfalls also make the relevant studies are highly heterogeneous.
If the scope of studies is wide, the validity of meta-analysis becomes problematic.
If the selection criteria is rigorous, there might be less than 10 studies on a topic.

Although meta-analysis in travel-urban form studies is not as reliable as the studies of RCT.
Scholars still put effort into this approach.
Because a generalized conclusion could have substential influence on policy making and social attitudes.

<!-- `r if (knitr:::is_html_output()) ' -->

# References {-}


<!--chapter:end:files/10-references.Rmd-->

